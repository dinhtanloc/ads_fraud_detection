{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0\n",
      "NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  \n",
    "print(torch.cuda.current_device())  \n",
    "print(torch.cuda.get_device_name(torch.cuda.current_device())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. Using GPU.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"CUDA is available. Using GPU.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA is not available. Using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from pytorch_tabnet.pretraining import TabNetPretrainer\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([700, 42]) torch.Size([700, 2]) torch.Size([300, 42]) torch.Size([300, 2])\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.DataFrame(np.random.rand(700, 42), columns=[f\"feature_{i}\" for i in range(42)])\n",
    "df_test = pd.DataFrame(np.random.rand(300, 42), columns=[f\"feature_{i}\" for i in range(42)])\n",
    "\n",
    "y_train = (np.random.rand(700) > 0.5).astype(np.float32)\n",
    "y_test = (np.random.rand(300) > 0.5).astype(np.float32)\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(df_train.values)\n",
    "X_test = scaler.transform(df_test.values)\n",
    "\n",
    "\n",
    "# y_train = y_train.values.astype(np.float32)\n",
    "# y_test = y_test.values.astype(np.float32)\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)\n",
    "num_classes = 2  # Assuming binary classification\n",
    "y_train = F.one_hot(y_train, num_classes=num_classes)\n",
    "y_test = F.one_hot(y_test, num_classes=num_classes)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "one_hot is only applicable to index tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[113], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m num_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m  \u001b[38;5;66;03m# Assuming binary classification\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m y_train_onehot \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mone_hot\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_classes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m y_test_onehot \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mone_hot(y_test, num_classes\u001b[38;5;241m=\u001b[39mnum_classes)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: one_hot is only applicable to index tensor."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "train_dataset = TensorDataset(X_train, y_train_onehot)\n",
    "test_dataset = TensorDataset(X_test, y_test_onehot)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\miniconda3\\envs\\pytorch\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 270.00562| val_0_unsup_loss_numpy: 94.52517700195312|  0:00:00s\n",
      "epoch 1  | loss: 638.96796| val_0_unsup_loss_numpy: 86.1489028930664|  0:00:00s\n",
      "epoch 2  | loss: 244.51616| val_0_unsup_loss_numpy: 40.36521911621094|  0:00:00s\n",
      "epoch 3  | loss: 193.12486| val_0_unsup_loss_numpy: 31.518159866333008|  0:00:00s\n",
      "epoch 4  | loss: 102.24525| val_0_unsup_loss_numpy: 17.251129150390625|  0:00:00s\n",
      "epoch 5  | loss: 57.67551| val_0_unsup_loss_numpy: 8.71133041381836|  0:00:00s\n",
      "epoch 6  | loss: 46.38808| val_0_unsup_loss_numpy: 9.282190322875977|  0:00:00s\n",
      "epoch 7  | loss: 34.91736| val_0_unsup_loss_numpy: 6.242280006408691|  0:00:00s\n",
      "epoch 8  | loss: 22.04987| val_0_unsup_loss_numpy: 5.376999855041504|  0:00:00s\n",
      "epoch 9  | loss: 18.68647| val_0_unsup_loss_numpy: 5.352340221405029|  0:00:00s\n",
      "epoch 10 | loss: 14.43801| val_0_unsup_loss_numpy: 3.8629400730133057|  0:00:00s\n",
      "epoch 11 | loss: 11.87551| val_0_unsup_loss_numpy: 3.0694000720977783|  0:00:01s\n",
      "epoch 12 | loss: 10.00301| val_0_unsup_loss_numpy: 3.524009943008423|  0:00:01s\n",
      "epoch 13 | loss: 9.26132 | val_0_unsup_loss_numpy: 2.52266001701355|  0:00:01s\n",
      "epoch 14 | loss: 7.36269 | val_0_unsup_loss_numpy: 3.6568400859832764|  0:00:01s\n",
      "epoch 15 | loss: 6.52974 | val_0_unsup_loss_numpy: 2.487459897994995|  0:00:01s\n",
      "epoch 16 | loss: 5.71704 | val_0_unsup_loss_numpy: 2.1758899688720703|  0:00:01s\n",
      "epoch 17 | loss: 5.00809 | val_0_unsup_loss_numpy: 2.3139901161193848|  0:00:01s\n",
      "epoch 18 | loss: 4.47515 | val_0_unsup_loss_numpy: 2.4486401081085205|  0:00:01s\n",
      "epoch 19 | loss: 3.92862 | val_0_unsup_loss_numpy: 1.9733699560165405|  0:00:01s\n",
      "epoch 20 | loss: 3.80728 | val_0_unsup_loss_numpy: 1.9179600477218628|  0:00:01s\n",
      "epoch 21 | loss: 3.24994 | val_0_unsup_loss_numpy: 2.3341100215911865|  0:00:01s\n",
      "epoch 22 | loss: 2.89993 | val_0_unsup_loss_numpy: 2.12362003326416|  0:00:01s\n",
      "epoch 23 | loss: 2.91079 | val_0_unsup_loss_numpy: 1.691100001335144|  0:00:01s\n",
      "epoch 24 | loss: 2.72789 | val_0_unsup_loss_numpy: 1.6399999856948853|  0:00:01s\n",
      "epoch 25 | loss: 2.48905 | val_0_unsup_loss_numpy: 2.0787200927734375|  0:00:01s\n",
      "epoch 26 | loss: 2.43139 | val_0_unsup_loss_numpy: 1.5094399452209473|  0:00:02s\n",
      "epoch 27 | loss: 2.13629 | val_0_unsup_loss_numpy: 1.679010033607483|  0:00:02s\n",
      "epoch 28 | loss: 2.16102 | val_0_unsup_loss_numpy: 1.562059998512268|  0:00:02s\n",
      "epoch 29 | loss: 2.0904  | val_0_unsup_loss_numpy: 1.5310200452804565|  0:00:02s\n",
      "epoch 30 | loss: 1.96816 | val_0_unsup_loss_numpy: 1.4540200233459473|  0:00:02s\n",
      "epoch 31 | loss: 1.88596 | val_0_unsup_loss_numpy: 1.5820200443267822|  0:00:02s\n",
      "epoch 32 | loss: 1.87202 | val_0_unsup_loss_numpy: 1.3168799877166748|  0:00:02s\n",
      "epoch 33 | loss: 1.74688 | val_0_unsup_loss_numpy: 1.6402699947357178|  0:00:02s\n",
      "epoch 34 | loss: 1.75846 | val_0_unsup_loss_numpy: 1.3742799758911133|  0:00:02s\n",
      "epoch 35 | loss: 1.72547 | val_0_unsup_loss_numpy: 1.322510004043579|  0:00:02s\n",
      "epoch 36 | loss: 1.59756 | val_0_unsup_loss_numpy: 1.490880012512207|  0:00:02s\n",
      "epoch 37 | loss: 1.70145 | val_0_unsup_loss_numpy: 1.3440699577331543|  0:00:02s\n",
      "epoch 38 | loss: 1.59377 | val_0_unsup_loss_numpy: 1.4196300506591797|  0:00:02s\n",
      "epoch 39 | loss: 1.58991 | val_0_unsup_loss_numpy: 1.3333899974822998|  0:00:02s\n",
      "epoch 40 | loss: 1.53307 | val_0_unsup_loss_numpy: 1.3366999626159668|  0:00:02s\n",
      "epoch 41 | loss: 1.52917 | val_0_unsup_loss_numpy: 1.254390001296997|  0:00:03s\n",
      "epoch 42 | loss: 1.53173 | val_0_unsup_loss_numpy: 1.2077900171279907|  0:00:03s\n",
      "epoch 43 | loss: 1.41413 | val_0_unsup_loss_numpy: 1.4259300231933594|  0:00:03s\n",
      "epoch 44 | loss: 1.47165 | val_0_unsup_loss_numpy: 1.2006399631500244|  0:00:03s\n",
      "epoch 45 | loss: 1.41687 | val_0_unsup_loss_numpy: 1.3009699583053589|  0:00:03s\n",
      "epoch 46 | loss: 1.43526 | val_0_unsup_loss_numpy: 1.2374999523162842|  0:00:03s\n",
      "epoch 47 | loss: 1.39662 | val_0_unsup_loss_numpy: 1.2648099660873413|  0:00:03s\n",
      "epoch 48 | loss: 1.394   | val_0_unsup_loss_numpy: 1.2750400304794312|  0:00:03s\n",
      "epoch 49 | loss: 1.4119  | val_0_unsup_loss_numpy: 1.213819980621338|  0:00:03s\n",
      "epoch 50 | loss: 1.36291 | val_0_unsup_loss_numpy: 1.2600200176239014|  0:00:03s\n",
      "epoch 51 | loss: 1.38891 | val_0_unsup_loss_numpy: 1.1727399826049805|  0:00:03s\n",
      "epoch 52 | loss: 1.37011 | val_0_unsup_loss_numpy: 1.299630045890808|  0:00:03s\n",
      "epoch 53 | loss: 1.3592  | val_0_unsup_loss_numpy: 1.1625200510025024|  0:00:03s\n",
      "epoch 54 | loss: 1.33187 | val_0_unsup_loss_numpy: 1.2187299728393555|  0:00:03s\n",
      "epoch 55 | loss: 1.35753 | val_0_unsup_loss_numpy: 1.1221599578857422|  0:00:03s\n",
      "epoch 56 | loss: 1.30457 | val_0_unsup_loss_numpy: 1.2200599908828735|  0:00:04s\n",
      "epoch 57 | loss: 1.33543 | val_0_unsup_loss_numpy: 1.166409969329834|  0:00:04s\n",
      "epoch 58 | loss: 1.33443 | val_0_unsup_loss_numpy: 1.1287800073623657|  0:00:04s\n",
      "epoch 59 | loss: 1.28638 | val_0_unsup_loss_numpy: 1.197890043258667|  0:00:04s\n",
      "epoch 60 | loss: 1.32196 | val_0_unsup_loss_numpy: 1.229349970817566|  0:00:04s\n",
      "epoch 61 | loss: 1.35968 | val_0_unsup_loss_numpy: 1.154520034790039|  0:00:04s\n",
      "epoch 62 | loss: 1.29381 | val_0_unsup_loss_numpy: 1.2559599876403809|  0:00:04s\n",
      "epoch 63 | loss: 1.29532 | val_0_unsup_loss_numpy: 1.22639000415802|  0:00:04s\n",
      "epoch 64 | loss: 1.30468 | val_0_unsup_loss_numpy: 1.206089973449707|  0:00:04s\n",
      "epoch 65 | loss: 1.31049 | val_0_unsup_loss_numpy: 1.12336003780365|  0:00:04s\n",
      "\n",
      "Early stopping occurred at epoch 65 with best_epoch = 55 and best_val_0_unsup_loss_numpy = 1.1221599578857422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\miniconda3\\envs\\pytorch\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    }
   ],
   "source": [
    "tabnet_params = {\n",
    "    \"n_d\": 512,\n",
    "    \"n_a\": 512,\n",
    "    \"n_steps\": 3,\n",
    "    \"n_shared\": 2,\n",
    "    \"n_independent\": 2,\n",
    "    \"gamma\": 1.3,\n",
    "    \"epsilon\": 1e-15,\n",
    "    \"momentum\": 0.98,\n",
    "    \"mask_type\": \"sparsemax\",\n",
    "    \"lambda_sparse\": 1e-3,\n",
    "    \"device_name\": \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "}\n",
    "\n",
    "\n",
    "unsupervised_model = TabNetPretrainer(\n",
    "    optimizer_fn=torch.optim.Adam,\n",
    "    optimizer_params=dict(lr=2e-2),\n",
    "    **tabnet_params\n",
    ")\n",
    " \n",
    "\n",
    "unsupervised_model.fit(\n",
    "    X_train,\n",
    "    eval_set=[X_test],  \n",
    "    pretraining_ratio=0.8,\n",
    "    max_epochs=101,\n",
    "    patience=10,\n",
    "    batch_size=1024,\n",
    "    virtual_batch_size=128,\n",
    "    num_workers=0,\n",
    "    drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==============================================================================================================\n",
       "Layer (type:depth-idx)                                       Output Shape              Param #\n",
       "==============================================================================================================\n",
       "TabNetPretraining                                            [700, 42]                 --\n",
       "├─EmbeddingGenerator: 1-1                                    [700, 42]                 --\n",
       "├─TabNetEncoder: 1-2                                         [700, 512]                --\n",
       "│    └─BatchNorm1d: 2-1                                      [700, 42]                 84\n",
       "│    └─FeatTransformer: 2-2                                  [700, 1024]               4,202,496\n",
       "│    │    └─GLU_Block: 3-1                                   [700, 1024]               2,191,360\n",
       "│    └─ModuleList: 2-12                                      --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-17                            --                        (recursive)\n",
       "│    └─FeatTransformer: 2-6                                  --                        (recursive)\n",
       "│    │    └─GLU_Block: 3-5                                   --                        (recursive)\n",
       "│    └─ModuleList: 2-12                                      --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-17                            --                        (recursive)\n",
       "│    └─FeatTransformer: 2-6                                  --                        (recursive)\n",
       "│    │    └─GLU_Block: 3-5                                   --                        (recursive)\n",
       "│    │    └─GLU_Block: 3-6                                   [700, 1024]               4,202,496\n",
       "│    └─ModuleList: 2-11                                      --                        (recursive)\n",
       "│    │    └─AttentiveTransformer: 3-7                        [700, 42]                 21,588\n",
       "│    └─ModuleList: 2-12                                      --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-8                             [700, 1024]               6,393,856\n",
       "│    │    └─FeatTransformer: 3-17                            --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-12                            --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-17                            --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-12                            --                        (recursive)\n",
       "│    └─ModuleList: 2-11                                      --                        (recursive)\n",
       "│    │    └─AttentiveTransformer: 3-13                       [700, 42]                 21,588\n",
       "│    └─ModuleList: 2-12                                      --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-14                            [700, 1024]               6,393,856\n",
       "│    │    └─FeatTransformer: 3-17                            --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-18                            --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-17                            --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-18                            --                        (recursive)\n",
       "│    └─ModuleList: 2-11                                      --                        (recursive)\n",
       "│    │    └─AttentiveTransformer: 3-19                       [700, 42]                 21,588\n",
       "│    └─ModuleList: 2-12                                      --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-20                            [700, 1024]               6,393,856\n",
       "├─TabNetDecoder: 1-3                                         [700, 42]                 --\n",
       "│    └─ModuleList: 2-13                                      --                        --\n",
       "│    │    └─FeatTransformer: 3-21                            [700, 512]                1,052,672\n",
       "│    │    └─FeatTransformer: 3-25                            --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-23                            --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-24                            [700, 512]                1,052,672\n",
       "│    │    └─FeatTransformer: 3-25                            --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-26                            --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-27                            [700, 512]                1,052,672\n",
       "│    └─Linear: 2-14                                          [700, 42]                 21,504\n",
       "==============================================================================================================\n",
       "Total params: 51,519,824\n",
       "Trainable params: 51,519,824\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 20.17\n",
       "==============================================================================================================\n",
       "Input size (MB): 0.12\n",
       "Forward/backward pass size (MB): 437.70\n",
       "Params size (MB): 84.89\n",
       "Estimated Total Size (MB): 522.70\n",
       "=============================================================================================================="
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Truy cập vào mô hình TabNet bên trong\n",
    "from torchinfo import summary\n",
    "\n",
    "tabnet_model = unsupervised_model.network.to(device)\n",
    "\n",
    "summary(tabnet_model, input_size=X_train.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Encoder Summary:\n",
      "TabNetEncoder(\n",
      "  (initial_bn): BatchNorm1d(42, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  (initial_splitter): FeatTransformer(\n",
      "    (shared): GLU_Block(\n",
      "      (shared_layers): ModuleList(\n",
      "        (0): Linear(in_features=42, out_features=2048, bias=False)\n",
      "        (1): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "      )\n",
      "      (glu_layers): ModuleList(\n",
      "        (0): GLU_Layer(\n",
      "          (fc): Linear(in_features=42, out_features=2048, bias=False)\n",
      "          (bn): GBN(\n",
      "            (bn): BatchNorm1d(2048, eps=1e-05, momentum=0.98, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): GLU_Layer(\n",
      "          (fc): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "          (bn): GBN(\n",
      "            (bn): BatchNorm1d(2048, eps=1e-05, momentum=0.98, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (specifics): GLU_Block(\n",
      "      (glu_layers): ModuleList(\n",
      "        (0-1): 2 x GLU_Layer(\n",
      "          (fc): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "          (bn): GBN(\n",
      "            (bn): BatchNorm1d(2048, eps=1e-05, momentum=0.98, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (feat_transformers): ModuleList(\n",
      "    (0-2): 3 x FeatTransformer(\n",
      "      (shared): GLU_Block(\n",
      "        (shared_layers): ModuleList(\n",
      "          (0): Linear(in_features=42, out_features=2048, bias=False)\n",
      "          (1): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "        )\n",
      "        (glu_layers): ModuleList(\n",
      "          (0): GLU_Layer(\n",
      "            (fc): Linear(in_features=42, out_features=2048, bias=False)\n",
      "            (bn): GBN(\n",
      "              (bn): BatchNorm1d(2048, eps=1e-05, momentum=0.98, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (1): GLU_Layer(\n",
      "            (fc): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "            (bn): GBN(\n",
      "              (bn): BatchNorm1d(2048, eps=1e-05, momentum=0.98, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (specifics): GLU_Block(\n",
      "        (glu_layers): ModuleList(\n",
      "          (0-1): 2 x GLU_Layer(\n",
      "            (fc): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "            (bn): GBN(\n",
      "              (bn): BatchNorm1d(2048, eps=1e-05, momentum=0.98, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (att_transformers): ModuleList(\n",
      "    (0-2): 3 x AttentiveTransformer(\n",
      "      (fc): Linear(in_features=512, out_features=42, bias=False)\n",
      "      (bn): GBN(\n",
      "        (bn): BatchNorm1d(42, eps=1e-05, momentum=0.98, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (selector): Sparsemax()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "encoder = tabnet_model.encoder\n",
    "\n",
    "print(\"\\nEncoder Summary:\")\n",
    "print(encoder)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decoder Summary:\n",
      "TabNetDecoder(\n",
      "  (feat_transformers): ModuleList(\n",
      "    (0-2): 3 x FeatTransformer(\n",
      "      (shared): GLU_Block(\n",
      "        (shared_layers): ModuleList(\n",
      "          (0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        )\n",
      "        (glu_layers): ModuleList(\n",
      "          (0): GLU_Layer(\n",
      "            (fc): Linear(in_features=512, out_features=1024, bias=False)\n",
      "            (bn): GBN(\n",
      "              (bn): BatchNorm1d(1024, eps=1e-05, momentum=0.98, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (specifics): GLU_Block(\n",
      "        (glu_layers): ModuleList(\n",
      "          (0): GLU_Layer(\n",
      "            (fc): Linear(in_features=512, out_features=1024, bias=False)\n",
      "            (bn): GBN(\n",
      "              (bn): BatchNorm1d(1024, eps=1e-05, momentum=0.98, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (reconstruction_layer): Linear(in_features=512, out_features=42, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "decoder = tabnet_model.decoder\n",
    "\n",
    "print(\"\\nDecoder Summary:\")\n",
    "print(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TabNet encoder trả về 2 giá trị.\n",
      "Đã xảy ra lỗi: 'list' object has no attribute 'shape'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_7196\\3356785120.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sample_input = torch.tensor(X_train[:5]).to(device)\n"
     ]
    }
   ],
   "source": [
    "sample_input = torch.tensor(X_train[:5]).to(device)  \n",
    "\n",
    "try:\n",
    "    result = tabnet_model.encoder(sample_input)\n",
    "    if isinstance(result, tuple):\n",
    "        print(f'TabNet encoder trả về {len(result)} giá trị.')\n",
    "        for i, res in enumerate(result):\n",
    "            print(f'Giá trị {i + 1} shape: {res.shape}')\n",
    "    else:\n",
    "        print('TabNet encoder chỉ trả về một giá trị.')\n",
    "        print(f'Giá trị shape: {result.shape}')\n",
    "except Exception as e:\n",
    "    print(f'Đã xảy ra lỗi: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(nn.Module):\n",
    "    def __init__(self, seed=1337):\n",
    "        super(Sampling, self).__init__()\n",
    "        self.seed = seed\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = z_mean.size(0)\n",
    "        dim = z_mean.size(1)\n",
    "        # print(batch, dim)\n",
    "        epsilon = torch.randn(batch, dim, generator=torch.Generator().manual_seed(self.seed)).to(device)\n",
    "        return z_mean + torch.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE_Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(VAE_Encoder, self).__init__()\n",
    "        self.tabnet_encoder = tabnet_model.encoder\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(512, 256),  \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 96),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(96, 96),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(96, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, latent_dim)\n",
    "        ).to(device)\n",
    "        self.fc_mean = nn.Linear(latent_dim, latent_dim).to(device)\n",
    "        self.fc_log_var = nn.Linear(latent_dim, latent_dim).to(device)\n",
    "        self.sampling = Sampling().to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        steps_output, _ = self.tabnet_encoder(x)\n",
    "        encoded = steps_output[-1]\n",
    "        # print(\"Shape of encoded tensor:\", encoded.shape)\n",
    "        encoded = self.mlp(encoded)\n",
    "        z_mean = self.fc_mean(encoded)\n",
    "        z_log_var = self.fc_log_var(encoded)\n",
    "        z = self.sampling((z_mean, z_log_var))\n",
    "        # print(f'Shape of z: {z.shape} - {z_log_var.shape} -{z_log_var.shape}')\n",
    "        return z_mean, z_log_var, z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE_Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim,encoded_dim, output_dim):\n",
    "        super(VAE_Decoder, self).__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 32),   \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 96),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(96, 96),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(96, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, encoded_dim),  \n",
    "        )\n",
    "        self.tabnet_decoder = tabnet_model.decoder\n",
    "        self.reshape = nn.Unflatten(1, (encoded_dim,))\n",
    "        self.output_dim=output_dim\n",
    "\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = F.relu(self.mlp(z))\n",
    "\n",
    "        print(\"Shape before reshape:\", x.shape)\n",
    "        x = self.reshape(x)\n",
    "        x = x[None, ...]\n",
    "\n",
    "        print(\"Shape after reshape:\", x.shape)\n",
    "        # x = x.view(x.size(0), output_dim)\n",
    "        \n",
    "        output = self.tabnet_decoder(x)\n",
    "        # print(output.shape)\n",
    "        # print(\"Shape of output from tabnet_decoder:\", output.shape)\n",
    "        output = torch.softmax(output, dim=-1)  # Assuming the output is a probability distribution\n",
    "        output = output.view(-1, self.output_dim)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data_range(tensor, name):\n",
    "    if not torch.all((tensor >= 0) & (tensor <= 1)):\n",
    "        print(f\"{name} contains values outside the range [0, 1]\")\n",
    "        print(f\"{name} min: {tensor.min()}, max: {tensor.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE_Tabnet_MLPS(nn.Module):\n",
    "    def __init__(self, encoder, decoder, classifier):\n",
    "        super(VAE_Tabnet_MLPS, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.classifier = classifier\n",
    "        self.total_loss_tracker = []\n",
    "        self.reconstruction_loss_tracker = []\n",
    "        self.kl_loss_tracker = []\n",
    "        self.classification_loss_tracker = []\n",
    "        self.accuracy_tracker = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        z_mean, z_log_var, z = self.encoder(x)\n",
    "        reconstruction = self.decoder(z)\n",
    "        classification_output = self.classifier(z)\n",
    "        return reconstruction, z_mean, z_log_var, classification_output\n",
    "\n",
    "    def train_step(self, data, labels, optimizer):\n",
    "        labels = torch.argmax(labels, dim=1) \n",
    "        optimizer.zero_grad()\n",
    "        # z_mean, z_log_var, z = self.encoder(data)\n",
    "        # reconstruction = self.decoder(z)\n",
    "        reconstruction, z_mean, z_log_var, classification_output = self.forward(data)\n",
    "        # print('classifi',classification_output.shape)\n",
    "        # print(check_data_range(data, 'data'))\n",
    "        # print(check_data_range(reconstruction, 'reconstruction'))\n",
    "        # reconstruction_loss = torch.mean(\n",
    "        #     torch.sum(\n",
    "        #         F.binary_cross_entropy(reconstruction, data, reduction='none'),\n",
    "        #         dim=1\n",
    "        #     )\n",
    "        # )\n",
    "        reconstruction_loss = torch.mean(\n",
    "            torch.sum(\n",
    "                F.binary_cross_entropy_with_logits(reconstruction, data, reduction='none'),\n",
    "                dim=1\n",
    "                # dim=(1, 2)\n",
    "                )  \n",
    "        )\n",
    "        classification_loss = torch.mean(\n",
    "            torch.sum(\n",
    "                F.cross_entropy(classification_output, labels, reduction='none'),\n",
    "                # dim=1\n",
    "                # dim=(1, 2)\n",
    "                )  \n",
    "        )\n",
    "        kl_loss = -0.5 * torch.sum(1 + z_log_var - z_mean.pow(2) - z_log_var.exp(), dim=1)\n",
    "        kl_loss = torch.mean(torch.sum(kl_loss))\n",
    "        total_loss = reconstruction_loss + kl_loss + classification_loss\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        self.total_loss_tracker.append(total_loss.item())\n",
    "        self.reconstruction_loss_tracker.append(reconstruction_loss.item())\n",
    "        self.kl_loss_tracker.append(kl_loss.item())\n",
    "        self.classification_loss_tracker.append(classification_loss.item())\n",
    "\n",
    "        preds = torch.softmax(classification_output)\n",
    "        correct = (preds == labels).float().sum()\n",
    "        accuracy = correct / labels.size(0)\n",
    "        self.accuracy_tracker.append(accuracy.item())\n",
    "\n",
    "        return {\n",
    "            \"loss\": total_loss.item(),\n",
    "            \"reconstruction_loss\": reconstruction_loss.item(),\n",
    "            \"kl_loss\": kl_loss.item(),\n",
    "            \"classification_loss\": classification_loss.item(),\n",
    "            \"accuracy\": accuracy.item()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 64\n",
    "encoded_dim = 512\n",
    "output_dim = X_train.shape[1]\n",
    "input_dim = X_train.shape[1]\n",
    "print(input_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(SimpleClassifier, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, output_dim),\n",
    "            nn.Softmax()  \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print('input: ',x.shape)\n",
    "        output = self.fc(x)\n",
    "        output = output.view(-1)\n",
    "        # print('output',output.shape)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SimpleClassifier(latent_dim, output_dim=1).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input size: torch.Size([32, 64])\n",
      "Output size: torch.Size([32])\n",
      "Output: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\miniconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "def check_output(model, input_tensor):\n",
    "    with torch.no_grad():  \n",
    "        output = model(input_tensor)\n",
    "        print(f\"Input size: {input_tensor.size()}\")\n",
    "        print(f\"Output size: {output.size()}\")\n",
    "        print(f\"Output: {output}\")\n",
    "\n",
    "model = SimpleClassifier(latent_dim, output_dim=1)\n",
    "\n",
    "input_tensor = torch.randn(32,latent_dim)  \n",
    "\n",
    "check_output(model, input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Summary:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==============================================================================================================\n",
       "Layer (type:depth-idx)                                       Output Shape              Param #\n",
       "==============================================================================================================\n",
       "VAE_Encoder                                                  [32, 64]                  --\n",
       "├─TabNetEncoder: 1-1                                         [32, 512]                 --\n",
       "│    └─BatchNorm1d: 2-1                                      [32, 42]                  84\n",
       "│    └─FeatTransformer: 2-2                                  [32, 1024]                4,202,496\n",
       "│    │    └─GLU_Block: 3-1                                   [32, 1024]                2,191,360\n",
       "│    └─ModuleList: 2-12                                      --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-17                            --                        (recursive)\n",
       "│    └─FeatTransformer: 2-6                                  --                        (recursive)\n",
       "│    │    └─GLU_Block: 3-5                                   --                        (recursive)\n",
       "│    └─ModuleList: 2-12                                      --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-17                            --                        (recursive)\n",
       "│    └─FeatTransformer: 2-6                                  --                        (recursive)\n",
       "│    │    └─GLU_Block: 3-5                                   --                        (recursive)\n",
       "│    │    └─GLU_Block: 3-6                                   [32, 1024]                4,202,496\n",
       "│    └─ModuleList: 2-11                                      --                        (recursive)\n",
       "│    │    └─AttentiveTransformer: 3-7                        [32, 42]                  21,588\n",
       "│    └─ModuleList: 2-12                                      --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-8                             [32, 1024]                6,393,856\n",
       "│    │    └─FeatTransformer: 3-17                            --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-12                            --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-17                            --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-12                            --                        (recursive)\n",
       "│    └─ModuleList: 2-11                                      --                        (recursive)\n",
       "│    │    └─AttentiveTransformer: 3-13                       [32, 42]                  21,588\n",
       "│    └─ModuleList: 2-12                                      --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-14                            [32, 1024]                6,393,856\n",
       "│    │    └─FeatTransformer: 3-17                            --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-18                            --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-17                            --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-18                            --                        (recursive)\n",
       "│    └─ModuleList: 2-11                                      --                        (recursive)\n",
       "│    │    └─AttentiveTransformer: 3-19                       [32, 42]                  21,588\n",
       "│    └─ModuleList: 2-12                                      --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-20                            [32, 1024]                6,393,856\n",
       "├─Sequential: 1-2                                            [32, 64]                  --\n",
       "│    └─Linear: 2-13                                          [32, 256]                 131,328\n",
       "│    └─ReLU: 2-14                                            [32, 256]                 --\n",
       "│    └─Linear: 2-15                                          [32, 128]                 32,896\n",
       "│    └─ReLU: 2-16                                            [32, 128]                 --\n",
       "│    └─Linear: 2-17                                          [32, 96]                  12,384\n",
       "│    └─ReLU: 2-18                                            [32, 96]                  --\n",
       "│    └─Linear: 2-19                                          [32, 96]                  9,312\n",
       "│    └─ReLU: 2-20                                            [32, 96]                  --\n",
       "│    └─Linear: 2-21                                          [32, 32]                  3,104\n",
       "│    └─ReLU: 2-22                                            [32, 32]                  --\n",
       "│    └─Linear: 2-23                                          [32, 64]                  2,112\n",
       "├─Linear: 1-3                                                [32, 64]                  4,160\n",
       "├─Linear: 1-4                                                [32, 64]                  4,160\n",
       "├─Sampling: 1-5                                              [32, 64]                  --\n",
       "==============================================================================================================\n",
       "Total params: 46,958,704\n",
       "Trainable params: 46,958,704\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 826.87\n",
       "==============================================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 17.06\n",
       "Params size (MB): 77.16\n",
       "Estimated Total Size (MB): 94.22\n",
       "=============================================================================================================="
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae_encoder = VAE_Encoder(latent_dim=latent_dim)\n",
    "print(\"Encoder Summary:\")\n",
    "# vae_encoder.to(device)\n",
    "\n",
    "summary(vae_encoder, input_size=(32, input_dim), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded shape: torch.Size([800, 512])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(800, 42).to(device)\n",
    "steps_output, _ = tabnet_model.encoder(x)\n",
    "encoded = steps_output[-1]\n",
    "print(f\"Encoded shape: {encoded.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of encoder output: [torch.Size([800, 512]), torch.Size([800, 512]), torch.Size([800, 512])]\n",
      "Decoder shape: torch.Size([800, 42])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.randn(800, 42).to(device)  # Đầu vào có kích thước (batch_size, features)\n",
    "\n",
    "steps_output, _ = tabnet_model.encoder(x)\n",
    "print(\"Shape of encoder output:\", [output.shape for output in steps_output])\n",
    "\n",
    "decoder_input = steps_output[-1]  \n",
    "decoder_input = decoder_input[None, ...]\n",
    "try:\n",
    "    decoder_output = tabnet_model.decoder(decoder_input)\n",
    "    print(f\"Decoder shape: {decoder_output.shape}\")\n",
    "except ValueError as e:\n",
    "    print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 42)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dim, output_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder Summary:\n",
      "Shape before reshape: torch.Size([32, 512])\n",
      "Shape after reshape: torch.Size([1, 32, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==============================================================================================================\n",
       "Layer (type:depth-idx)                                       Output Shape              Param #\n",
       "==============================================================================================================\n",
       "VAE_Decoder                                                  [32, 42]                  --\n",
       "├─Sequential: 1-1                                            [32, 512]                 --\n",
       "│    └─Linear: 2-1                                           [32, 32]                  2,080\n",
       "│    └─ReLU: 2-2                                             [32, 32]                  --\n",
       "│    └─Linear: 2-3                                           [32, 96]                  3,168\n",
       "│    └─ReLU: 2-4                                             [32, 96]                  --\n",
       "│    └─Linear: 2-5                                           [32, 96]                  9,312\n",
       "│    └─ReLU: 2-6                                             [32, 96]                  --\n",
       "│    └─Linear: 2-7                                           [32, 128]                 12,416\n",
       "│    └─ReLU: 2-8                                             [32, 128]                 --\n",
       "│    └─Linear: 2-9                                           [32, 256]                 33,024\n",
       "│    └─ReLU: 2-10                                            [32, 256]                 --\n",
       "│    └─Linear: 2-11                                          [32, 512]                 131,584\n",
       "├─Unflatten: 1-2                                             [32, 512]                 --\n",
       "├─TabNetDecoder: 1-3                                         [32, 42]                  --\n",
       "│    └─ModuleList: 2-12                                      --                        --\n",
       "│    │    └─FeatTransformer: 3-1                             [32, 512]                 1,052,672\n",
       "│    │    └─FeatTransformer: 3-2                             --                        1,052,672\n",
       "│    │    └─FeatTransformer: 3-3                             --                        (recursive)\n",
       "│    └─Linear: 2-13                                          [32, 42]                  21,504\n",
       "==============================================================================================================\n",
       "Total params: 2,846,816\n",
       "Trainable params: 2,846,816\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 40.50\n",
       "==============================================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 1.35\n",
       "Params size (MB): 5.06\n",
       "Estimated Total Size (MB): 6.42\n",
       "=============================================================================================================="
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae_decoder = VAE_Decoder(latent_dim=latent_dim, encoded_dim=encoded_dim, output_dim=output_dim).to(device)\n",
    "print(\"Decoder Summary:\")\n",
    "summary(vae_decoder, input_size=(32, latent_dim), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before reshape: torch.Size([32, 512])\n",
      "Shape after reshape: torch.Size([1, 32, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "===================================================================================================================\n",
       "Layer (type:depth-idx)                                            Output Shape              Param #\n",
       "===================================================================================================================\n",
       "VAE_Tabnet_MLPS                                                   [32, 42]                  --\n",
       "├─VAE_Encoder: 1-1                                                [32, 64]                  --\n",
       "│    └─TabNetEncoder: 2-1                                         [32, 512]                 --\n",
       "│    │    └─BatchNorm1d: 3-1                                      [32, 42]                  84\n",
       "│    │    └─FeatTransformer: 3-2                                  [32, 1024]                6,393,856\n",
       "│    │    └─ModuleList: 3-12                                      --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-6                                  --                        (recursive)\n",
       "│    │    └─ModuleList: 3-12                                      --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-6                                  --                        (recursive)\n",
       "│    │    └─ModuleList: 3-11                                      --                        (recursive)\n",
       "│    │    └─ModuleList: 3-12                                      --                        (recursive)\n",
       "│    │    └─ModuleList: 3-11                                      --                        (recursive)\n",
       "│    │    └─ModuleList: 3-12                                      --                        (recursive)\n",
       "│    │    └─ModuleList: 3-11                                      --                        (recursive)\n",
       "│    │    └─ModuleList: 3-12                                      --                        (recursive)\n",
       "│    └─Sequential: 2-2                                            [32, 64]                  --\n",
       "│    │    └─Linear: 3-13                                          [32, 256]                 131,328\n",
       "│    │    └─ReLU: 3-14                                            [32, 256]                 --\n",
       "│    │    └─Linear: 3-15                                          [32, 128]                 32,896\n",
       "│    │    └─ReLU: 3-16                                            [32, 128]                 --\n",
       "│    │    └─Linear: 3-17                                          [32, 96]                  12,384\n",
       "│    │    └─ReLU: 3-18                                            [32, 96]                  --\n",
       "│    │    └─Linear: 3-19                                          [32, 96]                  9,312\n",
       "│    │    └─ReLU: 3-20                                            [32, 96]                  --\n",
       "│    │    └─Linear: 3-21                                          [32, 32]                  3,104\n",
       "│    │    └─ReLU: 3-22                                            [32, 32]                  --\n",
       "│    │    └─Linear: 3-23                                          [32, 64]                  2,112\n",
       "│    └─Linear: 2-3                                                [32, 64]                  4,160\n",
       "│    └─Linear: 2-4                                                [32, 64]                  4,160\n",
       "│    └─Sampling: 2-5                                              [32, 64]                  --\n",
       "├─VAE_Decoder: 1-2                                                [32, 42]                  --\n",
       "│    └─Sequential: 2-6                                            [32, 512]                 --\n",
       "│    │    └─Linear: 3-24                                          [32, 32]                  2,080\n",
       "│    │    └─ReLU: 3-25                                            [32, 32]                  --\n",
       "│    │    └─Linear: 3-26                                          [32, 96]                  3,168\n",
       "│    │    └─ReLU: 3-27                                            [32, 96]                  --\n",
       "│    │    └─Linear: 3-28                                          [32, 96]                  9,312\n",
       "│    │    └─ReLU: 3-29                                            [32, 96]                  --\n",
       "│    │    └─Linear: 3-30                                          [32, 128]                 12,416\n",
       "│    │    └─ReLU: 3-31                                            [32, 128]                 --\n",
       "│    │    └─Linear: 3-32                                          [32, 256]                 33,024\n",
       "│    │    └─ReLU: 3-33                                            [32, 256]                 --\n",
       "│    │    └─Linear: 3-34                                          [32, 512]                 131,584\n",
       "│    └─Unflatten: 2-7                                             [32, 512]                 --\n",
       "│    └─TabNetDecoder: 2-8                                         [32, 42]                  --\n",
       "│    │    └─ModuleList: 3-35                                      --                        2,109,440\n",
       "│    │    └─Linear: 3-36                                          [32, 42]                  21,504\n",
       "├─SimpleClassifier: 1-3                                           [32]                      --\n",
       "│    └─Sequential: 2-9                                            [32, 1]                   --\n",
       "│    │    └─Linear: 3-37                                          [32, 64]                  4,160\n",
       "│    │    └─ReLU: 3-38                                            [32, 64]                  --\n",
       "│    │    └─Linear: 3-39                                          [32, 32]                  2,080\n",
       "│    │    └─ReLU: 3-40                                            [32, 32]                  --\n",
       "│    │    └─Linear: 3-41                                          [32, 1]                   33\n",
       "│    │    └─Softmax: 3-42                                         [32, 1]                   --\n",
       "===================================================================================================================\n",
       "Total params: 49,811,793\n",
       "Trainable params: 49,811,793\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 867.58\n",
       "===================================================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 18.43\n",
       "Params size (MB): 82.25\n",
       "Estimated Total Size (MB): 100.68\n",
       "==================================================================================================================="
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae = VAE_Tabnet_MLPS(encoder=vae_encoder, decoder=vae_decoder,classifier=classifier).to(device)\n",
    "summary(vae, input_size=(32, input_dim), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch data shape: torch.Size([32, 42]), Batch labels shape: torch.Size([32])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[106], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m batch_labels \u001b[38;5;241m=\u001b[39m batch_labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch data shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_data\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Batch labels shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_labels\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 17\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mvae\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     20\u001b[0m rec_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreconstruction_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "Cell \u001b[1;32mIn[94], line 20\u001b[0m, in \u001b[0;36mVAE_Tabnet_MLPS.train_step\u001b[1;34m(self, data, labels, optimizer)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, data, labels, optimizer):\n\u001b[1;32m---> 20\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \n\u001b[0;32m     21\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m# z_mean, z_log_var, z = self.encoder(data)\u001b[39;00m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;66;03m# reconstruction = self.decoder(z)\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.0001\n",
    "optimizer = optim.Adam(vae.parameters(), lr=learning_rate)\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    vae.train()\n",
    "    train_loss = 0\n",
    "    rec_loss = 0\n",
    "    kl_loss = 0\n",
    "    classification_loss = 0\n",
    "    accuracy = 0\n",
    "\n",
    "    for batch_data, batch_labels in train_loader:\n",
    "        batch_data = batch_data.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "        print(f\"Batch data shape: {batch_data.shape}, Batch labels shape: {batch_labels.shape}\")\n",
    "        results = vae.train_step(batch_data, batch_labels, optimizer)\n",
    "        \n",
    "        train_loss += results[\"loss\"]\n",
    "        rec_loss += results[\"reconstruction_loss\"]\n",
    "        kl_loss += results[\"kl_loss\"]\n",
    "        classification_loss += results[\"classification_loss\"]\n",
    "        accuracy += results[\"accuracy\"]\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    rec_loss /= len(train_loader)\n",
    "    kl_loss /= len(train_loader)\n",
    "    classification_loss /= len(train_loader)\n",
    "    accuracy /= len(train_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {train_loss:.4f}, Reconstruction Loss: {rec_loss:.4f}, KL Loss: {kl_loss:.4f}, Classification Loss: {classification_loss:.4f}, Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in vae.encoder.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 53.1120, Reconstruction Loss: 30.4644, KL Loss: 0.0400, Classification Loss: 22.6076, Accuracy: 0.4992\n",
      "Epoch 2/10, Loss: 53.0209, Reconstruction Loss: 30.4537, KL Loss: 0.0387, Classification Loss: 22.5284, Accuracy: 0.5002\n",
      "Epoch 3/10, Loss: 52.9978, Reconstruction Loss: 30.4485, KL Loss: 0.0430, Classification Loss: 22.5062, Accuracy: 0.4990\n",
      "Epoch 4/10, Loss: 52.9031, Reconstruction Loss: 30.4347, KL Loss: 0.0404, Classification Loss: 22.4280, Accuracy: 0.4998\n",
      "Epoch 5/10, Loss: 52.9155, Reconstruction Loss: 30.4229, KL Loss: 0.0395, Classification Loss: 22.4531, Accuracy: 0.4998\n",
      "Epoch 6/10, Loss: 52.8807, Reconstruction Loss: 30.4122, KL Loss: 0.0409, Classification Loss: 22.4276, Accuracy: 0.5000\n",
      "Epoch 7/10, Loss: 52.8405, Reconstruction Loss: 30.4025, KL Loss: 0.0428, Classification Loss: 22.3952, Accuracy: 0.5002\n",
      "Epoch 8/10, Loss: 52.8140, Reconstruction Loss: 30.3933, KL Loss: 0.0420, Classification Loss: 22.3787, Accuracy: 0.5000\n",
      "Epoch 9/10, Loss: 52.7752, Reconstruction Loss: 30.3814, KL Loss: 0.0417, Classification Loss: 22.3520, Accuracy: 0.5006\n",
      "Epoch 10/10, Loss: 52.7125, Reconstruction Loss: 30.3661, KL Loss: 0.0423, Classification Loss: 22.3041, Accuracy: 0.4992\n"
     ]
    }
   ],
   "source": [
    "vae_new = VAE_Tabnet_MLPS(vae.encoder, vae.decoder, vae.classifier).to(device)\n",
    "for param in vae_new.encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, vae_new.parameters()), lr=learning_rate)\n",
    "for epoch in range(num_epochs):\n",
    "    vae_new.train()\n",
    "    train_loss = 0\n",
    "    rec_loss = 0\n",
    "    kl_loss = 0\n",
    "    classification_loss = 0\n",
    "    accuracy = 0\n",
    "\n",
    "    for batch_data, batch_labels in train_loader:\n",
    "        batch_data = batch_data.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "        results = vae.train_step(batch_data, batch_labels, optimizer)\n",
    "        \n",
    "        train_loss += results[\"loss\"]\n",
    "        rec_loss += results[\"reconstruction_loss\"]\n",
    "        kl_loss += results[\"kl_loss\"]\n",
    "        classification_loss += results[\"classification_loss\"]\n",
    "        accuracy += results[\"accuracy\"]\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    rec_loss /= len(train_loader)\n",
    "    kl_loss /= len(train_loader)\n",
    "    classification_loss /= len(train_loader)\n",
    "    accuracy /= len(train_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {train_loss:.4f}, Reconstruction Loss: {rec_loss:.4f}, KL Loss: {kl_loss:.4f}, Classification Loss: {classification_loss:.4f}, Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleDiffusionModel(nn.Module):\n",
    "    def __init__(self, latent_dim, time_steps=1000):\n",
    "        super().__init__()\n",
    "        self.time_steps = time_steps\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Tạo các beta_schedule tuyến tính\n",
    "        beta = torch.linspace(0.0001, 0.02, time_steps)\n",
    "        alpha = 1. - beta\n",
    "        alpha_bar = torch.cumprod(alpha, dim=0)\n",
    "\n",
    "        self.register_buffer('beta', beta)\n",
    "        self.register_buffer('alpha', alpha)\n",
    "        self.register_buffer('alpha_bar', alpha_bar)\n",
    "\n",
    "        # Mạng neural đơn giản để dự đoán nhiễu\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim + 1, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, latent_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, z, t):\n",
    "        noise = torch.randn_like(z)\n",
    "        \n",
    "        # Đảm bảo t là long type và shape phù hợp\n",
    "        if isinstance(t, torch.Tensor):\n",
    "            t = t.to(dtype=torch.long)\n",
    "        else:\n",
    "            t = torch.tensor([t], device=z.device, dtype=torch.long).expand(z.shape[0])\n",
    "\n",
    "        sqrt_alpha_bar = torch.sqrt(self.alpha_bar[t])[:, None]\n",
    "        sqrt_one_minus_alpha_bar = torch.sqrt(1 - self.alpha_bar[t])[:, None]\n",
    "        noisy_z = sqrt_alpha_bar * z + sqrt_one_minus_alpha_bar * noise\n",
    "\n",
    "        predicted_noise = self.model(torch.cat([noisy_z, t.unsqueeze(1)], dim=1))\n",
    "        loss = F.mse_loss(predicted_noise, noise)\n",
    "        return loss\n",
    "\n",
    "    def sample(self, num_samples):\n",
    "        z = torch.randn(num_samples, self.latent_dim).to(next(self.parameters()).device)\n",
    "        for i in reversed(range(self.time_steps)):\n",
    "            t = torch.full((num_samples,), i, device=z.device, dtype=torch.long)\n",
    "            z = self.denoise_step(z, t)\n",
    "        return z\n",
    "    \n",
    "    def denoise_step(self, z, t):\n",
    "        timestep = t.item() if isinstance(t, torch.Tensor) else t\n",
    "        t_batch = torch.full((z.shape[0],), timestep, device=z.device, dtype=torch.long)\n",
    "\n",
    "        predicted_noise = self.model(torch.cat([z, t_batch.unsqueeze(1)], dim=1))\n",
    "\n",
    "        alpha = self.alpha[timestep]\n",
    "        alpha_bar = self.alpha_bar[timestep]\n",
    "        beta = self.beta[timestep]\n",
    "\n",
    "        z = (1 / torch.sqrt(alpha)) * (z - ((1 - alpha) / torch.sqrt(1 - alpha_bar)) * predicted_noise)\n",
    "        if timestep > 0:\n",
    "            noise = torch.randn_like(z)\n",
    "            z += torch.sqrt(beta) * noise\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thay đoạn lỗi này:\n",
    "# latent_dim = vae.encoder[-1].out_features\n",
    "\n",
    "# Bằng đoạn này:\n",
    "with torch.no_grad():\n",
    "    vae_new.encoder.eval()\n",
    "    dummy_input = torch.randn(1, input_dim).to(device)  # Thay input_dim theo đúng dữ liệu của bạn\n",
    "    z_mean, z_log_var, _ = vae_new.encoder(dummy_input)\n",
    "    latent_dim = z_mean.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# latent_dim = vae.encoder[-1].out_features  # Kích thước latent z\n",
    "diffusion_model = SimpleDiffusionModel(latent_dim=latent_dim).to(device)\n",
    "diffusion_optimizer = torch.optim.Adam(diffusion_model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def train_diffusion(vae, diffusion_model, dataloader, optimizer, device, time_steps=1000, epochs=20):\n",
    "    diffusion_model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch_data, _ in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            batch_data = batch_data.to(device)\n",
    "            with torch.no_grad():\n",
    "                z_mean, z_log_var, z = vae.encoder(batch_data)\n",
    "\n",
    "            t = torch.randint(0, time_steps, (z.shape[0],), device=device).long()\n",
    "            loss = diffusion_model(z, t)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Diffusion Loss: {total_loss/len(dataloader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_diffusion_with_classifier(vae, diffusion_model, classifier, test_loader, device, time_steps=1000):\n",
    "    vae.eval()\n",
    "    diffusion_model.eval()\n",
    "    classifier.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_data, batch_labels in test_loader:\n",
    "            batch_data = batch_data.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "\n",
    "            # Lấy z từ encoder\n",
    "            z_mean, z_log_var, z = vae.encoder(batch_data)\n",
    "\n",
    "            # Forward diffusion\n",
    "            t_forward = time_steps - 1\n",
    "            sqrt_alpha_bar = torch.sqrt(diffusion_model.alpha_bar[t_forward])\n",
    "            sqrt_one_minus_alpha_bar = torch.sqrt(1 - diffusion_model.alpha_bar[t_forward])\n",
    "            noisy_z = sqrt_alpha_bar * z + sqrt_one_minus_alpha_bar * torch.randn_like(z)\n",
    "\n",
    "            # Reverse diffusion (hoàn nhiễu)\n",
    "            z_recovered = noisy_z\n",
    "            for t in reversed(range(time_steps)):\n",
    "                z_recovered = diffusion_model.denoise_step(z_recovered, t)\n",
    "\n",
    "            # Phân loại trên z đã hoàn nhiễu\n",
    "            logits = classifier(z_recovered)\n",
    "\n",
    "            # Kiểm tra shape của logits\n",
    "            print(\"Logits shape:\", logits.shape)  # Debug\n",
    "\n",
    "            if len(logits.shape) == 1:\n",
    "                # Trường hợp: binary classification với output shape [batch_size]\n",
    "                preds = (torch.sigmoid(logits) > 0.5).float()\n",
    "            elif len(logits.shape) == 2:\n",
    "                # Trường hợp: multi-class classification\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected logits shape: {logits.shape}\")\n",
    "\n",
    "            # Cập nhật accuracy\n",
    "            if len(batch_labels.shape) == 2 and batch_labels.shape[1] == 1:\n",
    "                batch_labels = batch_labels.squeeze(1)  # về shape [batch_size]\n",
    "\n",
    "            correct += (preds == batch_labels).sum().item()\n",
    "            total += batch_labels.size(0)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f\"Accuracy on recovered z: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████| 22/22 [00:00<00:00, 92.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Diffusion Loss: 42.7793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|██████████| 22/22 [00:00<00:00, 111.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Diffusion Loss: 4.0952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|██████████| 22/22 [00:00<00:00, 108.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Diffusion Loss: 1.3671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|██████████| 22/22 [00:00<00:00, 109.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Diffusion Loss: 1.0407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|██████████| 22/22 [00:00<00:00, 108.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Diffusion Loss: 0.9676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|██████████| 22/22 [00:00<00:00, 107.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Diffusion Loss: 0.9269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|██████████| 22/22 [00:00<00:00, 105.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Diffusion Loss: 0.8888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|██████████| 22/22 [00:00<00:00, 105.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Diffusion Loss: 0.8463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|██████████| 22/22 [00:00<00:00, 108.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Diffusion Loss: 0.8085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|██████████| 22/22 [00:00<00:00, 101.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Diffusion Loss: 0.7707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|██████████| 22/22 [00:00<00:00, 100.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Diffusion Loss: 0.7214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|██████████| 22/22 [00:00<00:00, 118.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Diffusion Loss: 0.6964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|██████████| 22/22 [00:00<00:00, 116.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Diffusion Loss: 0.6584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|██████████| 22/22 [00:00<00:00, 119.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Diffusion Loss: 0.6505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|██████████| 22/22 [00:00<00:00, 117.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Diffusion Loss: 0.6098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|██████████| 22/22 [00:00<00:00, 112.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Diffusion Loss: 0.5598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20: 100%|██████████| 22/22 [00:00<00:00, 112.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Diffusion Loss: 0.5508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20: 100%|██████████| 22/22 [00:00<00:00, 122.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Diffusion Loss: 0.5294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20: 100%|██████████| 22/22 [00:00<00:00, 112.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Diffusion Loss: 0.5032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20: 100%|██████████| 22/22 [00:00<00:00, 110.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Diffusion Loss: 0.4683\n",
      "Logits shape: torch.Size([32])\n",
      "Logits shape: torch.Size([32])\n",
      "Logits shape: torch.Size([32])\n",
      "Logits shape: torch.Size([32])\n",
      "Logits shape: torch.Size([32])\n",
      "Logits shape: torch.Size([32])\n",
      "Logits shape: torch.Size([32])\n",
      "Logits shape: torch.Size([32])\n",
      "Logits shape: torch.Size([32])\n",
      "Logits shape: torch.Size([12])\n",
      "Accuracy on recovered z: 0.5433\n"
     ]
    }
   ],
   "source": [
    "# latent_dim = vae.encoder[-1].out_features\n",
    "diffusion_model = SimpleDiffusionModel(latent_dim=latent_dim).to(device)\n",
    "diffusion_optimizer = optim.Adam(diffusion_model.parameters(), lr=1e-3)\n",
    "\n",
    "train_diffusion(vae_new, diffusion_model, train_loader, diffusion_optimizer, device)\n",
    "\n",
    "evaluate_diffusion_with_classifier(vae, diffusion_model, vae.classifier, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
