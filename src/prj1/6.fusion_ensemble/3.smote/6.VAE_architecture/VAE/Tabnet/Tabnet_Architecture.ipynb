{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "class GLU(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(GLU, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.dense = tf.keras.layers.Dense(2 * self.units)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense(inputs)\n",
    "        a, b = tf.split(x, num_or_size_splits=2, axis=-1)\n",
    "        return a * tf.nn.sigmoid(b)\n",
    "\n",
    "class GhostBatchNormalization(tf.keras.layers.Layer):\n",
    "    def __init__(self, virtual_batch_size=64, momentum=0.99):\n",
    "        super(GhostBatchNormalization, self).__init__()\n",
    "        self.virtual_batch_size = virtual_batch_size\n",
    "        self.momentum = momentum\n",
    "        self.bn = tf.keras.layers.BatchNormalization(momentum=self.momentum)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        if self.virtual_batch_size is None:\n",
    "            return self.bn(inputs)\n",
    "        splits = tf.split(inputs, num_or_size_splits=self.virtual_batch_size, axis=0)\n",
    "        outputs = [self.bn(split) for split in splits]\n",
    "        return tf.concat(outputs, axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureTransformer(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, n_glus):\n",
    "        super(FeatureTransformer, self).__init__()\n",
    "        self.units = units\n",
    "        self.n_glus = n_glus\n",
    "        self.glu_layers = [GLU(self.units) for _ in range(self.n_glus)]\n",
    "        self.bn_layers = [GhostBatchNormalization() for _ in range(self.n_glus)]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        for glu, bn in zip(self.glu_layers, self.bn_layers):\n",
    "            x = glu(x)\n",
    "            x = bn(x)\n",
    "        return x\n",
    "\n",
    "class AttentiveTransformer(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(AttentiveTransformer, self).__init__()\n",
    "        self.units = units\n",
    "        self.fc = tf.keras.layers.Dense(self.units, activation=None)\n",
    "        self.bn = GhostBatchNormalization()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.fc(inputs)\n",
    "        x = self.bn(x)\n",
    "        x = tf.keras.layers.Softmax(axis=-1)(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tab_net_encoder_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " feature_transformer_4 (Fea  multiple                  15120     \n",
      " tureTransformer)                                                \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15120 (59.06 KB)\n",
      "Trainable params: 14784 (57.75 KB)\n",
      "Non-trainable params: 336 (1.31 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class TabNetEncoder(tf.keras.Model):\n",
    "    def __init__(self, feature_dim, n_glus):\n",
    "        super(TabNetEncoder, self).__init__()\n",
    "        self.feature_transformer = FeatureTransformer(feature_dim, n_glus)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.feature_transformer(inputs)\n",
    "        return x\n",
    "\n",
    "input_shape = (None, 42)\n",
    "\n",
    "encoder = TabNetEncoder(feature_dim=42, n_glus=4)  \n",
    "encoder.build(input_shape)\n",
    "\n",
    "encoder.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tab_net_decoder_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " feature_transformer_5 (Fea  multiple                  15120     \n",
      " tureTransformer)                                                \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15120 (59.06 KB)\n",
      "Trainable params: 14784 (57.75 KB)\n",
      "Non-trainable params: 336 (1.31 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "class TabNetDecoder(tf.keras.Model):\n",
    "    def __init__(self, feature_dim, n_glus):\n",
    "        super(TabNetDecoder, self).__init__()\n",
    "        self.feature_transformer = FeatureTransformer(feature_dim, n_glus)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.feature_transformer(inputs)\n",
    "        return x\n",
    "\n",
    "decoder = TabNetDecoder(feature_dim=42, n_glus=4)  \n",
    "\n",
    "decoder.build(input_shape)\n",
    "\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 42)]              0         \n",
      "                                                                 \n",
      " tab_net_3 (TabNet)          ((None, 42),              32214     \n",
      "                              [(None, 42),                       \n",
      "                              (None, 42),                        \n",
      "                              (None, 42),                        \n",
      "                              (None, 42),                        \n",
      "                              (None, 42)])                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 32214 (125.84 KB)\n",
      "Trainable params: 31458 (122.88 KB)\n",
      "Non-trainable params: 756 (2.95 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "class TabNet(tf.keras.Model):\n",
    "    def __init__(self, encoder, decoder, feature_dim, n_steps):\n",
    "        super(TabNet, self).__init__()\n",
    "        self.feature_dim = feature_dim\n",
    "        # self.output_dim = output_dim\n",
    "        self.n_steps = n_steps\n",
    "        self.encoder = encoder\n",
    "        self.attentive_transformer = AttentiveTransformer(self.feature_dim)\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.encoder(inputs)\n",
    "        outputs = []\n",
    "        masks = []\n",
    "\n",
    "        for step in range(self.n_steps):\n",
    "            mask = self.attentive_transformer(x)\n",
    "            masks.append(mask)\n",
    "            x = x * mask\n",
    "            x = self.decoder(x)\n",
    "            outputs.append(x)\n",
    "\n",
    "        outputs = tf.reduce_sum(outputs, axis=0)\n",
    "        return outputs, masks\n",
    "\n",
    "input_shape = (None, 42)  \n",
    "tabnet = TabNet(feature_dim=42, encoder=encoder,decoder=decoder,  n_steps=5)\n",
    "\n",
    "inputs = tf.keras.Input(shape=input_shape[1:])\n",
    "outputs, masks = tabnet(inputs)\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Display the model's architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
