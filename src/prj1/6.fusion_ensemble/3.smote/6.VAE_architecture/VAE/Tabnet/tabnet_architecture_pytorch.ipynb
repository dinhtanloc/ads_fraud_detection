{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ads_prj_nckh\\.conda\\lib\\site-packages\\pytorch_tabnet\\utils.py:5: UserWarning: A NumPy version >=1.23.5 and <2.3.0 is required for this version of SciPy (detected version 1.22.4)\n",
      "  import scipy\n"
     ]
    }
   ],
   "source": [
    "from pytorch_tabnet.pretraining import TabNetPretrainer\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torchinfo pytorch_tabnet pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(np.random.rand(800, 10), columns=[f\"feature_{i}\" for i in range(10)])\n",
    "df_test = pd.DataFrame(np.random.rand(200, 10), columns=[f\"feature_{i}\" for i in range(10)])\n",
    "\n",
    "# Và labels tương ứng\n",
    "y_train = pd.Series(np.random.rand(800))\n",
    "y_test = pd.Series(np.random.rand(200))\n",
    "\n",
    "# Chuyển đổi DataFrame và Series sang mảng NumPy với kiểu dữ liệu float32\n",
    "X_train = df_train.values.astype(np.float32)\n",
    "X_test = df_test.values.astype(np.float32)\n",
    "y_train = y_train.values.astype(np.float32)\n",
    "y_test = y_test.values.astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (800, 10)\n",
      "X_test shape: (200, 10)\n",
      "y_train shape: (800,)\n",
      "y_test shape: (200,)\n",
      "X_train has missing values: False\n",
      "X_test has missing values: False\n",
      "y_train has missing values: False\n",
      "y_test has missing values: False\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "# Kiểm tra giá trị thiếu\n",
    "print(\"X_train has missing values:\", np.any(np.isnan(X_train)))\n",
    "print(\"X_test has missing values:\", np.any(np.isnan(X_test)))\n",
    "print(\"y_train has missing values:\", np.any(np.isnan(y_train)))\n",
    "print(\"y_test has missing values:\", np.any(np.isnan(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ads_prj_nckh\\.conda\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 50.45297| val_0_unsup_loss_numpy: 6.042570114135742|  0:00:00s\n",
      "epoch 1  | loss: 29.11551| val_0_unsup_loss_numpy: 4.289559841156006|  0:00:00s\n",
      "epoch 2  | loss: 19.78084| val_0_unsup_loss_numpy: 4.747350215911865|  0:00:00s\n",
      "epoch 3  | loss: 14.06169| val_0_unsup_loss_numpy: 3.781209945678711|  0:00:00s\n",
      "epoch 4  | loss: 10.04586| val_0_unsup_loss_numpy: 3.6191000938415527|  0:00:00s\n",
      "epoch 5  | loss: 7.99131 | val_0_unsup_loss_numpy: 2.4802000522613525|  0:00:00s\n",
      "epoch 6  | loss: 6.31865 | val_0_unsup_loss_numpy: 2.1048200130462646|  0:00:00s\n",
      "epoch 7  | loss: 5.1697  | val_0_unsup_loss_numpy: 2.067650079727173|  0:00:00s\n",
      "epoch 8  | loss: 3.96239 | val_0_unsup_loss_numpy: 5.050839900970459|  0:00:00s\n",
      "epoch 9  | loss: 3.46388 | val_0_unsup_loss_numpy: 1.4152899980545044|  0:00:01s\n",
      "epoch 10 | loss: 2.9027  | val_0_unsup_loss_numpy: 1.3602499961853027|  0:00:01s\n",
      "epoch 11 | loss: 2.35654 | val_0_unsup_loss_numpy: 1.2529699802398682|  0:00:01s\n",
      "epoch 12 | loss: 2.11235 | val_0_unsup_loss_numpy: 1.272629976272583|  0:00:01s\n",
      "epoch 13 | loss: 1.94396 | val_0_unsup_loss_numpy: 1.4068399667739868|  0:00:01s\n",
      "epoch 14 | loss: 1.75531 | val_0_unsup_loss_numpy: 1.185349941253662|  0:00:01s\n",
      "epoch 15 | loss: 1.60638 | val_0_unsup_loss_numpy: 1.1641700267791748|  0:00:01s\n",
      "epoch 16 | loss: 1.51884 | val_0_unsup_loss_numpy: 1.1203800439834595|  0:00:01s\n",
      "epoch 17 | loss: 1.41005 | val_0_unsup_loss_numpy: 1.0153700113296509|  0:00:01s\n",
      "epoch 18 | loss: 1.37533 | val_0_unsup_loss_numpy: 1.2063100337982178|  0:00:02s\n",
      "epoch 19 | loss: 1.36514 | val_0_unsup_loss_numpy: 1.0677800178527832|  0:00:02s\n",
      "epoch 20 | loss: 1.26773 | val_0_unsup_loss_numpy: 1.0403399467468262|  0:00:02s\n",
      "epoch 21 | loss: 1.2495  | val_0_unsup_loss_numpy: 1.0398199558258057|  0:00:02s\n",
      "epoch 22 | loss: 1.18026 | val_0_unsup_loss_numpy: 1.0680999755859375|  0:00:02s\n",
      "epoch 23 | loss: 1.15219 | val_0_unsup_loss_numpy: 1.100790023803711|  0:00:02s\n",
      "epoch 24 | loss: 1.13786 | val_0_unsup_loss_numpy: 1.1661399602890015|  0:00:02s\n",
      "epoch 25 | loss: 1.14498 | val_0_unsup_loss_numpy: 1.0789400339126587|  0:00:02s\n",
      "epoch 26 | loss: 1.11638 | val_0_unsup_loss_numpy: 1.0552899837493896|  0:00:02s\n",
      "epoch 27 | loss: 1.13485 | val_0_unsup_loss_numpy: 1.0398199558258057|  0:00:02s\n",
      "\n",
      "Early stopping occurred at epoch 27 with best_epoch = 17 and best_val_0_unsup_loss_numpy = 1.0153700113296509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ads_prj_nckh\\.conda\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    }
   ],
   "source": [
    "tabnet_params = {\n",
    "    \"n_d\": 16,\n",
    "    \"n_a\": 16,\n",
    "    \"n_steps\": 3,\n",
    "    \"n_shared\": 2,\n",
    "    \"n_independent\": 2,\n",
    "    \"gamma\": 1.3,\n",
    "    \"epsilon\": 1e-15,\n",
    "    \"momentum\": 0.98,\n",
    "    \"mask_type\": \"sparsemax\",\n",
    "    \"lambda_sparse\": 1e-3\n",
    "}\n",
    "\n",
    "\n",
    "unsupervised_model = TabNetPretrainer(\n",
    "    optimizer_fn=torch.optim.Adam,\n",
    "    optimizer_params=dict(lr=2e-2),\n",
    "    **tabnet_params\n",
    ")\n",
    " \n",
    "\n",
    "# Huấn luyện mô hình tiền huấn luyện\n",
    "unsupervised_model.fit(\n",
    "    X_train,\n",
    "    eval_set=[X_test],  # Đảm bảo eval_set là danh sách các tuple (X, y)\n",
    "    pretraining_ratio=0.8,\n",
    "    max_epochs=101,\n",
    "    patience=10,\n",
    "    batch_size=1024,\n",
    "    virtual_batch_size=128,\n",
    "    num_workers=0,\n",
    "    drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==============================================================================================================\n",
       "Layer (type:depth-idx)                                       Output Shape              Param #\n",
       "==============================================================================================================\n",
       "TabNetPretraining                                            [1, 10]                   --\n",
       "├─EmbeddingGenerator: 1-1                                    [1, 10]                   --\n",
       "├─TabNetEncoder: 1-2                                         [1, 16]                   --\n",
       "│    └─BatchNorm1d: 2-1                                      [1, 10]                   20\n",
       "│    └─FeatTransformer: 2-2                                  [1, 32]                   4,352\n",
       "│    │    └─GLU_Block: 3-1                                   [1, 32]                   2,944\n",
       "│    └─ModuleList: 2-12                                      --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-17                            --                        (recursive)\n",
       "│    └─FeatTransformer: 2-6                                  --                        (recursive)\n",
       "│    │    └─GLU_Block: 3-5                                   --                        (recursive)\n",
       "│    └─ModuleList: 2-12                                      --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-17                            --                        (recursive)\n",
       "│    └─FeatTransformer: 2-6                                  --                        (recursive)\n",
       "│    │    └─GLU_Block: 3-5                                   --                        (recursive)\n",
       "│    │    └─GLU_Block: 3-6                                   [1, 32]                   4,352\n",
       "│    └─ModuleList: 2-11                                      --                        (recursive)\n",
       "│    │    └─AttentiveTransformer: 3-7                        [1, 10]                   180\n",
       "│    └─ModuleList: 2-12                                      --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-8                             [1, 32]                   7,296\n",
       "│    │    └─FeatTransformer: 3-17                            --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-12                            --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-17                            --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-12                            --                        (recursive)\n",
       "│    └─ModuleList: 2-11                                      --                        (recursive)\n",
       "│    │    └─AttentiveTransformer: 3-13                       [1, 10]                   180\n",
       "│    └─ModuleList: 2-12                                      --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-14                            [1, 32]                   7,296\n",
       "│    │    └─FeatTransformer: 3-17                            --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-18                            --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-17                            --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-18                            --                        (recursive)\n",
       "│    └─ModuleList: 2-11                                      --                        (recursive)\n",
       "│    │    └─AttentiveTransformer: 3-19                       [1, 10]                   180\n",
       "│    └─ModuleList: 2-12                                      --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-20                            [1, 32]                   7,296\n",
       "├─TabNetDecoder: 1-3                                         [1, 10]                   --\n",
       "│    └─ModuleList: 2-13                                      --                        --\n",
       "│    │    └─FeatTransformer: 3-21                            [1, 16]                   1,152\n",
       "│    │    └─FeatTransformer: 3-25                            --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-23                            --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-24                            [1, 16]                   1,152\n",
       "│    │    └─FeatTransformer: 3-25                            --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-26                            --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-27                            [1, 16]                   1,152\n",
       "│    └─Linear: 2-14                                          [1, 10]                   160\n",
       "==============================================================================================================\n",
       "Total params: 58,192\n",
       "Trainable params: 58,192\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.03\n",
       "==============================================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.02\n",
       "Params size (MB): 0.10\n",
       "Estimated Total Size (MB): 0.12\n",
       "=============================================================================================================="
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Truy cập vào mô hình TabNet bên trong\n",
    "from torchinfo import summary\n",
    "\n",
    "tabnet_model = unsupervised_model.network\n",
    "\n",
    "# Sử dụng torchinfo để in ra kiến trúc mô hình\n",
    "summary(tabnet_model, input_size=(1, 10))  # input_size dựa trên kích thước của dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Encoder Summary:\n",
      "TabNetEncoder(\n",
      "  (initial_bn): BatchNorm1d(10, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  (initial_splitter): FeatTransformer(\n",
      "    (shared): GLU_Block(\n",
      "      (shared_layers): ModuleList(\n",
      "        (0): Linear(in_features=10, out_features=64, bias=False)\n",
      "        (1): Linear(in_features=32, out_features=64, bias=False)\n",
      "      )\n",
      "      (glu_layers): ModuleList(\n",
      "        (0): GLU_Layer(\n",
      "          (fc): Linear(in_features=10, out_features=64, bias=False)\n",
      "          (bn): GBN(\n",
      "            (bn): BatchNorm1d(64, eps=1e-05, momentum=0.98, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): GLU_Layer(\n",
      "          (fc): Linear(in_features=32, out_features=64, bias=False)\n",
      "          (bn): GBN(\n",
      "            (bn): BatchNorm1d(64, eps=1e-05, momentum=0.98, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (specifics): GLU_Block(\n",
      "      (glu_layers): ModuleList(\n",
      "        (0-1): 2 x GLU_Layer(\n",
      "          (fc): Linear(in_features=32, out_features=64, bias=False)\n",
      "          (bn): GBN(\n",
      "            (bn): BatchNorm1d(64, eps=1e-05, momentum=0.98, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (feat_transformers): ModuleList(\n",
      "    (0-2): 3 x FeatTransformer(\n",
      "      (shared): GLU_Block(\n",
      "        (shared_layers): ModuleList(\n",
      "          (0): Linear(in_features=10, out_features=64, bias=False)\n",
      "          (1): Linear(in_features=32, out_features=64, bias=False)\n",
      "        )\n",
      "        (glu_layers): ModuleList(\n",
      "          (0): GLU_Layer(\n",
      "            (fc): Linear(in_features=10, out_features=64, bias=False)\n",
      "            (bn): GBN(\n",
      "              (bn): BatchNorm1d(64, eps=1e-05, momentum=0.98, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (1): GLU_Layer(\n",
      "            (fc): Linear(in_features=32, out_features=64, bias=False)\n",
      "            (bn): GBN(\n",
      "              (bn): BatchNorm1d(64, eps=1e-05, momentum=0.98, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (specifics): GLU_Block(\n",
      "        (glu_layers): ModuleList(\n",
      "          (0-1): 2 x GLU_Layer(\n",
      "            (fc): Linear(in_features=32, out_features=64, bias=False)\n",
      "            (bn): GBN(\n",
      "              (bn): BatchNorm1d(64, eps=1e-05, momentum=0.98, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (att_transformers): ModuleList(\n",
      "    (0-2): 3 x AttentiveTransformer(\n",
      "      (fc): Linear(in_features=16, out_features=10, bias=False)\n",
      "      (bn): GBN(\n",
      "        (bn): BatchNorm1d(10, eps=1e-05, momentum=0.98, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (selector): Sparsemax()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "Decoder Summary:\n",
      "TabNetDecoder(\n",
      "  (feat_transformers): ModuleList(\n",
      "    (0-2): 3 x FeatTransformer(\n",
      "      (shared): GLU_Block(\n",
      "        (shared_layers): ModuleList(\n",
      "          (0): Linear(in_features=16, out_features=32, bias=False)\n",
      "        )\n",
      "        (glu_layers): ModuleList(\n",
      "          (0): GLU_Layer(\n",
      "            (fc): Linear(in_features=16, out_features=32, bias=False)\n",
      "            (bn): GBN(\n",
      "              (bn): BatchNorm1d(32, eps=1e-05, momentum=0.98, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (specifics): GLU_Block(\n",
      "        (glu_layers): ModuleList(\n",
      "          (0): GLU_Layer(\n",
      "            (fc): Linear(in_features=16, out_features=32, bias=False)\n",
      "            (bn): GBN(\n",
      "              (bn): BatchNorm1d(32, eps=1e-05, momentum=0.98, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (reconstruction_layer): Linear(in_features=16, out_features=10, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "encoder = tabnet_model.encoder\n",
    "decoder = tabnet_model.decoder\n",
    "\n",
    "print(\"\\nEncoder Summary:\")\n",
    "print(encoder)\n",
    "\n",
    "print(\"\\nDecoder Summary:\")\n",
    "print(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
