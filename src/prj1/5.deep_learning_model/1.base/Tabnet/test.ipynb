{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_27680\\614757177.py:9: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
      "  from kerastuner.tuners import RandomSearch\n",
      "c:\\Users\\PC\\Data\\ads_fraud_detection\\.conda\\lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "usage: ipykernel_launcher.py [-h] [--trials TRIALS] [--epochs EPOCHS]\n",
      "                             [--bs BS] [--exec_per_trial EXEC_PER_TRIAL]\n",
      "                             [--project PROJECT] [--cleanup]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --f=c:\\Users\\PC\\AppData\\Roaming\\jupyter\\runtime\\kernel-v2-19860c8ZKQajohics.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\Data\\ads_fraud_detection\\.conda\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3516: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from typing import Text, List\n",
    "import pickle\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from kerastuner.tuners import RandomSearch\n",
    "\n",
    "from tabnet.models.classify import TabNetClassifier\n",
    "from tabnet.utils import set_seed\n",
    "from tabnet.schedules import DecayWithWarmupSchedule\n",
    "\n",
    "\n",
    "SEARCH_DIR = \".search\"\n",
    "SEED = 42\n",
    "DEFAULTS = {\"num_features\": 784, \"n_classes\": 10, \"min_learning_rate\": 1e-6}  # 28x28\n",
    "\n",
    "\n",
    "# because doing a training on MNIST is something I MUST do, no?\n",
    "# this time let's add a twist & do hyperparameter optimization with kerastuner\n",
    "\n",
    "\n",
    "def build_model(hp):\n",
    "    model = TabNetClassifier(\n",
    "        num_features=DEFAULTS[\"num_features\"],\n",
    "        feature_dim=hp.Choice(\"feature_dim\", values=[16, 32, 64], default=32),\n",
    "        output_dim=hp.Choice(\"output_dim\", values=[16, 32, 64], default=32),\n",
    "        n_classes=DEFAULTS[\"n_classes\"],\n",
    "        n_step=hp.Choice(\"n_step\", values=[2, 4, 5, 6], default=4),\n",
    "        relaxation_factor=hp.Choice(\n",
    "            \"relaxation_factor\", values=[1.0, 1.25, 1.5, 2.0, 3.0], default=1.5\n",
    "        ),\n",
    "        sparsity_coefficient=hp.Choice(\n",
    "            \"sparsity_coefficient\",\n",
    "            values=[0.0001, 0.001, 0.01, 0.02, 0.05],\n",
    "            default=0.0001,\n",
    "        ),\n",
    "        bn_momentum=hp.Choice(\"bn_momentum\", values=[0.6, 0.7, 0.9], default=0.7),\n",
    "        bn_virtual_divider=1,  # let's not use Ghost Batch Normalization. batch sizes are too small\n",
    "        dp=hp.Choice(\"dp\", values=[0.0, 0.1, 0.2, 0.3, 0.4], default=0.0),\n",
    "    )\n",
    "    lr = DecayWithWarmupSchedule(\n",
    "        hp.Choice(\n",
    "            \"learning_rate\", values=[0.001, 0.005, 0.01, 0.02, 0.05], default=0.02\n",
    "        ),\n",
    "        DEFAULTS[\"min_learning_rate\"],\n",
    "        hp.Choice(\"warmup\", values=[1, 5, 10, 20], default=5),\n",
    "        hp.Choice(\"decay_rate\", values=[0.8, 0.90, 0.95, 0.99], default=0.95),\n",
    "        hp.Choice(\"decay_steps\", values=[10, 100, 500, 1000], default=500),\n",
    "    )\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(\n",
    "        learning_rate=lr,\n",
    "        clipnorm=hp.Choice(\"clipnorm\", values=[1, 2, 5, 10], default=2),\n",
    "    )\n",
    "\n",
    "    lossf = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer,\n",
    "        loss=lossf,\n",
    "        metrics=[tf.keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\")],\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def prepare_dataset(\n",
    "    ds: tf.data.Dataset,\n",
    "    batch_size: int,\n",
    "    shuffle: bool = False,\n",
    "    drop_remainder: bool = False,\n",
    "):\n",
    "    size_of_dataset = ds.reduce(0, lambda x, _: x + 1).numpy()\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=size_of_dataset, seed=SEED)\n",
    "    ds: tf.data.Dataset = ds.batch(batch_size, drop_remainder=drop_remainder)\n",
    "\n",
    "    @tf.function\n",
    "    def prepare_data(features):\n",
    "        image = tf.cast(features[\"image\"], tf.float32)\n",
    "        bs = tf.shape(image)[0]\n",
    "        image = tf.reshape(image / 255.0, (bs, -1))\n",
    "        return image, features[\"label\"]\n",
    "\n",
    "    autotune = tf.data.experimental.AUTOTUNE\n",
    "    ds = ds.map(prepare_data, num_parallel_calls=autotune).prefetch(autotune)\n",
    "    return ds\n",
    "\n",
    "\n",
    "def search(\n",
    "    epochs: int,\n",
    "    batch_size: int,\n",
    "    n_trials: int,\n",
    "    execution_per_trial: int,\n",
    "    project: Text,\n",
    "    do_cleanup: bool,\n",
    "):\n",
    "    set_seed(SEED)\n",
    "\n",
    "    dir_to_clean = os.path.join(SEARCH_DIR, project)\n",
    "    if do_cleanup and os.path.exists(dir_to_clean):\n",
    "        shutil.rmtree(dir_to_clean)\n",
    "\n",
    "    # first 80% for train. remaining 20% for val & test dataset for final eval.\n",
    "    ds_tr, ds_val, ds_test = tfds.load(\n",
    "        name=\"mnist\",\n",
    "        split=[\"train[:80%]\", \"train[-20%:]\", \"test\"],\n",
    "        data_dir=\"mnist\",\n",
    "        shuffle_files=False,\n",
    "    )\n",
    "\n",
    "    ds_tr = prepare_dataset(ds_tr, batch_size, shuffle=True, drop_remainder=True)\n",
    "    ds_val = prepare_dataset(ds_val, batch_size, shuffle=False, drop_remainder=False)\n",
    "    ds_test = prepare_dataset(ds_test, batch_size, shuffle=False, drop_remainder=False)\n",
    "\n",
    "    tuner = RandomSearch(\n",
    "        build_model,\n",
    "        objective=\"val_accuracy\",\n",
    "        max_trials=n_trials,\n",
    "        executions_per_trial=execution_per_trial,\n",
    "        directory=SEARCH_DIR,\n",
    "        project_name=project,\n",
    "    )\n",
    "\n",
    "    # ? add callbacks\n",
    "    tuner.search(\n",
    "        ds_tr, epochs=epochs, validation_data=ds_val,\n",
    "    )\n",
    "\n",
    "    best_model: tf.keras.Model = tuner.get_best_models(num_models=1)[0]\n",
    "    best_model.build((None, DEFAULTS[\"num_features\"]))\n",
    "    results = best_model.evaluate(ds_test, return_dict=True)\n",
    "\n",
    "    tuner.results_summary(num_trials=1)\n",
    "    best_hyperparams = tuner.get_best_hyperparameters(num_trials=1)\n",
    "    print(f\"Test results: {results}\")\n",
    "\n",
    "    output = {\"results\": results, \"best_hyperparams\": best_hyperparams}\n",
    "    with open(\"search_results.pickle\", \"wb\") as f:\n",
    "        pickle.dump(output, f)\n",
    "\n",
    "    # best_model.save(\"tabnet_saved_model\")\n",
    "\n",
    "\n",
    "# python3 examples/train_mnist.py --trials 2 --epochs 10 --bs 128\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--trials\", default=1, type=int)\n",
    "    parser.add_argument(\"--epochs\", default=1, type=int)\n",
    "    parser.add_argument(\"--bs\", default=32, type=int)\n",
    "    parser.add_argument(\"--exec_per_trial\", default=2, type=int)\n",
    "    parser.add_argument(\"--project\", default=\"test\", type=str)\n",
    "    parser.add_argument(\"--cleanup\", action=\"store_true\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    search(\n",
    "        args.epochs,\n",
    "        args.bs,\n",
    "        args.trials,\n",
    "        args.exec_per_trial,\n",
    "        args.project,\n",
    "        args.cleanup,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
