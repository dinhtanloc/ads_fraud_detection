{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0\n",
      "NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  \n",
    "print(torch.cuda.current_device())  \n",
    "print(torch.cuda.get_device_name(torch.cuda.current_device())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\Data\\ads_fraud_detection\n",
      "c:/Users/Admin/Data/ads_fraud_detection\n"
     ]
    }
   ],
   "source": [
    "import sys,os\n",
    "__script_path=os.path.abspath(globals().get('__file__','.'))\n",
    "__script_dir = os.path.dirname(__script_path)\n",
    "root_dir = os.path.abspath(f'{__script_dir}/../../../../..')\n",
    "print(root_dir)\n",
    "for lib in [root_dir][::-1]:\n",
    "    if lib in sys.path:\n",
    "        sys.path.remove(lib)\n",
    "    sys.path.insert(0,lib)\n",
    "from config.config import *\n",
    "# from libs.common import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir=f\"{exps_dir}/exp2/exp_smote\"\n",
    "if os.path.exists(save_dir) == False: \n",
    "  os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "test_size=0.33\n",
    "seed=42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. Using GPU.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"CUDA is available. Using GPU.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA is not available. Using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from pytorch_tabnet.pretraining import TabNetPretrainer\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.6378163041991519, 1: 2.3140088827134457}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train=pd.read_excel(f'{save_dir}/x_train.xlsx')\n",
    "y_train=pd.read_excel(f'{save_dir}/y_train.xlsx')\n",
    "x_test=pd.read_excel(f'{save_dir}/x_test.xlsx')\n",
    "y_test=pd.read_excel(f'{save_dir}/y_test.xlsx')\n",
    "class_weights_dict=dict(np.load(f'{save_dir}/class_weights_dict.npz',allow_pickle=True))['class_weights_dict']\n",
    "class_weights_dict = {key: value for key, value in class_weights_dict.item().items()}\n",
    "class_weights_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([19430, 33]) torch.Size([19430, 1, 2]) torch.Size([5089, 33]) torch.Size([5089, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "X_train = torch.tensor(x_train.values, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train.values, dtype=torch.long)\n",
    "X_test = torch.tensor(x_test.values, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test.values, dtype=torch.long)\n",
    "num_classes = 2  # Assuming binary classification\n",
    "y_train = F.one_hot(y_train, num_classes=num_classes)\n",
    "y_test = F.one_hot(y_test, num_classes=num_classes)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\miniconda3\\envs\\pytorch\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1941.51756| val_0_unsup_loss_numpy: 21.037090301513672|  0:00:01s\n",
      "epoch 1  | loss: 22.06558| val_0_unsup_loss_numpy: 4.085599899291992|  0:00:03s\n",
      "epoch 2  | loss: 8.62228 | val_0_unsup_loss_numpy: 6.7566399574279785|  0:00:04s\n",
      "epoch 3  | loss: 8.90729 | val_0_unsup_loss_numpy: 2.1685800552368164|  0:00:05s\n",
      "epoch 4  | loss: 4.31033 | val_0_unsup_loss_numpy: 88.85392761230469|  0:00:07s\n",
      "epoch 5  | loss: 6.68755 | val_0_unsup_loss_numpy: 1.4134299755096436|  0:00:08s\n",
      "epoch 6  | loss: 4.51434 | val_0_unsup_loss_numpy: 1.9488600492477417|  0:00:10s\n",
      "epoch 7  | loss: 5.04796 | val_0_unsup_loss_numpy: 1.7437700033187866|  0:00:11s\n",
      "epoch 8  | loss: 2.8203  | val_0_unsup_loss_numpy: 6.641610145568848|  0:00:13s\n",
      "epoch 9  | loss: 2.32682 | val_0_unsup_loss_numpy: 1.6450400352478027|  0:00:14s\n",
      "epoch 10 | loss: 2.08004 | val_0_unsup_loss_numpy: 2.615489959716797|  0:00:16s\n",
      "epoch 11 | loss: 2.23871 | val_0_unsup_loss_numpy: 1.3475799560546875|  0:00:17s\n",
      "epoch 12 | loss: 1.88427 | val_0_unsup_loss_numpy: 1.262120008468628|  0:00:19s\n",
      "epoch 13 | loss: 1.56024 | val_0_unsup_loss_numpy: 1.3143600225448608|  0:00:20s\n",
      "epoch 14 | loss: 1.36775 | val_0_unsup_loss_numpy: 1.360550045967102|  0:00:22s\n",
      "epoch 15 | loss: 1.37813 | val_0_unsup_loss_numpy: 1.2533899545669556|  0:00:23s\n",
      "epoch 16 | loss: 1.25151 | val_0_unsup_loss_numpy: 1.191249966621399|  0:00:25s\n",
      "epoch 17 | loss: 1.22788 | val_0_unsup_loss_numpy: 1.1953999996185303|  0:00:26s\n",
      "epoch 18 | loss: 1.2381  | val_0_unsup_loss_numpy: 1.2280900478363037|  0:00:28s\n",
      "epoch 19 | loss: 1.13947 | val_0_unsup_loss_numpy: 1.0085899829864502|  0:00:29s\n",
      "epoch 20 | loss: 1.0784  | val_0_unsup_loss_numpy: 1.0561800003051758|  0:00:31s\n",
      "epoch 21 | loss: 1.04977 | val_0_unsup_loss_numpy: 1.0153599977493286|  0:00:32s\n",
      "epoch 22 | loss: 1.14197 | val_0_unsup_loss_numpy: 1.0738099813461304|  0:00:34s\n",
      "epoch 23 | loss: 1.08661 | val_0_unsup_loss_numpy: 1.0555000305175781|  0:00:35s\n",
      "epoch 24 | loss: 1.18632 | val_0_unsup_loss_numpy: 1.277359962463379|  0:00:36s\n",
      "epoch 25 | loss: 1.09346 | val_0_unsup_loss_numpy: 1.1215900182724|  0:00:38s\n",
      "epoch 26 | loss: 1.09819 | val_0_unsup_loss_numpy: 1.1039700508117676|  0:00:39s\n",
      "epoch 27 | loss: 1.12579 | val_0_unsup_loss_numpy: 1.1165399551391602|  0:00:41s\n",
      "epoch 28 | loss: 1.09386 | val_0_unsup_loss_numpy: 1.0944099426269531|  0:00:43s\n",
      "epoch 29 | loss: 1.04237 | val_0_unsup_loss_numpy: 0.9616699814796448|  0:00:44s\n",
      "epoch 30 | loss: 1.0422  | val_0_unsup_loss_numpy: 1.2115700244903564|  0:00:46s\n",
      "epoch 31 | loss: 1.03273 | val_0_unsup_loss_numpy: 0.9550399780273438|  0:00:47s\n",
      "epoch 32 | loss: 1.03919 | val_0_unsup_loss_numpy: 1.02128005027771|  0:00:49s\n",
      "epoch 33 | loss: 1.05916 | val_0_unsup_loss_numpy: 0.9645599722862244|  0:00:50s\n",
      "epoch 34 | loss: 1.14837 | val_0_unsup_loss_numpy: 1.2175099849700928|  0:00:52s\n",
      "epoch 35 | loss: 1.13161 | val_0_unsup_loss_numpy: 1.0161700248718262|  0:00:53s\n",
      "epoch 36 | loss: 1.04864 | val_0_unsup_loss_numpy: 0.9416400194168091|  0:00:55s\n",
      "epoch 37 | loss: 1.03796 | val_0_unsup_loss_numpy: 0.9466099739074707|  0:00:56s\n",
      "epoch 38 | loss: 1.04712 | val_0_unsup_loss_numpy: 1.1171000003814697|  0:00:58s\n",
      "epoch 39 | loss: 1.03224 | val_0_unsup_loss_numpy: 1.0069600343704224|  0:00:59s\n",
      "epoch 40 | loss: 1.01635 | val_0_unsup_loss_numpy: 0.9125400185585022|  0:01:01s\n",
      "epoch 41 | loss: 0.96449 | val_0_unsup_loss_numpy: 0.9045199751853943|  0:01:02s\n",
      "epoch 42 | loss: 0.96617 | val_0_unsup_loss_numpy: 0.90038001537323|  0:01:04s\n",
      "epoch 43 | loss: 0.98035 | val_0_unsup_loss_numpy: 0.8801800012588501|  0:01:05s\n",
      "epoch 44 | loss: 0.98193 | val_0_unsup_loss_numpy: 0.895039975643158|  0:01:07s\n",
      "epoch 45 | loss: 0.96042 | val_0_unsup_loss_numpy: 0.8669599890708923|  0:01:08s\n",
      "epoch 46 | loss: 0.95853 | val_0_unsup_loss_numpy: 0.8800399899482727|  0:01:10s\n",
      "epoch 47 | loss: 0.95059 | val_0_unsup_loss_numpy: 0.8652499914169312|  0:01:11s\n",
      "epoch 48 | loss: 0.97695 | val_0_unsup_loss_numpy: 0.8845800161361694|  0:01:13s\n",
      "epoch 49 | loss: 0.99737 | val_0_unsup_loss_numpy: 0.8735799789428711|  0:01:15s\n",
      "epoch 50 | loss: 0.94668 | val_0_unsup_loss_numpy: 0.8738600015640259|  0:01:16s\n",
      "epoch 51 | loss: 0.96331 | val_0_unsup_loss_numpy: 0.8837500214576721|  0:01:18s\n",
      "epoch 52 | loss: 0.98914 | val_0_unsup_loss_numpy: 0.8901900053024292|  0:01:19s\n",
      "epoch 53 | loss: 0.96705 | val_0_unsup_loss_numpy: 0.8846499919891357|  0:01:21s\n",
      "epoch 54 | loss: 0.95374 | val_0_unsup_loss_numpy: 0.900950014591217|  0:01:22s\n",
      "epoch 55 | loss: 0.95413 | val_0_unsup_loss_numpy: 0.8563699722290039|  0:01:24s\n",
      "epoch 56 | loss: 0.93821 | val_0_unsup_loss_numpy: 0.82819002866745|  0:01:25s\n",
      "epoch 57 | loss: 0.93614 | val_0_unsup_loss_numpy: 0.8290600180625916|  0:01:27s\n",
      "epoch 58 | loss: 0.94227 | val_0_unsup_loss_numpy: 0.8391900062561035|  0:01:28s\n",
      "epoch 59 | loss: 0.98242 | val_0_unsup_loss_numpy: 0.8648899793624878|  0:01:30s\n",
      "epoch 60 | loss: 0.94819 | val_0_unsup_loss_numpy: 0.8229299783706665|  0:01:31s\n",
      "epoch 61 | loss: 0.94446 | val_0_unsup_loss_numpy: 0.8197299838066101|  0:01:33s\n",
      "epoch 62 | loss: 0.97418 | val_0_unsup_loss_numpy: 0.9168499708175659|  0:01:34s\n",
      "epoch 63 | loss: 1.02273 | val_0_unsup_loss_numpy: 0.822160005569458|  0:01:36s\n",
      "epoch 64 | loss: 0.96167 | val_0_unsup_loss_numpy: 0.8324000239372253|  0:01:38s\n",
      "epoch 65 | loss: 0.94561 | val_0_unsup_loss_numpy: 0.8230999708175659|  0:01:39s\n",
      "epoch 66 | loss: 0.93456 | val_0_unsup_loss_numpy: 0.8163700103759766|  0:01:41s\n",
      "epoch 67 | loss: 0.9439  | val_0_unsup_loss_numpy: 0.8678100109100342|  0:01:42s\n",
      "epoch 68 | loss: 0.93858 | val_0_unsup_loss_numpy: 0.8542199730873108|  0:01:44s\n",
      "epoch 69 | loss: 0.93572 | val_0_unsup_loss_numpy: 0.8215600252151489|  0:01:45s\n",
      "epoch 70 | loss: 0.92469 | val_0_unsup_loss_numpy: 0.8466200232505798|  0:01:47s\n",
      "epoch 71 | loss: 0.94253 | val_0_unsup_loss_numpy: 0.825659990310669|  0:01:48s\n",
      "epoch 72 | loss: 0.93626 | val_0_unsup_loss_numpy: 0.7912899851799011|  0:01:50s\n",
      "epoch 73 | loss: 0.91903 | val_0_unsup_loss_numpy: 0.8102099895477295|  0:01:52s\n",
      "epoch 74 | loss: 0.92683 | val_0_unsup_loss_numpy: 0.8240500092506409|  0:01:53s\n",
      "epoch 75 | loss: 0.92591 | val_0_unsup_loss_numpy: 0.8643199801445007|  0:01:55s\n",
      "epoch 76 | loss: 0.95138 | val_0_unsup_loss_numpy: 0.8731499910354614|  0:01:56s\n",
      "epoch 77 | loss: 0.94109 | val_0_unsup_loss_numpy: 0.8157299757003784|  0:01:58s\n",
      "epoch 78 | loss: 0.92133 | val_0_unsup_loss_numpy: 0.8320900201797485|  0:01:59s\n",
      "epoch 79 | loss: 0.94743 | val_0_unsup_loss_numpy: 0.8314899802207947|  0:02:01s\n",
      "epoch 80 | loss: 0.93404 | val_0_unsup_loss_numpy: 0.8224700093269348|  0:02:02s\n",
      "epoch 81 | loss: 0.92291 | val_0_unsup_loss_numpy: 0.8184700012207031|  0:02:04s\n",
      "epoch 82 | loss: 0.92311 | val_0_unsup_loss_numpy: 0.8024200201034546|  0:02:06s\n",
      "\n",
      "Early stopping occurred at epoch 82 with best_epoch = 72 and best_val_0_unsup_loss_numpy = 0.7912899851799011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\miniconda3\\envs\\pytorch\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    }
   ],
   "source": [
    "tabnet_params = {\n",
    "    \"n_d\": 512,\n",
    "    \"n_a\": 512,\n",
    "    \"n_steps\": 3,\n",
    "    \"n_shared\": 2,\n",
    "    \"n_independent\": 2,\n",
    "    \"gamma\": 1.3,\n",
    "    \"epsilon\": 1e-15,\n",
    "    \"momentum\": 0.98,\n",
    "    \"mask_type\": \"sparsemax\",\n",
    "    \"lambda_sparse\": 1e-3,\n",
    "    \"device_name\": \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "}\n",
    "\n",
    "\n",
    "unsupervised_model = TabNetPretrainer(\n",
    "    optimizer_fn=torch.optim.Adam,\n",
    "    optimizer_params=dict(lr=2e-2),\n",
    "    **tabnet_params\n",
    ")\n",
    " \n",
    "\n",
    "unsupervised_model.fit(\n",
    "    X_train,\n",
    "    eval_set=[X_test],  \n",
    "    pretraining_ratio=0.8,\n",
    "    max_epochs=101,\n",
    "    patience=10,\n",
    "    batch_size=1024,\n",
    "    virtual_batch_size=128,\n",
    "    num_workers=0,\n",
    "    drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==============================================================================================================\n",
       "Layer (type:depth-idx)                                       Output Shape              Param #\n",
       "==============================================================================================================\n",
       "TabNetPretraining                                            [19430, 33]               --\n",
       "├─EmbeddingGenerator: 1-1                                    [19430, 33]               --\n",
       "├─TabNetEncoder: 1-2                                         [19430, 512]              --\n",
       "│    └─BatchNorm1d: 2-1                                      [19430, 33]               66\n",
       "│    └─FeatTransformer: 2-2                                  [19430, 1024]             4,202,496\n",
       "│    │    └─GLU_Block: 3-1                                   [19430, 1024]             2,172,928\n",
       "│    └─ModuleList: 2-12                                      --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-17                            --                        (recursive)\n",
       "│    └─FeatTransformer: 2-6                                  --                        (recursive)\n",
       "│    │    └─GLU_Block: 3-5                                   --                        (recursive)\n",
       "│    └─ModuleList: 2-12                                      --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-17                            --                        (recursive)\n",
       "│    └─FeatTransformer: 2-6                                  --                        (recursive)\n",
       "│    │    └─GLU_Block: 3-5                                   --                        (recursive)\n",
       "│    │    └─GLU_Block: 3-6                                   [19430, 1024]             4,202,496\n",
       "│    └─ModuleList: 2-11                                      --                        (recursive)\n",
       "│    │    └─AttentiveTransformer: 3-7                        [19430, 33]               16,962\n",
       "│    └─ModuleList: 2-12                                      --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-8                             [19430, 1024]             6,375,424\n",
       "│    │    └─FeatTransformer: 3-17                            --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-12                            --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-17                            --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-12                            --                        (recursive)\n",
       "│    └─ModuleList: 2-11                                      --                        (recursive)\n",
       "│    │    └─AttentiveTransformer: 3-13                       [19430, 33]               16,962\n",
       "│    └─ModuleList: 2-12                                      --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-14                            [19430, 1024]             6,375,424\n",
       "│    │    └─FeatTransformer: 3-17                            --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-18                            --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-17                            --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-18                            --                        (recursive)\n",
       "│    └─ModuleList: 2-11                                      --                        (recursive)\n",
       "│    │    └─AttentiveTransformer: 3-19                       [19430, 33]               16,962\n",
       "│    └─ModuleList: 2-12                                      --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-20                            [19430, 1024]             6,375,424\n",
       "├─TabNetDecoder: 1-3                                         [19430, 33]               --\n",
       "│    └─ModuleList: 2-13                                      --                        --\n",
       "│    │    └─FeatTransformer: 3-21                            [19430, 512]              1,052,672\n",
       "│    │    └─FeatTransformer: 3-25                            --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-23                            --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-24                            [19430, 512]              1,052,672\n",
       "│    │    └─FeatTransformer: 3-25                            --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-26                            --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-27                            [19430, 512]              1,052,672\n",
       "│    └─Linear: 2-14                                          [19430, 33]               16,896\n",
       "==============================================================================================================\n",
       "Total params: 51,409,160\n",
       "Trainable params: 51,409,160\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 558.18\n",
       "==============================================================================================================\n",
       "Input size (MB): 2.56\n",
       "Forward/backward pass size (MB): 12138.00\n",
       "Params size (MB): 84.74\n",
       "Estimated Total Size (MB): 12225.30\n",
       "=============================================================================================================="
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Truy cập vào mô hình TabNet bên trong\n",
    "from torchinfo import summary\n",
    "\n",
    "tabnet_model = unsupervised_model.network.to(device)\n",
    "\n",
    "summary(tabnet_model, input_size=X_train.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Encoder Summary:\n",
      "TabNetEncoder(\n",
      "  (initial_bn): BatchNorm1d(33, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  (initial_splitter): FeatTransformer(\n",
      "    (shared): GLU_Block(\n",
      "      (shared_layers): ModuleList(\n",
      "        (0): Linear(in_features=33, out_features=2048, bias=False)\n",
      "        (1): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "      )\n",
      "      (glu_layers): ModuleList(\n",
      "        (0): GLU_Layer(\n",
      "          (fc): Linear(in_features=33, out_features=2048, bias=False)\n",
      "          (bn): GBN(\n",
      "            (bn): BatchNorm1d(2048, eps=1e-05, momentum=0.98, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): GLU_Layer(\n",
      "          (fc): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "          (bn): GBN(\n",
      "            (bn): BatchNorm1d(2048, eps=1e-05, momentum=0.98, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (specifics): GLU_Block(\n",
      "      (glu_layers): ModuleList(\n",
      "        (0-1): 2 x GLU_Layer(\n",
      "          (fc): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "          (bn): GBN(\n",
      "            (bn): BatchNorm1d(2048, eps=1e-05, momentum=0.98, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (feat_transformers): ModuleList(\n",
      "    (0-2): 3 x FeatTransformer(\n",
      "      (shared): GLU_Block(\n",
      "        (shared_layers): ModuleList(\n",
      "          (0): Linear(in_features=33, out_features=2048, bias=False)\n",
      "          (1): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "        )\n",
      "        (glu_layers): ModuleList(\n",
      "          (0): GLU_Layer(\n",
      "            (fc): Linear(in_features=33, out_features=2048, bias=False)\n",
      "            (bn): GBN(\n",
      "              (bn): BatchNorm1d(2048, eps=1e-05, momentum=0.98, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (1): GLU_Layer(\n",
      "            (fc): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "            (bn): GBN(\n",
      "              (bn): BatchNorm1d(2048, eps=1e-05, momentum=0.98, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (specifics): GLU_Block(\n",
      "        (glu_layers): ModuleList(\n",
      "          (0-1): 2 x GLU_Layer(\n",
      "            (fc): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "            (bn): GBN(\n",
      "              (bn): BatchNorm1d(2048, eps=1e-05, momentum=0.98, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (att_transformers): ModuleList(\n",
      "    (0-2): 3 x AttentiveTransformer(\n",
      "      (fc): Linear(in_features=512, out_features=33, bias=False)\n",
      "      (bn): GBN(\n",
      "        (bn): BatchNorm1d(33, eps=1e-05, momentum=0.98, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (selector): Sparsemax()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "encoder = tabnet_model.encoder\n",
    "\n",
    "print(\"\\nEncoder Summary:\")\n",
    "print(encoder)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decoder Summary:\n",
      "TabNetDecoder(\n",
      "  (feat_transformers): ModuleList(\n",
      "    (0-2): 3 x FeatTransformer(\n",
      "      (shared): GLU_Block(\n",
      "        (shared_layers): ModuleList(\n",
      "          (0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        )\n",
      "        (glu_layers): ModuleList(\n",
      "          (0): GLU_Layer(\n",
      "            (fc): Linear(in_features=512, out_features=1024, bias=False)\n",
      "            (bn): GBN(\n",
      "              (bn): BatchNorm1d(1024, eps=1e-05, momentum=0.98, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (specifics): GLU_Block(\n",
      "        (glu_layers): ModuleList(\n",
      "          (0): GLU_Layer(\n",
      "            (fc): Linear(in_features=512, out_features=1024, bias=False)\n",
      "            (bn): GBN(\n",
      "              (bn): BatchNorm1d(1024, eps=1e-05, momentum=0.98, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (reconstruction_layer): Linear(in_features=512, out_features=33, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "decoder = tabnet_model.decoder\n",
    "\n",
    "print(\"\\nDecoder Summary:\")\n",
    "print(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TabNet encoder trả về 2 giá trị.\n",
      "Đã xảy ra lỗi: 'list' object has no attribute 'shape'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_23028\\3356785120.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sample_input = torch.tensor(X_train[:5]).to(device)\n"
     ]
    }
   ],
   "source": [
    "sample_input = torch.tensor(X_train[:5]).to(device)  \n",
    "\n",
    "try:\n",
    "    result = tabnet_model.encoder(sample_input)\n",
    "    if isinstance(result, tuple):\n",
    "        print(f'TabNet encoder trả về {len(result)} giá trị.')\n",
    "        for i, res in enumerate(result):\n",
    "            print(f'Giá trị {i + 1} shape: {res.shape}')\n",
    "    else:\n",
    "        print('TabNet encoder chỉ trả về một giá trị.')\n",
    "        print(f'Giá trị shape: {result.shape}')\n",
    "except Exception as e:\n",
    "    print(f'Đã xảy ra lỗi: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(nn.Module):\n",
    "    def __init__(self, seed=1337):\n",
    "        super(Sampling, self).__init__()\n",
    "        self.seed = seed\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = z_mean.size(0)\n",
    "        dim = z_mean.size(1)\n",
    "        # print(batch, dim)\n",
    "        epsilon = torch.randn(batch, dim, generator=torch.Generator().manual_seed(self.seed)).to(device)\n",
    "        return z_mean + torch.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class VAE_Encoder(nn.Module):\n",
    "#     def __init__(self, latent_dim):\n",
    "#         super(VAE_Encoder, self).__init__()\n",
    "#         self.tabnet_encoder = tabnet_model.encoder\n",
    "#         self.mlp = nn.Sequential(\n",
    "#             nn.Linear(512, 256),  \n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(256, 128),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(128, 96),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(96, 96),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(96, 32),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(32, latent_dim)\n",
    "#         ).to(device)\n",
    "#         self.fc_mean = nn.Linear(latent_dim, latent_dim).to(device)\n",
    "#         self.fc_log_var = nn.Linear(latent_dim, latent_dim).to(device)\n",
    "#         self.sampling = Sampling().to(device)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = x.to(device)\n",
    "#         steps_output, _ = self.tabnet_encoder(x)\n",
    "#         encoded = steps_output[-1]\n",
    "#         # print(\"Shape of encoded tensor:\", encoded.shape)\n",
    "#         encoded = self.mlp(encoded)\n",
    "#         z_mean = self.fc_mean(encoded)\n",
    "#         z_log_var = self.fc_log_var(encoded)\n",
    "#         z = self.sampling((z_mean, z_log_var))\n",
    "#         # print(f'Shape of z: {z.shape} - {z_log_var.shape} -{z_log_var.shape}')\n",
    "#         return z_mean, z_log_var, z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE_Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(VAE_Encoder, self).__init__()\n",
    "        # Giữ nguyên TabNet encoder\n",
    "        self.tabnet_encoder = tabnet_model.encoder\n",
    "\n",
    "        # Thay thế MLP bằng Conv1D\n",
    "        self.conv_branch_0 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(128),  # Giữ cố định kích thước\n",
    "        )\n",
    "\n",
    "        self.conv_branch_1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(128),\n",
    "        )\n",
    "\n",
    "        # Ánh xạ sang latent space\n",
    "        self.fc_mean_0 = nn.Linear(32 * 128, latent_dim)\n",
    "        self.fc_log_var_0 = nn.Linear(32 * 128, latent_dim)\n",
    "\n",
    "        self.fc_mean_1 = nn.Linear(32 * 128, latent_dim)\n",
    "        self.fc_log_var_1 = nn.Linear(32 * 128, latent_dim)\n",
    "\n",
    "        self.sampling = Sampling()\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        if labels.dim() == 2 and labels.size(1) == 2:\n",
    "            labels = torch.argmax(labels, dim=1)\n",
    "        labels = labels.long().to(x.device)\n",
    "\n",
    "        # Qua TabNet encoder\n",
    "        steps_output, _ = self.tabnet_encoder(x)\n",
    "        encoded = steps_output[-1]  # shape: [batch_size, 512]\n",
    "\n",
    "        batch_size = encoded.shape[0]\n",
    "        z_mean = torch.zeros(batch_size, self.latent_dim).to(x.device)\n",
    "        z_log_var = torch.zeros(batch_size, self.latent_dim).to(x.device)\n",
    "        z = torch.zeros(batch_size, self.latent_dim).to(x.device)\n",
    "\n",
    "        idx_0 = (labels == 0)\n",
    "        idx_1 = (labels == 1)\n",
    "\n",
    "        # Xử lý lớp 0\n",
    "        if idx_0.any():\n",
    "            h0 = encoded[idx_0].unsqueeze(1)  # [B', 1, 512]\n",
    "            h0 = self.conv_branch_0(h0)       # [B', 32, 128]\n",
    "            h0 = h0.view(h0.shape[0], -1)     # [B', 32*128]\n",
    "\n",
    "            mu0 = self.fc_mean_0(h0)\n",
    "            logvar0 = self.fc_log_var_0(h0)\n",
    "            z0 = self.sampling((mu0, logvar0))\n",
    "\n",
    "            z_mean[idx_0] = mu0\n",
    "            z_log_var[idx_0] = logvar0\n",
    "            z[idx_0] = z0\n",
    "\n",
    "        # Xử lý lớp 1\n",
    "        if idx_1.any():\n",
    "            h1 = encoded[idx_1].unsqueeze(1)  # [B', 1, 512]\n",
    "            h1 = self.conv_branch_1(h1)       # [B', 32, 128]\n",
    "            h1 = h1.view(h1.shape[0], -1)     # [B', 32*128]\n",
    "\n",
    "            mu1 = self.fc_mean_1(h1)\n",
    "            logvar1 = self.fc_log_var_1(h1)\n",
    "            z1 = self.sampling((mu1, logvar1))\n",
    "\n",
    "            z_mean[idx_1] = mu1\n",
    "            z_log_var[idx_1] = logvar1\n",
    "            z[idx_1] = z1\n",
    "\n",
    "        return z_mean, z_log_var, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE_Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, encoded_dim=512, output_dim=None):\n",
    "        super(VAE_Decoder, self).__init__()\n",
    "        self.encoded_dim = encoded_dim\n",
    "        self.output_dim = output_dim  # Số lượng class hoặc số feature cần sinh ra\n",
    "\n",
    "        self.decoder_input = nn.Linear(latent_dim, 32 * encoded_dim)\n",
    "        self.unflatten = nn.Unflatten(1, (32, encoded_dim))\n",
    "\n",
    "        self.conv_decoder = nn.Sequential(\n",
    "            nn.ConvTranspose1d(32, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(16, 1, kernel_size=3, stride=1, padding=1),\n",
    "        )\n",
    "\n",
    "        # Dùng TabNet decoder cuối cùng\n",
    "        self.tabnet_decoder = tabnet_model.decoder\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = self.decoder_input(z)  # [B, 32 * 512]\n",
    "        x = self.unflatten(x)  # [B, 32, 512]\n",
    "        x = self.conv_decoder(x)  # [B, 1, 512]\n",
    "        x = x.squeeze(1)  # [B, 512]\n",
    "\n",
    "        # Truyền vào TabNet decoder\n",
    "        x = x.unsqueeze(0)  # [1, B, 512]\n",
    "        output = self.tabnet_decoder(x)  # [T, B, output_dim]\n",
    "\n",
    "        # Áp dụng softmax trên dim cuối cùng\n",
    "        output = F.softmax(output, dim=-1)\n",
    "\n",
    "        # Flatten lại nếu cần so sánh với ground-truth one-hot\n",
    "        output = output.view(-1, self.output_dim)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data_range(tensor, name):\n",
    "    if not torch.all((tensor >= 0) & (tensor <= 1)):\n",
    "        print(f\"{name} contains values outside the range [0, 1]\")\n",
    "        print(f\"{name} min: {tensor.min()}, max: {tensor.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE_Tabnet_MLPS(nn.Module):\n",
    "    def __init__(self, encoder, decoder, classifier):\n",
    "        super(VAE_Tabnet_MLPS, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.classifier = classifier\n",
    "        self.total_loss_tracker = []\n",
    "        self.reconstruction_loss_tracker = []\n",
    "        self.kl_loss_tracker = []\n",
    "        self.classification_loss_tracker = []\n",
    "        self.accuracy_tracker = []\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        z_mean, z_log_var, z = self.encoder(x, labels)\n",
    "        reconstruction = self.decoder(z)\n",
    "        classification_output = self.classifier(z)\n",
    "        return reconstruction, z_mean, z_log_var, classification_output\n",
    "\n",
    "    def train_step(self, data, labels, optimizer):\n",
    "        labels = labels.squeeze(1) \n",
    "        labels = torch.argmax(labels, dim=1).long()\n",
    "        labels = labels.to(device)\n",
    "        # print(f\"Shape of labels: {labels.shape}\")\n",
    "        optimizer.zero_grad()\n",
    "        # z_mean, z_log_var, z = self.encoder(data)\n",
    "        # reconstruction = self.decoder(z)\n",
    "        reconstruction, z_mean, z_log_var, classification_output = self.forward(data, labels)\n",
    "        # print('classifi',classification_output.shape)\n",
    "        # print(check_data_range(data, 'data'))\n",
    "        # print(check_data_range(reconstruction, 'reconstruction'))\n",
    "        # reconstruction_loss = torch.mean(\n",
    "        #     torch.sum(\n",
    "        #         F.binary_cross_entropy(reconstruction, data, reduction='none'),\n",
    "        #         dim=1\n",
    "        #     )\n",
    "        # )\n",
    "        reconstruction_loss = torch.mean(\n",
    "            torch.sum(\n",
    "                F.binary_cross_entropy_with_logits(reconstruction, data, reduction='none'),\n",
    "                dim=1\n",
    "                # dim=(1, 2)\n",
    "                )  \n",
    "        )\n",
    "\n",
    "        # print(\"labels:\", labels)\n",
    "        # print(\"labels.shape:\", labels.shape)\n",
    "        # print(\"labels.dtype:\", labels.dtype)\n",
    "        # print(\"labels.min():\", labels.min().item(), \"labels.max():\", labels.max().item())\n",
    "        # print(\"classification_output.shape:\", classification_output.shape)\n",
    "\n",
    "        classification_loss = F.cross_entropy(classification_output, labels)\n",
    "                # dim=1\n",
    "                # dim=(1, 2)\n",
    "                \n",
    "        \n",
    "        kl_loss = -0.5 * torch.sum(1 + z_log_var - z_mean.pow(2) - z_log_var.exp(), dim=1)\n",
    "        kl_loss = torch.mean(torch.sum(kl_loss))\n",
    "        total_loss = reconstruction_loss + kl_loss + classification_loss\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        self.total_loss_tracker.append(total_loss.item())\n",
    "        self.reconstruction_loss_tracker.append(reconstruction_loss.item())\n",
    "        self.kl_loss_tracker.append(kl_loss.item())\n",
    "        self.classification_loss_tracker.append(classification_loss.item())\n",
    "        # print(classification_output.shape, labels.shape)\n",
    "\n",
    "        preds = torch.softmax(classification_output, dim=1)\n",
    "        pred_labels = torch.argmax(preds, dim=1)\n",
    "        correct = (pred_labels == labels).float().sum()\n",
    "        accuracy = correct / labels.size(0)\n",
    "        self.accuracy_tracker.append(accuracy.item())\n",
    "\n",
    "        return {\n",
    "            \"loss\": total_loss.item(),\n",
    "            \"reconstruction_loss\": reconstruction_loss.item(),\n",
    "            \"kl_loss\": kl_loss.item(),\n",
    "            \"classification_loss\": classification_loss.item(),\n",
    "            \"accuracy\": accuracy.item()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 64\n",
    "encoded_dim = 512\n",
    "output_dim = X_train.shape[1]\n",
    "input_dim = X_train.shape[1]\n",
    "print(input_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(SimpleClassifier, self).__init__()\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "\n",
    "            nn.Linear(64, 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "\n",
    "            nn.Linear(32, output_dim),\n",
    "            # nn.Softmax()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print('input: ',x.shape)\n",
    "        output = self.fc_layers(x)\n",
    "        # output = output.view(-1)\n",
    "        # print('output',output.shape)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SimpleClassifier(latent_dim, output_dim=2).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input size: torch.Size([32, 64])\n",
      "Output size: torch.Size([32, 1])\n",
      "Output: tensor([[ 0.3256],\n",
      "        [ 1.0663],\n",
      "        [-0.0336],\n",
      "        [-0.2313],\n",
      "        [ 0.0378],\n",
      "        [ 0.1005],\n",
      "        [-0.3622],\n",
      "        [-0.0750],\n",
      "        [ 0.7117],\n",
      "        [ 0.0388],\n",
      "        [-0.6199],\n",
      "        [ 0.4649],\n",
      "        [-0.3589],\n",
      "        [-0.0025],\n",
      "        [-0.6826],\n",
      "        [-0.1581],\n",
      "        [-0.6116],\n",
      "        [ 0.1627],\n",
      "        [ 0.2698],\n",
      "        [-0.1503],\n",
      "        [ 0.1860],\n",
      "        [-0.0651],\n",
      "        [-0.2239],\n",
      "        [-0.4741],\n",
      "        [ 0.1823],\n",
      "        [ 0.6643],\n",
      "        [-0.0823],\n",
      "        [ 0.2351],\n",
      "        [ 0.0924],\n",
      "        [-0.1615],\n",
      "        [-0.1163],\n",
      "        [-0.6726]])\n"
     ]
    }
   ],
   "source": [
    "def check_output(model, input_tensor):\n",
    "    with torch.no_grad():  \n",
    "        output = model(input_tensor)\n",
    "        print(f\"Input size: {input_tensor.size()}\")\n",
    "        print(f\"Output size: {output.size()}\")\n",
    "        print(f\"Output: {output}\")\n",
    "\n",
    "model = SimpleClassifier(latent_dim, output_dim=1)\n",
    "\n",
    "input_tensor = torch.randn(32,latent_dim)  \n",
    "\n",
    "check_output(model, input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_encoder = VAE_Encoder(latent_dim=latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vae_encoder = VAE_Encoder(latent_dim=latent_dim)\n",
    "# print(\"Encoder Summary:\")\n",
    "# # vae_encoder.to(device)\n",
    "\n",
    "# summary(vae_encoder, input_size=(32, input_dim), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded shape: torch.Size([800, 512])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(800, X_train.shape[1]).to(device)\n",
    "steps_output, _ = tabnet_model.encoder(x)\n",
    "encoded = steps_output[-1]\n",
    "print(f\"Encoded shape: {encoded.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of encoder output: [torch.Size([800, 512]), torch.Size([800, 512]), torch.Size([800, 512])]\n",
      "Decoder shape: torch.Size([800, 33])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.randn(800, X_train.shape[1]).to(device)  # Đầu vào có kích thước (batch_size, features)\n",
    "\n",
    "steps_output, _ = tabnet_model.encoder(x)\n",
    "print(\"Shape of encoder output:\", [output.shape for output in steps_output])\n",
    "\n",
    "decoder_input = steps_output[-1]  \n",
    "decoder_input = decoder_input[None, ...]\n",
    "try:\n",
    "    decoder_output = tabnet_model.decoder(decoder_input)\n",
    "    print(f\"Decoder shape: {decoder_output.shape}\")\n",
    "except ValueError as e:\n",
    "    print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 33)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dim, output_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder Summary:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==============================================================================================================\n",
       "Layer (type:depth-idx)                                       Output Shape              Param #\n",
       "==============================================================================================================\n",
       "VAE_Decoder                                                  [32, 33]                  --\n",
       "├─Sequential: 1-1                                            [32, 512]                 --\n",
       "│    └─Linear: 2-1                                           [32, 32]                  2,080\n",
       "│    └─ReLU: 2-2                                             [32, 32]                  --\n",
       "│    └─Linear: 2-3                                           [32, 96]                  3,168\n",
       "│    └─ReLU: 2-4                                             [32, 96]                  --\n",
       "│    └─Linear: 2-5                                           [32, 96]                  9,312\n",
       "│    └─ReLU: 2-6                                             [32, 96]                  --\n",
       "│    └─Linear: 2-7                                           [32, 128]                 12,416\n",
       "│    └─ReLU: 2-8                                             [32, 128]                 --\n",
       "│    └─Linear: 2-9                                           [32, 256]                 33,024\n",
       "│    └─ReLU: 2-10                                            [32, 256]                 --\n",
       "│    └─Linear: 2-11                                          [32, 512]                 131,584\n",
       "├─Unflatten: 1-2                                             [32, 512]                 --\n",
       "├─TabNetDecoder: 1-3                                         [32, 33]                  --\n",
       "│    └─ModuleList: 2-12                                      --                        --\n",
       "│    │    └─FeatTransformer: 3-1                             [32, 512]                 1,052,672\n",
       "│    │    └─FeatTransformer: 3-2                             --                        1,052,672\n",
       "│    │    └─FeatTransformer: 3-3                             --                        (recursive)\n",
       "│    └─Linear: 2-13                                          [32, 33]                  16,896\n",
       "==============================================================================================================\n",
       "Total params: 2,842,208\n",
       "Trainable params: 2,842,208\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 40.36\n",
       "==============================================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 1.34\n",
       "Params size (MB): 5.04\n",
       "Estimated Total Size (MB): 6.40\n",
       "=============================================================================================================="
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae_decoder = VAE_Decoder(latent_dim=latent_dim, encoded_dim=encoded_dim, output_dim=output_dim).to(device)\n",
    "print(\"Decoder Summary:\")\n",
    "summary(vae_decoder, input_size=(32, latent_dim), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE_Tabnet_MLPS(encoder=vae_encoder, decoder=vae_decoder,classifier=classifier).to(device)\n",
    "# summary(vae, input_size=(32, input_dim), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning_rate = 0.0001\n",
    "# optimizer = optim.Adam(vae.parameters(), lr=learning_rate)\n",
    "# num_epochs = 10\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     vae.train()\n",
    "#     train_loss = 0\n",
    "#     rec_loss = 0\n",
    "#     kl_loss = 0\n",
    "#     classification_loss = 0\n",
    "#     accuracy = 0\n",
    "\n",
    "#     for batch_data, batch_labels in train_loader:\n",
    "#         batch_data = batch_data.to(device)\n",
    "#         batch_labels = batch_labels.to(device)\n",
    "#         # print(f\"Batch data shape: {batch_data.shape}, Batch labels shape: {batch_labels.shape}\")\n",
    "#         results = vae.train_step(batch_data, batch_labels, optimizer)\n",
    "        \n",
    "#         train_loss += results[\"loss\"]\n",
    "#         rec_loss += results[\"reconstruction_loss\"]\n",
    "#         kl_loss += results[\"kl_loss\"]\n",
    "#         classification_loss += results[\"classification_loss\"]\n",
    "#         accuracy += results[\"accuracy\"]\n",
    "\n",
    "#     train_loss /= len(train_loader)\n",
    "#     rec_loss /= len(train_loader)\n",
    "#     kl_loss /= len(train_loader)\n",
    "#     classification_loss /= len(train_loader)\n",
    "#     accuracy /= len(train_loader)\n",
    "\n",
    "#     print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {train_loss:.4f}, Reconstruction Loss: {rec_loss:.4f}, KL Loss: {kl_loss:.4f}, Classification Loss: {classification_loss:.4f}, Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in vae.encoder.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: -402.6622, Reconstruction Loss: -434.8605, KL Loss: 31.5445, Classification Loss: 0.6538, Accuracy: 0.6138\n",
      "Epoch 2/50, Loss: -408.1541, Reconstruction Loss: -440.0485, KL Loss: 31.5452, Classification Loss: 0.3493, Accuracy: 0.8981\n",
      "Epoch 3/50, Loss: -407.1337, Reconstruction Loss: -438.8141, KL Loss: 31.5459, Classification Loss: 0.1345, Accuracy: 0.9818\n",
      "Epoch 4/50, Loss: -408.5418, Reconstruction Loss: -440.1528, KL Loss: 31.5452, Classification Loss: 0.0659, Accuracy: 0.9942\n",
      "Epoch 5/50, Loss: -406.6109, Reconstruction Loss: -438.1958, KL Loss: 31.5440, Classification Loss: 0.0409, Accuracy: 0.9955\n",
      "Epoch 6/50, Loss: -407.6235, Reconstruction Loss: -439.1914, KL Loss: 31.5441, Classification Loss: 0.0237, Accuracy: 0.9984\n",
      "Epoch 7/50, Loss: -437.6828, Reconstruction Loss: -469.2460, KL Loss: 31.5452, Classification Loss: 0.0180, Accuracy: 0.9978\n",
      "Epoch 8/50, Loss: -466.4237, Reconstruction Loss: -497.9810, KL Loss: 31.5447, Classification Loss: 0.0126, Accuracy: 0.9989\n",
      "Epoch 9/50, Loss: -466.4022, Reconstruction Loss: -497.9551, KL Loss: 31.5443, Classification Loss: 0.0086, Accuracy: 0.9994\n",
      "Epoch 10/50, Loss: -466.5154, Reconstruction Loss: -498.0684, KL Loss: 31.5446, Classification Loss: 0.0084, Accuracy: 0.9988\n",
      "Epoch 11/50, Loss: -466.3865, Reconstruction Loss: -497.9366, KL Loss: 31.5452, Classification Loss: 0.0049, Accuracy: 0.9999\n",
      "Epoch 12/50, Loss: -466.4991, Reconstruction Loss: -498.0481, KL Loss: 31.5452, Classification Loss: 0.0038, Accuracy: 0.9998\n",
      "Epoch 13/50, Loss: -466.5331, Reconstruction Loss: -498.0812, KL Loss: 31.5435, Classification Loss: 0.0045, Accuracy: 0.9993\n",
      "Epoch 14/50, Loss: -466.6780, Reconstruction Loss: -498.2249, KL Loss: 31.5441, Classification Loss: 0.0028, Accuracy: 0.9997\n",
      "Epoch 15/50, Loss: -466.4920, Reconstruction Loss: -498.0383, KL Loss: 31.5440, Classification Loss: 0.0023, Accuracy: 0.9998\n",
      "Epoch 16/50, Loss: -466.3507, Reconstruction Loss: -497.8976, KL Loss: 31.5447, Classification Loss: 0.0022, Accuracy: 0.9997\n",
      "Epoch 17/50, Loss: -466.5354, Reconstruction Loss: -498.0822, KL Loss: 31.5446, Classification Loss: 0.0022, Accuracy: 0.9996\n",
      "Epoch 18/50, Loss: -466.5270, Reconstruction Loss: -498.0739, KL Loss: 31.5442, Classification Loss: 0.0027, Accuracy: 0.9995\n",
      "Epoch 19/50, Loss: -466.4337, Reconstruction Loss: -497.9794, KL Loss: 31.5432, Classification Loss: 0.0025, Accuracy: 0.9994\n",
      "Epoch 20/50, Loss: -466.5410, Reconstruction Loss: -498.0873, KL Loss: 31.5449, Classification Loss: 0.0014, Accuracy: 0.9998\n",
      "Epoch 21/50, Loss: -466.3905, Reconstruction Loss: -497.9353, KL Loss: 31.5437, Classification Loss: 0.0012, Accuracy: 0.9998\n",
      "Epoch 22/50, Loss: -466.4740, Reconstruction Loss: -498.0186, KL Loss: 31.5430, Classification Loss: 0.0016, Accuracy: 0.9996\n",
      "Epoch 23/50, Loss: -466.4016, Reconstruction Loss: -497.9463, KL Loss: 31.5438, Classification Loss: 0.0010, Accuracy: 0.9999\n",
      "Epoch 24/50, Loss: -466.4754, Reconstruction Loss: -498.0213, KL Loss: 31.5448, Classification Loss: 0.0011, Accuracy: 0.9997\n",
      "Epoch 25/50, Loss: -466.5233, Reconstruction Loss: -498.0694, KL Loss: 31.5447, Classification Loss: 0.0014, Accuracy: 0.9996\n",
      "Epoch 26/50, Loss: -466.5974, Reconstruction Loss: -498.1424, KL Loss: 31.5445, Classification Loss: 0.0006, Accuracy: 0.9999\n",
      "Epoch 27/50, Loss: -466.5814, Reconstruction Loss: -498.1260, KL Loss: 31.5439, Classification Loss: 0.0007, Accuracy: 0.9998\n",
      "Epoch 28/50, Loss: -466.5510, Reconstruction Loss: -498.0957, KL Loss: 31.5439, Classification Loss: 0.0007, Accuracy: 0.9999\n",
      "Epoch 29/50, Loss: -466.4141, Reconstruction Loss: -497.9606, KL Loss: 31.5447, Classification Loss: 0.0018, Accuracy: 0.9997\n",
      "Epoch 30/50, Loss: -466.4687, Reconstruction Loss: -498.0132, KL Loss: 31.5440, Classification Loss: 0.0005, Accuracy: 0.9999\n",
      "Epoch 31/50, Loss: -466.5590, Reconstruction Loss: -498.1038, KL Loss: 31.5445, Classification Loss: 0.0003, Accuracy: 1.0000\n",
      "Epoch 32/50, Loss: -466.4895, Reconstruction Loss: -498.0350, KL Loss: 31.5452, Classification Loss: 0.0004, Accuracy: 0.9999\n",
      "Epoch 33/50, Loss: -466.6065, Reconstruction Loss: -498.1509, KL Loss: 31.5437, Classification Loss: 0.0007, Accuracy: 0.9998\n",
      "Epoch 34/50, Loss: -466.5113, Reconstruction Loss: -498.0557, KL Loss: 31.5442, Classification Loss: 0.0003, Accuracy: 0.9999\n",
      "Epoch 35/50, Loss: -466.4397, Reconstruction Loss: -497.9851, KL Loss: 31.5442, Classification Loss: 0.0013, Accuracy: 0.9998\n",
      "Epoch 36/50, Loss: -466.3836, Reconstruction Loss: -497.9280, KL Loss: 31.5442, Classification Loss: 0.0002, Accuracy: 1.0000\n",
      "Epoch 37/50, Loss: -466.4700, Reconstruction Loss: -498.0152, KL Loss: 31.5444, Classification Loss: 0.0007, Accuracy: 0.9998\n",
      "Epoch 38/50, Loss: -466.4711, Reconstruction Loss: -498.0160, KL Loss: 31.5446, Classification Loss: 0.0003, Accuracy: 0.9999\n",
      "Epoch 39/50, Loss: -466.6108, Reconstruction Loss: -498.1562, KL Loss: 31.5445, Classification Loss: 0.0009, Accuracy: 0.9998\n",
      "Epoch 40/50, Loss: -466.5439, Reconstruction Loss: -498.0900, KL Loss: 31.5446, Classification Loss: 0.0015, Accuracy: 0.9996\n",
      "Epoch 41/50, Loss: -466.4825, Reconstruction Loss: -498.0272, KL Loss: 31.5440, Classification Loss: 0.0007, Accuracy: 0.9999\n",
      "Epoch 42/50, Loss: -466.6626, Reconstruction Loss: -498.2077, KL Loss: 31.5443, Classification Loss: 0.0008, Accuracy: 0.9997\n",
      "Epoch 43/50, Loss: -466.4407, Reconstruction Loss: -497.9856, KL Loss: 31.5447, Classification Loss: 0.0002, Accuracy: 0.9999\n",
      "Epoch 44/50, Loss: -466.4765, Reconstruction Loss: -498.0214, KL Loss: 31.5445, Classification Loss: 0.0004, Accuracy: 0.9998\n",
      "Epoch 45/50, Loss: -466.4139, Reconstruction Loss: -497.9574, KL Loss: 31.5434, Classification Loss: 0.0002, Accuracy: 1.0000\n",
      "Epoch 46/50, Loss: -466.5875, Reconstruction Loss: -498.1314, KL Loss: 31.5438, Classification Loss: 0.0001, Accuracy: 1.0000\n",
      "Epoch 47/50, Loss: -466.3591, Reconstruction Loss: -497.9043, KL Loss: 31.5443, Classification Loss: 0.0009, Accuracy: 0.9997\n",
      "Epoch 48/50, Loss: -466.5856, Reconstruction Loss: -498.1307, KL Loss: 31.5442, Classification Loss: 0.0009, Accuracy: 0.9998\n",
      "Epoch 49/50, Loss: -466.4670, Reconstruction Loss: -498.0106, KL Loss: 31.5432, Classification Loss: 0.0004, Accuracy: 0.9999\n",
      "Epoch 50/50, Loss: -466.6422, Reconstruction Loss: -498.1874, KL Loss: 31.5448, Classification Loss: 0.0003, Accuracy: 0.9999\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.0001\n",
    "optimizer = optim.Adam(vae.parameters(), lr=learning_rate)\n",
    "num_epochs = 50\n",
    "vae_new = VAE_Tabnet_MLPS(vae.encoder, vae.decoder, vae.classifier).to(device)\n",
    "for param in vae_new.encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, vae_new.parameters()), lr=learning_rate)\n",
    "for epoch in range(num_epochs):\n",
    "    vae_new.train()\n",
    "    train_loss = 0\n",
    "    rec_loss = 0\n",
    "    kl_loss = 0\n",
    "    classification_loss = 0\n",
    "    accuracy = 0\n",
    "\n",
    "    for batch_data, batch_labels in train_loader:\n",
    "        batch_data = batch_data.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "        results = vae.train_step(batch_data, batch_labels, optimizer)\n",
    "        \n",
    "        train_loss += results[\"loss\"]\n",
    "        rec_loss += results[\"reconstruction_loss\"]\n",
    "        kl_loss += results[\"kl_loss\"]\n",
    "        classification_loss += results[\"classification_loss\"]\n",
    "        accuracy += results[\"accuracy\"]\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    rec_loss /= len(train_loader)\n",
    "    kl_loss /= len(train_loader)\n",
    "    classification_loss /= len(train_loader)\n",
    "    accuracy /= len(train_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {train_loss:.4f}, Reconstruction Loss: {rec_loss:.4f}, KL Loss: {kl_loss:.4f}, Classification Loss: {classification_loss:.4f}, Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleDiffusionModel(nn.Module):\n",
    "    def __init__(self, latent_dim, time_steps=1000):\n",
    "        super().__init__()\n",
    "        self.time_steps = time_steps\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Tạo các beta_schedule tuyến tính\n",
    "        beta = torch.linspace(0.0001, 0.02, time_steps)\n",
    "        alpha = 1. - beta\n",
    "        alpha_bar = torch.cumprod(alpha, dim=0)\n",
    "\n",
    "        self.register_buffer('beta', beta)\n",
    "        self.register_buffer('alpha', alpha)\n",
    "        self.register_buffer('alpha_bar', alpha_bar)\n",
    "\n",
    "        # Mạng neural đơn giản để dự đoán nhiễu\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim + 1, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, latent_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, z, t):\n",
    "        noise = torch.randn_like(z)\n",
    "        \n",
    "        # Đảm bảo t là long type và shape phù hợp\n",
    "        if isinstance(t, torch.Tensor):\n",
    "            t = t.to(dtype=torch.long)\n",
    "        else:\n",
    "            t = torch.tensor([t], device=z.device, dtype=torch.long).expand(z.shape[0])\n",
    "\n",
    "        sqrt_alpha_bar = torch.sqrt(self.alpha_bar[t])[:, None]\n",
    "        sqrt_one_minus_alpha_bar = torch.sqrt(1 - self.alpha_bar[t])[:, None]\n",
    "        noisy_z = sqrt_alpha_bar * z + sqrt_one_minus_alpha_bar * noise\n",
    "\n",
    "        predicted_noise = self.model(torch.cat([noisy_z, t.unsqueeze(1)], dim=1))\n",
    "        loss = F.mse_loss(predicted_noise, noise)\n",
    "        return loss\n",
    "\n",
    "    def sample(self, z):\n",
    "        for i in reversed(range(self.time_steps)):\n",
    "            z = self.denoise_step(z, i)\n",
    "        return z\n",
    "    \n",
    "    def denoise_step(self, z, t):\n",
    "        timestep = t.item() if isinstance(t, torch.Tensor) else t\n",
    "        t_batch = torch.full((z.shape[0],), timestep, device=z.device, dtype=torch.long)\n",
    "\n",
    "        predicted_noise = self.model(torch.cat([z, t_batch.unsqueeze(1)], dim=1))\n",
    "\n",
    "        alpha = self.alpha[timestep]\n",
    "        alpha_bar = self.alpha_bar[timestep]\n",
    "        beta = self.beta[timestep]\n",
    "\n",
    "        z = (1 / torch.sqrt(alpha)) * (z - ((1 - alpha) / torch.sqrt(1 - alpha_bar)) * predicted_noise)\n",
    "        if timestep > 0:\n",
    "            noise = torch.randn_like(z)\n",
    "            z += torch.sqrt(beta) * noise\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    vae_new.encoder.eval()\n",
    "    dummy_input = torch.randn(1, input_dim).to(device)  # Thay input_dim theo đúng dữ liệu của bạn\n",
    "    dummy_labels = torch.zeros(1, dtype=torch.long).to(device)\n",
    "    z_mean, z_log_var, _ = vae_new.encoder(dummy_input, dummy_labels)\n",
    "    latent_dim = z_mean.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# latent_dim = vae.encoder[-1].out_features  # Kích thước latent z\n",
    "diffusion_model = SimpleDiffusionModel(latent_dim=latent_dim).to(device)\n",
    "diffusion_optimizer = torch.optim.Adam(diffusion_model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullPipelineModel(nn.Module):\n",
    "    def __init__(self, vae, diffusion, classifier):\n",
    "        super(FullPipelineModel, self).__init__()\n",
    "        self.vae = vae\n",
    "        self.diffusion = diffusion\n",
    "        self.classifier = classifier\n",
    "\n",
    "        self.total_loss = []\n",
    "        self.recon_loss = []\n",
    "        self.kl_loss = []\n",
    "        self.cls_loss = []\n",
    "        self.accuracy = []\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        z_mean, z_log_var, z = self.vae.encoder(x, labels)\n",
    "        reconstruction = self.vae.decoder(z)\n",
    "        z_diffused = self.diffusion.sample(z)  \n",
    "\n",
    "        classification_logits = self.classifier(z_diffused).float()\n",
    "        return reconstruction, z_mean, z_log_var, classification_logits\n",
    "\n",
    "    def train_step(self, x, labels, optimizer):\n",
    "        # print(f\"Shape of x: {x.shape}, Shape of labels: {labels.shape}\")\n",
    "        if labels.dim() == 2 and labels.size(1) == 2:\n",
    "            labels = torch.argmax(labels, dim=1)\n",
    "        # print(f\"Shape of x: {x.shape}, Shape of labels: {labels.shape}\")\n",
    "        # labels = labels.squeeze().long().to(device)\n",
    "        labels = labels.squeeze().to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        reconstruction, z_mean, z_log_var, classification_logits = self.forward(x, labels)\n",
    "        recon_loss = F.binary_cross_entropy_with_logits(reconstruction, x, reduction='mean')\n",
    "        kl_loss = -0.5 * torch.mean(torch.sum(1 + z_log_var - z_mean.pow(2) - z_log_var.exp(), dim=1))\n",
    "        # print(f'type of classification_logits: {classification_logits.dtype}')\n",
    "        # print(f'type of labels: {labels.dtype}')\n",
    "        # print(\"classification_logits.shape:\", classification_logits.shape)\n",
    "        # print(\"labels.shape:\", labels.shape)\n",
    "        cls_loss = F.cross_entropy(classification_logits, labels.float())\n",
    "        total_loss = recon_loss + kl_loss + cls_loss\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        self.total_loss.append(total_loss.item())\n",
    "        self.recon_loss.append(recon_loss.item())\n",
    "        self.kl_loss.append(kl_loss.item())\n",
    "        self.cls_loss.append(cls_loss.item())\n",
    "\n",
    "        preds = torch.argmax(classification_logits, dim=1)\n",
    "        labels = torch.argmax(labels, dim=1)\n",
    "        # print(f'preds: {preds.shape} - labels: {labels.shape}')\n",
    "        acc = (preds == labels).float().mean()\n",
    "        self.accuracy.append(acc.item())\n",
    "\n",
    "        return {\n",
    "            \"total_loss\": total_loss.item(),\n",
    "            \"recon_loss\": recon_loss.item(),\n",
    "            \"kl_loss\": kl_loss.item(),\n",
    "            \"cls_loss\": cls_loss.item(),\n",
    "            \"accuracy\": acc.item()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def train_diffusion(vae, diffusion_model, dataloader, optimizer, device, time_steps=1000, epochs=150):\n",
    "    diffusion_model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch_data, batch_labels in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            batch_data = batch_data.to(device)\n",
    "            batch_labels = batch_labels.squeeze().to(device)\n",
    "            if batch_labels.dim() > 1:  # nếu là one-hot hoặc có chiều phụ\n",
    "                batch_labels = batch_labels.argmax(dim=1)\n",
    "            with torch.no_grad():\n",
    "                z_mean, z_log_var, z = vae.encoder(batch_data, batch_labels)\n",
    "\n",
    "            t = torch.randint(0, time_steps, (z.shape[0],), device=device).long()\n",
    "            loss = diffusion_model(z, t)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Diffusion Loss: {total_loss/len(dataloader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, precision_score, recall_score\n",
    "\n",
    "def evaluate_diffusion_with_classifier(vae, diffusion_model, classifier, test_loader, device, time_steps=1000):\n",
    "    vae.eval()\n",
    "    diffusion_model.eval()\n",
    "    classifier.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_data, batch_labels in test_loader:\n",
    "            batch_data = batch_data.to(device)\n",
    "            batch_labels = batch_labels.squeeze().to(device)\n",
    "            if batch_labels.ndim == 3 and batch_labels.shape[1] == 1:\n",
    "                batch_labels = batch_labels.squeeze(1)  # bỏ chiều 1\n",
    "\n",
    "            if batch_labels.ndim == 2 and batch_labels.shape[1] == 2:\n",
    "                batch_labels = torch.argmax(batch_labels, dim=1)\n",
    "            elif batch_labels.ndim == 2 and batch_labels.shape[1] == 1:\n",
    "                batch_labels = batch_labels.squeeze(1)\n",
    "\n",
    "            z_mean, z_log_var, z = vae.encoder(batch_data, batch_labels)\n",
    "\n",
    "            t_forward = time_steps - 1\n",
    "            sqrt_alpha_bar = torch.sqrt(diffusion_model.alpha_bar[t_forward])\n",
    "            sqrt_one_minus_alpha_bar = torch.sqrt(1 - diffusion_model.alpha_bar[t_forward])\n",
    "            noisy_z = sqrt_alpha_bar * z + sqrt_one_minus_alpha_bar * torch.randn_like(z)\n",
    "\n",
    "            z_recovered = diffusion_model.sample(noisy_z)\n",
    "            logits = classifier(z_recovered)\n",
    "\n",
    "            # print(\"Logits shape:\", logits.shape)  # Debug\n",
    "\n",
    "            if len(logits.shape) == 1:\n",
    "                preds = (torch.sigmoid(logits) > 0.5).float()\n",
    "            elif len(logits.shape) == 2:\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected logits shape: {logits.shape}\")\n",
    "\n",
    "            # print(f\"preds shape: {preds.shape}, batch_labels shape: {batch_labels.shape}\")  # Debug\n",
    "            preds = preds.cpu().numpy().flatten()\n",
    "            labels = batch_labels.squeeze().cpu().numpy().flatten()\n",
    "\n",
    "            # Lưu lại cho metrics\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels)\n",
    "\n",
    "            correct += (preds == labels).sum()\n",
    "            total += len(labels)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(all_labels, all_preds))\n",
    "\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(all_labels, all_preds))\n",
    "\n",
    "    # Nếu bạn muốn trả về thêm các chỉ số cụ thể:\n",
    "    accuracy = sum(np.array(all_preds) == np.array(all_labels)) / len(all_labels)\n",
    "    f1 = f1_score(all_labels, all_preds, average='binary' if len(np.unique(all_labels)) == 2 else 'macro')\n",
    "    precision = precision_score(all_labels, all_preds, average='binary' if len(np.unique(all_labels)) == 2 else 'macro')\n",
    "    recall = recall_score(all_labels, all_preds, average='binary' if len(np.unique(all_labels)) == 2 else 'macro')\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"f1_score\": f1,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall\n",
    "    }\n",
    "    print(f\"Accuracy on recovered z: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# latent_dim = vae.encoder[-1].out_features\n",
    "diffusion_model = SimpleDiffusionModel(latent_dim=latent_dim).to(device)\n",
    "diffusion_optimizer = optim.Adam(diffusion_model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss: -15.7032 | Acc: 0.7632\n",
      "Epoch 2 | Loss: -13.0239 | Acc: 0.6842\n"
     ]
    }
   ],
   "source": [
    "# Giả sử bạn đã có sẵn các model con\n",
    "full_model = FullPipelineModel(vae=vae, diffusion=diffusion_model, classifier=classifier).to(device)\n",
    "optimizer = torch.optim.Adam(full_model.parameters(), lr=1e-4)\n",
    "\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    full_model.train()\n",
    "    for batch_data, batch_labels in train_loader:\n",
    "        batch_data = batch_data.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "\n",
    "        # Nếu label là one-hot encoded, chuyển thành dạng long\n",
    "        if batch_labels.dim() == 2 and batch_labels.size(1) == 2:\n",
    "            batch_labels = torch.argmax(batch_labels, dim=1)\n",
    "\n",
    "        metrics = full_model.train_step(batch_data, batch_labels, optimizer)\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Loss: {metrics['total_loss']:.4f} | Acc: {metrics['accuracy']:.4f}\")\n",
    "\n",
    "metrics = evaluate_diffusion_with_classifier(\n",
    "    vae=vae,\n",
    "    diffusion_model=diffusion_model,\n",
    "    classifier=classifier,\n",
    "    test_loader=test_loader,\n",
    "    device=device,\n",
    "    time_steps=100\n",
    ")\n",
    "\n",
    "print(f\"Accuracy on recovered z: {metrics['accuracy']:.4f}\")\n",
    "print(f\"F1 Score: {metrics['f1_score']:.4f}\")\n",
    "print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "print(f\"Recall: {metrics['recall']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # latent_dim = vae.encoder[-1].out_features\n",
    "# diffusion_model = SimpleDiffusionModel(latent_dim=latent_dim).to(device)\n",
    "# diffusion_optimizer = optim.Adam(diffusion_model.parameters(), lr=1e-3)\n",
    "\n",
    "# train_diffusion(vae_new, diffusion_model, train_loader, diffusion_optimizer, device, epochs=100)\n",
    "\n",
    "# evaluate_diffusion_with_classifier(vae_new, diffusion_model, vae.classifier, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
