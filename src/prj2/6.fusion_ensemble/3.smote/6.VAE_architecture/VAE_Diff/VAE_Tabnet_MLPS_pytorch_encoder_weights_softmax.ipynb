{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0\n",
      "NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  \n",
    "print(torch.cuda.current_device())  \n",
    "print(torch.cuda.get_device_name(torch.cuda.current_device())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\Data\\ads_fraud_detection\n",
      "c:/Users/Admin/Data/ads_fraud_detection\n"
     ]
    }
   ],
   "source": [
    "import sys,os\n",
    "__script_path=os.path.abspath(globals().get('__file__','.'))\n",
    "__script_dir = os.path.dirname(__script_path)\n",
    "root_dir = os.path.abspath(f'{__script_dir}/../../../../..')\n",
    "print(root_dir)\n",
    "for lib in [root_dir][::-1]:\n",
    "    if lib in sys.path:\n",
    "        sys.path.remove(lib)\n",
    "    sys.path.insert(0,lib)\n",
    "from config.config import *\n",
    "# from libs.common import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir=f\"{exps_dir}/exp2/exp_smote\"\n",
    "if os.path.exists(save_dir) == False: \n",
    "  os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "test_size=0.33\n",
    "seed=42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. Using GPU.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"CUDA is available. Using GPU.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA is not available. Using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from pytorch_tabnet.pretraining import TabNetPretrainer\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.6378163041991519, 1: 2.3140088827134457}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train=pd.read_excel(f'{save_dir}/x_train.xlsx')\n",
    "y_train=pd.read_excel(f'{save_dir}/y_train.xlsx')\n",
    "x_test=pd.read_excel(f'{save_dir}/x_test.xlsx')\n",
    "y_test=pd.read_excel(f'{save_dir}/y_test.xlsx')\n",
    "class_weights_dict=dict(np.load(f'{save_dir}/class_weights_dict.npz',allow_pickle=True))['class_weights_dict']\n",
    "class_weights_dict = {key: value for key, value in class_weights_dict.item().items()}\n",
    "class_weights_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_indices = sorted(class_weights_dict.keys())\n",
    "class_weights_list = [class_weights_dict[c] for c in class_indices]\n",
    "\n",
    "# Chuyển thành tensor\n",
    "class_weights_tensor = torch.tensor(class_weights_list, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([19430, 33]) torch.Size([19430, 1, 2]) torch.Size([5089, 33]) torch.Size([5089, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "X_train = torch.tensor(x_train.values, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train.values, dtype=torch.long)\n",
    "X_test = torch.tensor(x_test.values, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test.values, dtype=torch.long)\n",
    "num_classes = 2  # Assuming binary classification\n",
    "y_train = F.one_hot(y_train, num_classes=num_classes)\n",
    "y_test = F.one_hot(y_test, num_classes=num_classes)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\miniconda3\\envs\\pytorch\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1941.51756| val_0_unsup_loss_numpy: 21.037090301513672|  0:00:01s\n",
      "epoch 1  | loss: 22.06558| val_0_unsup_loss_numpy: 4.085599899291992|  0:00:03s\n",
      "epoch 2  | loss: 8.62228 | val_0_unsup_loss_numpy: 6.7566399574279785|  0:00:04s\n",
      "epoch 3  | loss: 8.90729 | val_0_unsup_loss_numpy: 2.1685800552368164|  0:00:06s\n",
      "epoch 4  | loss: 4.31033 | val_0_unsup_loss_numpy: 88.85392761230469|  0:00:07s\n",
      "epoch 5  | loss: 6.68755 | val_0_unsup_loss_numpy: 1.4134299755096436|  0:00:09s\n",
      "epoch 6  | loss: 4.51434 | val_0_unsup_loss_numpy: 1.9488600492477417|  0:00:10s\n",
      "epoch 7  | loss: 5.04796 | val_0_unsup_loss_numpy: 1.7437700033187866|  0:00:12s\n",
      "epoch 8  | loss: 2.8203  | val_0_unsup_loss_numpy: 6.641610145568848|  0:00:13s\n",
      "epoch 9  | loss: 2.32682 | val_0_unsup_loss_numpy: 1.6450400352478027|  0:00:15s\n",
      "epoch 10 | loss: 2.08004 | val_0_unsup_loss_numpy: 2.615489959716797|  0:00:16s\n",
      "epoch 11 | loss: 2.23871 | val_0_unsup_loss_numpy: 1.3475799560546875|  0:00:18s\n",
      "epoch 12 | loss: 1.88427 | val_0_unsup_loss_numpy: 1.262120008468628|  0:00:20s\n",
      "epoch 13 | loss: 1.56024 | val_0_unsup_loss_numpy: 1.3143600225448608|  0:00:21s\n",
      "epoch 14 | loss: 1.36775 | val_0_unsup_loss_numpy: 1.360550045967102|  0:00:23s\n",
      "epoch 15 | loss: 1.37813 | val_0_unsup_loss_numpy: 1.2533899545669556|  0:00:24s\n",
      "epoch 16 | loss: 1.25151 | val_0_unsup_loss_numpy: 1.191249966621399|  0:00:26s\n",
      "epoch 17 | loss: 1.22788 | val_0_unsup_loss_numpy: 1.1953999996185303|  0:00:27s\n",
      "epoch 18 | loss: 1.2381  | val_0_unsup_loss_numpy: 1.2280900478363037|  0:00:29s\n",
      "epoch 19 | loss: 1.13947 | val_0_unsup_loss_numpy: 1.0085899829864502|  0:00:30s\n",
      "epoch 20 | loss: 1.0784  | val_0_unsup_loss_numpy: 1.0561800003051758|  0:00:32s\n",
      "epoch 21 | loss: 1.04977 | val_0_unsup_loss_numpy: 1.0153599977493286|  0:00:34s\n",
      "epoch 22 | loss: 1.14197 | val_0_unsup_loss_numpy: 1.0738099813461304|  0:00:35s\n",
      "epoch 23 | loss: 1.08661 | val_0_unsup_loss_numpy: 1.0555000305175781|  0:00:37s\n",
      "epoch 24 | loss: 1.18632 | val_0_unsup_loss_numpy: 1.277359962463379|  0:00:38s\n",
      "epoch 25 | loss: 1.09346 | val_0_unsup_loss_numpy: 1.1215900182724|  0:00:40s\n",
      "epoch 26 | loss: 1.09819 | val_0_unsup_loss_numpy: 1.1039700508117676|  0:00:41s\n",
      "epoch 27 | loss: 1.12579 | val_0_unsup_loss_numpy: 1.1165399551391602|  0:00:43s\n",
      "epoch 28 | loss: 1.09386 | val_0_unsup_loss_numpy: 1.0944099426269531|  0:00:44s\n",
      "epoch 29 | loss: 1.04237 | val_0_unsup_loss_numpy: 0.9616699814796448|  0:00:46s\n",
      "epoch 30 | loss: 1.0422  | val_0_unsup_loss_numpy: 1.2115700244903564|  0:00:48s\n",
      "epoch 31 | loss: 1.03273 | val_0_unsup_loss_numpy: 0.9550399780273438|  0:00:49s\n",
      "epoch 32 | loss: 1.03919 | val_0_unsup_loss_numpy: 1.02128005027771|  0:00:51s\n",
      "epoch 33 | loss: 1.05916 | val_0_unsup_loss_numpy: 0.9645599722862244|  0:00:52s\n",
      "epoch 34 | loss: 1.14837 | val_0_unsup_loss_numpy: 1.2175099849700928|  0:00:54s\n",
      "epoch 35 | loss: 1.13161 | val_0_unsup_loss_numpy: 1.0161700248718262|  0:00:55s\n",
      "epoch 36 | loss: 1.04864 | val_0_unsup_loss_numpy: 0.9416400194168091|  0:00:57s\n",
      "epoch 37 | loss: 1.03796 | val_0_unsup_loss_numpy: 0.9466099739074707|  0:00:59s\n",
      "epoch 38 | loss: 1.04712 | val_0_unsup_loss_numpy: 1.1171000003814697|  0:01:00s\n",
      "epoch 39 | loss: 1.03224 | val_0_unsup_loss_numpy: 1.0069600343704224|  0:01:02s\n",
      "epoch 40 | loss: 1.01635 | val_0_unsup_loss_numpy: 0.9125400185585022|  0:01:03s\n",
      "epoch 41 | loss: 0.96449 | val_0_unsup_loss_numpy: 0.9045199751853943|  0:01:05s\n",
      "epoch 42 | loss: 0.96617 | val_0_unsup_loss_numpy: 0.90038001537323|  0:01:06s\n",
      "epoch 43 | loss: 0.98035 | val_0_unsup_loss_numpy: 0.8801800012588501|  0:01:08s\n",
      "epoch 44 | loss: 0.98193 | val_0_unsup_loss_numpy: 0.895039975643158|  0:01:10s\n",
      "epoch 45 | loss: 0.96042 | val_0_unsup_loss_numpy: 0.8669599890708923|  0:01:11s\n",
      "epoch 46 | loss: 0.95853 | val_0_unsup_loss_numpy: 0.8800399899482727|  0:01:13s\n",
      "epoch 47 | loss: 0.95059 | val_0_unsup_loss_numpy: 0.8652499914169312|  0:01:14s\n",
      "epoch 48 | loss: 0.97695 | val_0_unsup_loss_numpy: 0.8845800161361694|  0:01:16s\n",
      "epoch 49 | loss: 0.99737 | val_0_unsup_loss_numpy: 0.8735799789428711|  0:01:18s\n",
      "epoch 50 | loss: 0.94668 | val_0_unsup_loss_numpy: 0.8738600015640259|  0:01:19s\n",
      "epoch 51 | loss: 0.96331 | val_0_unsup_loss_numpy: 0.8837500214576721|  0:01:21s\n",
      "epoch 52 | loss: 0.98914 | val_0_unsup_loss_numpy: 0.8901900053024292|  0:01:22s\n",
      "epoch 53 | loss: 0.96705 | val_0_unsup_loss_numpy: 0.8846499919891357|  0:01:24s\n",
      "epoch 54 | loss: 0.95374 | val_0_unsup_loss_numpy: 0.900950014591217|  0:01:26s\n",
      "epoch 55 | loss: 0.95413 | val_0_unsup_loss_numpy: 0.8563699722290039|  0:01:27s\n",
      "epoch 56 | loss: 0.93821 | val_0_unsup_loss_numpy: 0.82819002866745|  0:01:29s\n",
      "epoch 57 | loss: 0.93614 | val_0_unsup_loss_numpy: 0.8290600180625916|  0:01:31s\n",
      "epoch 58 | loss: 0.94227 | val_0_unsup_loss_numpy: 0.8391900062561035|  0:01:32s\n",
      "epoch 59 | loss: 0.98242 | val_0_unsup_loss_numpy: 0.8648899793624878|  0:01:34s\n",
      "epoch 60 | loss: 0.94819 | val_0_unsup_loss_numpy: 0.8229299783706665|  0:01:35s\n",
      "epoch 61 | loss: 0.94446 | val_0_unsup_loss_numpy: 0.8197299838066101|  0:01:37s\n",
      "epoch 62 | loss: 0.97418 | val_0_unsup_loss_numpy: 0.9168499708175659|  0:01:39s\n",
      "epoch 63 | loss: 1.02273 | val_0_unsup_loss_numpy: 0.822160005569458|  0:01:40s\n",
      "epoch 64 | loss: 0.96167 | val_0_unsup_loss_numpy: 0.8324000239372253|  0:01:42s\n",
      "epoch 65 | loss: 0.94561 | val_0_unsup_loss_numpy: 0.8230999708175659|  0:01:44s\n",
      "epoch 66 | loss: 0.93456 | val_0_unsup_loss_numpy: 0.8163700103759766|  0:01:45s\n",
      "epoch 67 | loss: 0.9439  | val_0_unsup_loss_numpy: 0.8678100109100342|  0:01:47s\n",
      "epoch 68 | loss: 0.93858 | val_0_unsup_loss_numpy: 0.8542199730873108|  0:01:49s\n",
      "epoch 69 | loss: 0.93572 | val_0_unsup_loss_numpy: 0.8215600252151489|  0:01:50s\n",
      "epoch 70 | loss: 0.92469 | val_0_unsup_loss_numpy: 0.8466200232505798|  0:01:52s\n",
      "epoch 71 | loss: 0.94253 | val_0_unsup_loss_numpy: 0.825659990310669|  0:01:54s\n",
      "epoch 72 | loss: 0.93626 | val_0_unsup_loss_numpy: 0.7912899851799011|  0:01:55s\n",
      "epoch 73 | loss: 0.91903 | val_0_unsup_loss_numpy: 0.8102099895477295|  0:01:57s\n",
      "epoch 74 | loss: 0.92683 | val_0_unsup_loss_numpy: 0.8240500092506409|  0:01:59s\n",
      "epoch 75 | loss: 0.92591 | val_0_unsup_loss_numpy: 0.8643199801445007|  0:02:00s\n",
      "epoch 76 | loss: 0.95138 | val_0_unsup_loss_numpy: 0.8731499910354614|  0:02:02s\n",
      "epoch 77 | loss: 0.94109 | val_0_unsup_loss_numpy: 0.8157299757003784|  0:02:04s\n",
      "epoch 78 | loss: 0.92133 | val_0_unsup_loss_numpy: 0.8320900201797485|  0:02:05s\n",
      "epoch 79 | loss: 0.94743 | val_0_unsup_loss_numpy: 0.8314899802207947|  0:02:07s\n",
      "epoch 80 | loss: 0.93404 | val_0_unsup_loss_numpy: 0.8224700093269348|  0:02:09s\n",
      "epoch 81 | loss: 0.92291 | val_0_unsup_loss_numpy: 0.8184700012207031|  0:02:11s\n",
      "epoch 82 | loss: 0.92311 | val_0_unsup_loss_numpy: 0.8024200201034546|  0:02:12s\n",
      "\n",
      "Early stopping occurred at epoch 82 with best_epoch = 72 and best_val_0_unsup_loss_numpy = 0.7912899851799011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\miniconda3\\envs\\pytorch\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    }
   ],
   "source": [
    "tabnet_params = {\n",
    "    \"n_d\": 512,\n",
    "    \"n_a\": 512,\n",
    "    \"n_steps\": 3,\n",
    "    \"n_shared\": 2,\n",
    "    \"n_independent\": 2,\n",
    "    \"gamma\": 1.3,\n",
    "    \"epsilon\": 1e-15,\n",
    "    \"momentum\": 0.98,\n",
    "    \"mask_type\": \"sparsemax\",\n",
    "    \"lambda_sparse\": 1e-3,\n",
    "    \"device_name\": \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "}\n",
    "\n",
    "\n",
    "unsupervised_model = TabNetPretrainer(\n",
    "    optimizer_fn=torch.optim.Adam,\n",
    "    optimizer_params=dict(lr=2e-2),\n",
    "    **tabnet_params\n",
    ")\n",
    " \n",
    "\n",
    "unsupervised_model.fit(\n",
    "    X_train,\n",
    "    eval_set=[X_test],  \n",
    "    pretraining_ratio=0.8,\n",
    "    max_epochs=101,\n",
    "    patience=10,\n",
    "    batch_size=1024,\n",
    "    virtual_batch_size=128,\n",
    "    num_workers=0,\n",
    "    drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(y_true, y_pred, class_weights=None, smooth=1e-6):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        y_true: shape [batch_size, num_classes] (one-hot)\n",
    "        y_pred: shape [batch_size, num_classes] (logits)\n",
    "        class_weights: tensor [num_classes]\n",
    "    \"\"\"\n",
    "    y_true = y_true.float()\n",
    "    y_pred = torch.softmax(y_pred, dim=1)\n",
    "\n",
    "    intersection = (y_pred * y_true).sum(dim=0)\n",
    "    union = (y_pred + y_true).sum(dim=0)\n",
    "\n",
    "    dice = (2. * intersection + smooth) / (union + smooth)\n",
    "\n",
    "    if class_weights is not None:\n",
    "        dice = dice * class_weights\n",
    "\n",
    "    return 1 - dice.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==============================================================================================================\n",
       "Layer (type:depth-idx)                                       Output Shape              Param #\n",
       "==============================================================================================================\n",
       "TabNetPretraining                                            [19430, 33]               --\n",
       "├─EmbeddingGenerator: 1-1                                    [19430, 33]               --\n",
       "├─TabNetEncoder: 1-2                                         [19430, 512]              --\n",
       "│    └─BatchNorm1d: 2-1                                      [19430, 33]               66\n",
       "│    └─FeatTransformer: 2-2                                  [19430, 1024]             4,202,496\n",
       "│    │    └─GLU_Block: 3-1                                   [19430, 1024]             2,172,928\n",
       "│    └─ModuleList: 2-12                                      --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-17                            --                        (recursive)\n",
       "│    └─FeatTransformer: 2-6                                  --                        (recursive)\n",
       "│    │    └─GLU_Block: 3-5                                   --                        (recursive)\n",
       "│    └─ModuleList: 2-12                                      --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-17                            --                        (recursive)\n",
       "│    └─FeatTransformer: 2-6                                  --                        (recursive)\n",
       "│    │    └─GLU_Block: 3-5                                   --                        (recursive)\n",
       "│    │    └─GLU_Block: 3-6                                   [19430, 1024]             4,202,496\n",
       "│    └─ModuleList: 2-11                                      --                        (recursive)\n",
       "│    │    └─AttentiveTransformer: 3-7                        [19430, 33]               16,962\n",
       "│    └─ModuleList: 2-12                                      --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-8                             [19430, 1024]             6,375,424\n",
       "│    │    └─FeatTransformer: 3-17                            --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-12                            --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-17                            --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-12                            --                        (recursive)\n",
       "│    └─ModuleList: 2-11                                      --                        (recursive)\n",
       "│    │    └─AttentiveTransformer: 3-13                       [19430, 33]               16,962\n",
       "│    └─ModuleList: 2-12                                      --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-14                            [19430, 1024]             6,375,424\n",
       "│    │    └─FeatTransformer: 3-17                            --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-18                            --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-17                            --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-18                            --                        (recursive)\n",
       "│    └─ModuleList: 2-11                                      --                        (recursive)\n",
       "│    │    └─AttentiveTransformer: 3-19                       [19430, 33]               16,962\n",
       "│    └─ModuleList: 2-12                                      --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-20                            [19430, 1024]             6,375,424\n",
       "├─TabNetDecoder: 1-3                                         [19430, 33]               --\n",
       "│    └─ModuleList: 2-13                                      --                        --\n",
       "│    │    └─FeatTransformer: 3-21                            [19430, 512]              1,052,672\n",
       "│    │    └─FeatTransformer: 3-25                            --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-23                            --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-24                            [19430, 512]              1,052,672\n",
       "│    │    └─FeatTransformer: 3-25                            --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-26                            --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-27                            [19430, 512]              1,052,672\n",
       "│    └─Linear: 2-14                                          [19430, 33]               16,896\n",
       "==============================================================================================================\n",
       "Total params: 51,409,160\n",
       "Trainable params: 51,409,160\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 558.18\n",
       "==============================================================================================================\n",
       "Input size (MB): 2.56\n",
       "Forward/backward pass size (MB): 12138.00\n",
       "Params size (MB): 84.74\n",
       "Estimated Total Size (MB): 12225.30\n",
       "=============================================================================================================="
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Truy cập vào mô hình TabNet bên trong\n",
    "from torchinfo import summary\n",
    "\n",
    "tabnet_model = unsupervised_model.network.to(device)\n",
    "\n",
    "summary(tabnet_model, input_size=X_train.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Encoder Summary:\n",
      "TabNetEncoder(\n",
      "  (initial_bn): BatchNorm1d(33, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  (initial_splitter): FeatTransformer(\n",
      "    (shared): GLU_Block(\n",
      "      (shared_layers): ModuleList(\n",
      "        (0): Linear(in_features=33, out_features=2048, bias=False)\n",
      "        (1): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "      )\n",
      "      (glu_layers): ModuleList(\n",
      "        (0): GLU_Layer(\n",
      "          (fc): Linear(in_features=33, out_features=2048, bias=False)\n",
      "          (bn): GBN(\n",
      "            (bn): BatchNorm1d(2048, eps=1e-05, momentum=0.98, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): GLU_Layer(\n",
      "          (fc): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "          (bn): GBN(\n",
      "            (bn): BatchNorm1d(2048, eps=1e-05, momentum=0.98, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (specifics): GLU_Block(\n",
      "      (glu_layers): ModuleList(\n",
      "        (0-1): 2 x GLU_Layer(\n",
      "          (fc): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "          (bn): GBN(\n",
      "            (bn): BatchNorm1d(2048, eps=1e-05, momentum=0.98, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (feat_transformers): ModuleList(\n",
      "    (0-2): 3 x FeatTransformer(\n",
      "      (shared): GLU_Block(\n",
      "        (shared_layers): ModuleList(\n",
      "          (0): Linear(in_features=33, out_features=2048, bias=False)\n",
      "          (1): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "        )\n",
      "        (glu_layers): ModuleList(\n",
      "          (0): GLU_Layer(\n",
      "            (fc): Linear(in_features=33, out_features=2048, bias=False)\n",
      "            (bn): GBN(\n",
      "              (bn): BatchNorm1d(2048, eps=1e-05, momentum=0.98, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (1): GLU_Layer(\n",
      "            (fc): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "            (bn): GBN(\n",
      "              (bn): BatchNorm1d(2048, eps=1e-05, momentum=0.98, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (specifics): GLU_Block(\n",
      "        (glu_layers): ModuleList(\n",
      "          (0-1): 2 x GLU_Layer(\n",
      "            (fc): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "            (bn): GBN(\n",
      "              (bn): BatchNorm1d(2048, eps=1e-05, momentum=0.98, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (att_transformers): ModuleList(\n",
      "    (0-2): 3 x AttentiveTransformer(\n",
      "      (fc): Linear(in_features=512, out_features=33, bias=False)\n",
      "      (bn): GBN(\n",
      "        (bn): BatchNorm1d(33, eps=1e-05, momentum=0.98, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (selector): Sparsemax()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "encoder = tabnet_model.encoder\n",
    "\n",
    "print(\"\\nEncoder Summary:\")\n",
    "print(encoder)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decoder Summary:\n",
      "TabNetDecoder(\n",
      "  (feat_transformers): ModuleList(\n",
      "    (0-2): 3 x FeatTransformer(\n",
      "      (shared): GLU_Block(\n",
      "        (shared_layers): ModuleList(\n",
      "          (0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        )\n",
      "        (glu_layers): ModuleList(\n",
      "          (0): GLU_Layer(\n",
      "            (fc): Linear(in_features=512, out_features=1024, bias=False)\n",
      "            (bn): GBN(\n",
      "              (bn): BatchNorm1d(1024, eps=1e-05, momentum=0.98, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (specifics): GLU_Block(\n",
      "        (glu_layers): ModuleList(\n",
      "          (0): GLU_Layer(\n",
      "            (fc): Linear(in_features=512, out_features=1024, bias=False)\n",
      "            (bn): GBN(\n",
      "              (bn): BatchNorm1d(1024, eps=1e-05, momentum=0.98, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (reconstruction_layer): Linear(in_features=512, out_features=33, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "decoder = tabnet_model.decoder\n",
    "\n",
    "print(\"\\nDecoder Summary:\")\n",
    "print(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TabNet encoder trả về 2 giá trị.\n",
      "Đã xảy ra lỗi: 'list' object has no attribute 'shape'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_30408\\3356785120.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sample_input = torch.tensor(X_train[:5]).to(device)\n"
     ]
    }
   ],
   "source": [
    "sample_input = torch.tensor(X_train[:5]).to(device)  \n",
    "\n",
    "try:\n",
    "    result = tabnet_model.encoder(sample_input)\n",
    "    if isinstance(result, tuple):\n",
    "        print(f'TabNet encoder trả về {len(result)} giá trị.')\n",
    "        for i, res in enumerate(result):\n",
    "            print(f'Giá trị {i + 1} shape: {res.shape}')\n",
    "    else:\n",
    "        print('TabNet encoder chỉ trả về một giá trị.')\n",
    "        print(f'Giá trị shape: {result.shape}')\n",
    "except Exception as e:\n",
    "    print(f'Đã xảy ra lỗi: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(nn.Module):\n",
    "    def __init__(self, seed=1337):\n",
    "        super(Sampling, self).__init__()\n",
    "        self.seed = seed\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = z_mean.size(0)\n",
    "        dim = z_mean.size(1)\n",
    "        # print(batch, dim)\n",
    "        epsilon = torch.randn(batch, dim, generator=torch.Generator().manual_seed(self.seed)).to(device)\n",
    "        return z_mean + torch.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class VAE_Encoder(nn.Module):\n",
    "#     def __init__(self, latent_dim):\n",
    "#         super(VAE_Encoder, self).__init__()\n",
    "#         self.tabnet_encoder = tabnet_model.encoder\n",
    "#         self.mlp = nn.Sequential(\n",
    "#             nn.Linear(512, 256),  \n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(256, 128),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(128, 96),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(96, 96),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(96, 32),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(32, latent_dim)\n",
    "#         ).to(device)\n",
    "#         self.fc_mean = nn.Linear(latent_dim, latent_dim).to(device)\n",
    "#         self.fc_log_var = nn.Linear(latent_dim, latent_dim).to(device)\n",
    "#         self.sampling = Sampling().to(device)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = x.to(device)\n",
    "#         steps_output, _ = self.tabnet_encoder(x)\n",
    "#         encoded = steps_output[-1]\n",
    "#         # print(\"Shape of encoded tensor:\", encoded.shape)\n",
    "#         encoded = self.mlp(encoded)\n",
    "#         z_mean = self.fc_mean(encoded)\n",
    "#         z_log_var = self.fc_log_var(encoded)\n",
    "#         z = self.sampling((z_mean, z_log_var))\n",
    "#         # print(f'Shape of z: {z.shape} - {z_log_var.shape} -{z_log_var.shape}')\n",
    "#         return z_mean, z_log_var, z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE_Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(VAE_Encoder, self).__init__()\n",
    "        self.tabnet_encoder = tabnet_model.encoder\n",
    "\n",
    "        self.mlp_0 = nn.Sequential(\n",
    "            nn.Linear(512, 256),  \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 96),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(96, 96),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(96, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, latent_dim)\n",
    "        ).to(device)\n",
    "\n",
    "        self.mlp_1 = nn.Sequential(\n",
    "            nn.Linear(512, 256),  \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 96),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(96, 96),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(96, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, latent_dim)\n",
    "        ).to(device)\n",
    "\n",
    "        self.fc_mean_0 = nn.Linear(latent_dim, latent_dim).to(device)\n",
    "        self.fc_log_var_0 = nn.Linear(latent_dim, latent_dim).to(device)\n",
    "\n",
    "        self.fc_mean_1 = nn.Linear(latent_dim, latent_dim).to(device)\n",
    "        self.fc_log_var_1 = nn.Linear(latent_dim, latent_dim).to(device)\n",
    "\n",
    "        self.sampling = Sampling().to(device)\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        x = x.to(device)\n",
    "\n",
    "        # Chuyển labels thành class index nếu là one-hot\n",
    "        if labels.dim() == 2 and labels.size(1) > 1:\n",
    "            labels = torch.argmax(labels, dim=1)  # CHUYỂN TỪ ONE-HOT SANG LONG\n",
    "        labels = labels.squeeze().long().to(device)\n",
    "\n",
    "        steps_output, _ = self.tabnet_encoder(x)\n",
    "        encoded = steps_output[-1]  # Shape: [batch_size, 512]\n",
    "\n",
    "        batch_size = x.size(0)\n",
    "        latent_dim = self.fc_mean_0.out_features\n",
    "\n",
    "        z_mean = torch.zeros(batch_size, latent_dim).to(device)\n",
    "        z_log_var = torch.zeros(batch_size, latent_dim).to(device)\n",
    "        z = torch.zeros(batch_size, latent_dim).to(device)\n",
    "\n",
    "        idx_0 = (labels == 0)\n",
    "        idx_1 = (labels == 1)\n",
    "\n",
    "        # Xử lý lớp 0\n",
    "        if idx_0.any():\n",
    "            encoded_0 = encoded[idx_0]\n",
    "            out_0 = self.mlp_0(encoded_0)\n",
    "            mu_0 = self.fc_mean_0(out_0)\n",
    "            logvar_0 = self.fc_log_var_0(out_0)\n",
    "            z_0 = self.sampling((mu_0, logvar_0))\n",
    "\n",
    "            z_mean[idx_0] = mu_0\n",
    "            z_log_var[idx_0] = logvar_0\n",
    "            z[idx_0] = z_0\n",
    "\n",
    "        # Xử lý lớp 1\n",
    "        if idx_1.any():\n",
    "            encoded_1 = encoded[idx_1]\n",
    "            out_1 = self.mlp_1(encoded_1)\n",
    "            mu_1 = self.fc_mean_1(out_1)\n",
    "            logvar_1 = self.fc_log_var_1(out_1)\n",
    "            z_1 = self.sampling((mu_1, logvar_1))\n",
    "\n",
    "            z_mean[idx_1] = mu_1\n",
    "            z_log_var[idx_1] = logvar_1\n",
    "            z[idx_1] = z_1\n",
    "\n",
    "        return z_mean, z_log_var, z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE_Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim,encoded_dim, output_dim):\n",
    "        super(VAE_Decoder, self).__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 32),   \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 96),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(96, 96),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(96, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, encoded_dim),  \n",
    "        )\n",
    "        self.tabnet_decoder = tabnet_model.decoder\n",
    "        self.reshape = nn.Unflatten(1, (encoded_dim,))\n",
    "        self.output_dim=output_dim\n",
    "\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = F.relu(self.mlp(z))\n",
    "\n",
    "        # print(\"Shape before reshape:\", x.shape)\n",
    "        x = self.reshape(x)\n",
    "        x = x[None, ...]\n",
    "\n",
    "        # print(\"Shape after reshape:\", x.shape)\n",
    "        # x = x.view(x.size(0), output_dim)\n",
    "        \n",
    "        output = self.tabnet_decoder(x)\n",
    "        # print(output.shape)\n",
    "        # print(\"Shape of output from tabnet_decoder:\", output.shape)\n",
    "        output = torch.softmax(output, dim=-1)  # Assuming the output is a probability distribution\n",
    "        output = output.view(-1, self.output_dim)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data_range(tensor, name):\n",
    "    if not torch.all((tensor >= 0) & (tensor <= 1)):\n",
    "        print(f\"{name} contains values outside the range [0, 1]\")\n",
    "        print(f\"{name} min: {tensor.min()}, max: {tensor.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE_Tabnet_MLPS(nn.Module):\n",
    "    def __init__(self, encoder, decoder, classifier):\n",
    "        super(VAE_Tabnet_MLPS, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.classifier = classifier\n",
    "        self.total_loss_tracker = []\n",
    "        self.reconstruction_loss_tracker = []\n",
    "        self.kl_loss_tracker = []\n",
    "        self.classification_loss_tracker = []\n",
    "        self.accuracy_tracker = []\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        z_mean, z_log_var, z = self.encoder(x, labels)\n",
    "        reconstruction = self.decoder(z)\n",
    "        classification_output = self.classifier(z)\n",
    "        return reconstruction, z_mean, z_log_var, classification_output\n",
    "\n",
    "    def train_step(self, data, labels, optimizer):\n",
    "        labels = labels.squeeze(1) \n",
    "        labels = torch.argmax(labels, dim=1).long().to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        reconstruction, z_mean, z_log_var, classification_output = self.forward(data, labels)\n",
    "\n",
    "        # Reconstruction Loss\n",
    "        reconstruction_loss = torch.mean(\n",
    "            torch.sum(F.binary_cross_entropy_with_logits(reconstruction, data, reduction='none'), dim=1)\n",
    "        )\n",
    "\n",
    "        # Classification Loss với class weights\n",
    "        ce_loss = F.cross_entropy(classification_output, labels, weight=class_weights_tensor)\n",
    "\n",
    "        # Dice Loss\n",
    "        y_true = F.one_hot(labels, num_classes=classification_output.shape[1]).float().to(device)\n",
    "        d_loss = dice_loss(y_true, classification_output, class_weights=class_weights_tensor)\n",
    "\n",
    "        # Kết hợp CE và Dice Loss\n",
    "        alpha = 0.5  # Tỷ lệ giữa cross entropy và dice loss\n",
    "        classification_loss = alpha * ce_loss + (1 - alpha) * d_loss\n",
    "\n",
    "        # KL Divergence Loss\n",
    "        kl_loss = -0.5 * torch.mean(torch.sum(\n",
    "            1 + z_log_var - z_mean.pow(2) - z_log_var.exp(), dim=1\n",
    "        ))\n",
    "\n",
    "        # Tổng loss\n",
    "        total_loss = reconstruction_loss + kl_loss + classification_loss\n",
    "\n",
    "        # Backward & optimize\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Lưu lại metrics\n",
    "        self.total_loss_tracker.append(total_loss.item())\n",
    "        self.reconstruction_loss_tracker.append(reconstruction_loss.item())\n",
    "        self.kl_loss_tracker.append(kl_loss.item())\n",
    "        self.classification_loss_tracker.append(classification_loss.item())\n",
    "        # self.accuracy_tracker.append(accuracy.item())\n",
    "\n",
    "        # Tính accuracy\n",
    "        preds = torch.softmax(classification_output, dim=1)\n",
    "        pred_labels = torch.argmax(preds, dim=1)\n",
    "        correct = (pred_labels == labels).float().sum()\n",
    "        accuracy = correct / labels.size(0)\n",
    "        self.accuracy_tracker.append(accuracy.item())\n",
    "\n",
    "        return {\n",
    "            \"loss\": total_loss.item(),\n",
    "            \"reconstruction_loss\": reconstruction_loss.item(),\n",
    "            \"kl_loss\": kl_loss.item(),\n",
    "            \"classification_loss\": classification_loss.item(),\n",
    "            \"accuracy\": accuracy.item()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 64\n",
    "encoded_dim = 512\n",
    "output_dim = X_train.shape[1]\n",
    "input_dim = X_train.shape[1]\n",
    "print(input_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(SimpleClassifier, self).__init__()\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "\n",
    "            nn.Linear(64, 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "\n",
    "            nn.Linear(32, output_dim),\n",
    "            # nn.Softmax()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print('input: ',x.shape)\n",
    "        output = self.fc_layers(x)\n",
    "        # output = output.view(-1)\n",
    "        # print('output',output.shape)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SimpleClassifier(latent_dim, output_dim=2).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input size: torch.Size([32, 64])\n",
      "Output size: torch.Size([32, 1])\n",
      "Output: tensor([[-0.2849],\n",
      "        [ 0.4297],\n",
      "        [ 0.2478],\n",
      "        [ 0.3464],\n",
      "        [ 0.1749],\n",
      "        [ 0.4117],\n",
      "        [-0.7240],\n",
      "        [-0.3241],\n",
      "        [-0.0748],\n",
      "        [-0.2098],\n",
      "        [-0.1626],\n",
      "        [-0.0530],\n",
      "        [ 0.1158],\n",
      "        [ 0.0951],\n",
      "        [ 0.1079],\n",
      "        [-0.5118],\n",
      "        [-0.2990],\n",
      "        [-0.0234],\n",
      "        [ 0.4438],\n",
      "        [ 0.3664],\n",
      "        [-0.2696],\n",
      "        [-0.0315],\n",
      "        [-0.4494],\n",
      "        [ 0.0214],\n",
      "        [-0.3248],\n",
      "        [ 0.1291],\n",
      "        [-0.1351],\n",
      "        [-0.2795],\n",
      "        [ 0.1824],\n",
      "        [ 0.0813],\n",
      "        [ 0.0633],\n",
      "        [-0.6573]])\n"
     ]
    }
   ],
   "source": [
    "def check_output(model, input_tensor):\n",
    "    with torch.no_grad():  \n",
    "        output = model(input_tensor)\n",
    "        print(f\"Input size: {input_tensor.size()}\")\n",
    "        print(f\"Output size: {output.size()}\")\n",
    "        print(f\"Output: {output}\")\n",
    "\n",
    "model = SimpleClassifier(latent_dim, output_dim=1)\n",
    "\n",
    "input_tensor = torch.randn(32,latent_dim)  \n",
    "\n",
    "check_output(model, input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_encoder = VAE_Encoder(latent_dim=latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vae_encoder = VAE_Encoder(latent_dim=latent_dim)\n",
    "# print(\"Encoder Summary:\")\n",
    "# # vae_encoder.to(device)\n",
    "\n",
    "# summary(vae_encoder, input_size=(32, input_dim), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded shape: torch.Size([800, 512])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(800, X_train.shape[1]).to(device)\n",
    "steps_output, _ = tabnet_model.encoder(x)\n",
    "encoded = steps_output[-1]\n",
    "print(f\"Encoded shape: {encoded.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of encoder output: [torch.Size([800, 512]), torch.Size([800, 512]), torch.Size([800, 512])]\n",
      "Decoder shape: torch.Size([800, 33])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.randn(800, X_train.shape[1]).to(device)  # Đầu vào có kích thước (batch_size, features)\n",
    "\n",
    "steps_output, _ = tabnet_model.encoder(x)\n",
    "print(\"Shape of encoder output:\", [output.shape for output in steps_output])\n",
    "\n",
    "decoder_input = steps_output[-1]  \n",
    "decoder_input = decoder_input[None, ...]\n",
    "try:\n",
    "    decoder_output = tabnet_model.decoder(decoder_input)\n",
    "    print(f\"Decoder shape: {decoder_output.shape}\")\n",
    "except ValueError as e:\n",
    "    print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 33)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dim, output_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder Summary:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==============================================================================================================\n",
       "Layer (type:depth-idx)                                       Output Shape              Param #\n",
       "==============================================================================================================\n",
       "VAE_Decoder                                                  [32, 33]                  --\n",
       "├─Sequential: 1-1                                            [32, 512]                 --\n",
       "│    └─Linear: 2-1                                           [32, 32]                  2,080\n",
       "│    └─ReLU: 2-2                                             [32, 32]                  --\n",
       "│    └─Linear: 2-3                                           [32, 96]                  3,168\n",
       "│    └─ReLU: 2-4                                             [32, 96]                  --\n",
       "│    └─Linear: 2-5                                           [32, 96]                  9,312\n",
       "│    └─ReLU: 2-6                                             [32, 96]                  --\n",
       "│    └─Linear: 2-7                                           [32, 128]                 12,416\n",
       "│    └─ReLU: 2-8                                             [32, 128]                 --\n",
       "│    └─Linear: 2-9                                           [32, 256]                 33,024\n",
       "│    └─ReLU: 2-10                                            [32, 256]                 --\n",
       "│    └─Linear: 2-11                                          [32, 512]                 131,584\n",
       "├─Unflatten: 1-2                                             [32, 512]                 --\n",
       "├─TabNetDecoder: 1-3                                         [32, 33]                  --\n",
       "│    └─ModuleList: 2-12                                      --                        --\n",
       "│    │    └─FeatTransformer: 3-1                             [32, 512]                 1,052,672\n",
       "│    │    └─FeatTransformer: 3-2                             --                        1,052,672\n",
       "│    │    └─FeatTransformer: 3-3                             --                        (recursive)\n",
       "│    └─Linear: 2-13                                          [32, 33]                  16,896\n",
       "==============================================================================================================\n",
       "Total params: 2,842,208\n",
       "Trainable params: 2,842,208\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 40.36\n",
       "==============================================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 1.34\n",
       "Params size (MB): 5.04\n",
       "Estimated Total Size (MB): 6.40\n",
       "=============================================================================================================="
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae_decoder = VAE_Decoder(latent_dim=latent_dim, encoded_dim=encoded_dim, output_dim=output_dim).to(device)\n",
    "print(\"Decoder Summary:\")\n",
    "summary(vae_decoder, input_size=(32, latent_dim), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE_Tabnet_MLPS(encoder=vae_encoder, decoder=vae_decoder,classifier=classifier).to(device)\n",
    "# summary(vae, input_size=(32, input_dim), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning_rate = 0.0001\n",
    "# optimizer = optim.Adam(vae.parameters(), lr=learning_rate)\n",
    "# num_epochs = 10\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     vae.train()\n",
    "#     train_loss = 0\n",
    "#     rec_loss = 0\n",
    "#     kl_loss = 0\n",
    "#     classification_loss = 0\n",
    "#     accuracy = 0\n",
    "\n",
    "#     for batch_data, batch_labels in train_loader:\n",
    "#         batch_data = batch_data.to(device)\n",
    "#         batch_labels = batch_labels.to(device)\n",
    "#         # print(f\"Batch data shape: {batch_data.shape}, Batch labels shape: {batch_labels.shape}\")\n",
    "#         results = vae.train_step(batch_data, batch_labels, optimizer)\n",
    "        \n",
    "#         train_loss += results[\"loss\"]\n",
    "#         rec_loss += results[\"reconstruction_loss\"]\n",
    "#         kl_loss += results[\"kl_loss\"]\n",
    "#         classification_loss += results[\"classification_loss\"]\n",
    "#         accuracy += results[\"accuracy\"]\n",
    "\n",
    "#     train_loss /= len(train_loader)\n",
    "#     rec_loss /= len(train_loader)\n",
    "#     kl_loss /= len(train_loader)\n",
    "#     classification_loss /= len(train_loader)\n",
    "#     accuracy /= len(train_loader)\n",
    "\n",
    "#     print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {train_loss:.4f}, Reconstruction Loss: {rec_loss:.4f}, KL Loss: {kl_loss:.4f}, Classification Loss: {classification_loss:.4f}, Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in vae.encoder.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "The shape of the mask [64, 2] at index 1 does not match the shape of the indexed tensor [64, 512] at index 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 35\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Lấy classification_logits từ model\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 35\u001b[0m     _, _, _, classification_logits \u001b[38;5;241m=\u001b[39m \u001b[43mvae_new\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Tạo lại one-hot label để tính Dice Loss\u001b[39;00m\n\u001b[0;32m     38\u001b[0m num_classes \u001b[38;5;241m=\u001b[39m classification_logits\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Admin\\miniconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\miniconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[39], line 14\u001b[0m, in \u001b[0;36mVAE_Tabnet_MLPS.forward\u001b[1;34m(self, x, labels)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, labels):\n\u001b[1;32m---> 14\u001b[0m     z_mean, z_log_var, z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m     reconstruction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(z)\n\u001b[0;32m     16\u001b[0m     classification_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(z)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\miniconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\miniconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[36], line 65\u001b[0m, in \u001b[0;36mVAE_Encoder.forward\u001b[1;34m(self, x, labels)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# Xử lý lớp 0\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m idx_0\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m---> 65\u001b[0m     encoded_0 \u001b[38;5;241m=\u001b[39m \u001b[43mencoded\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx_0\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     66\u001b[0m     out_0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp_0(encoded_0)\n\u001b[0;32m     67\u001b[0m     mu_0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_mean_0(out_0)\n",
      "\u001b[1;31mIndexError\u001b[0m: The shape of the mask [64, 2] at index 1 does not match the shape of the indexed tensor [64, 512] at index 1"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.0001\n",
    "optimizer = optim.Adam(vae.parameters(), lr=learning_rate)\n",
    "\n",
    "num_epochs = 50\n",
    "vae_new = VAE_Tabnet_MLPS(vae.encoder, vae.decoder, vae.classifier).to(device)\n",
    "\n",
    "for param in vae_new.encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, vae_new.parameters()), lr=learning_rate)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    vae_new.train()\n",
    "    train_loss = 0\n",
    "    rec_loss = 0\n",
    "    kl_loss_val = 0\n",
    "    classification_loss = 0\n",
    "    dice_loss_val = 0\n",
    "    accuracy = 0\n",
    "\n",
    "    for batch_data, batch_labels in train_loader:\n",
    "        batch_data = batch_data.to(device)\n",
    "\n",
    "        if batch_labels.dim() == 2 and batch_labels.size(1) == 2:\n",
    "            batch_labels = torch.argmax(batch_labels, dim=1)\n",
    "        batch_labels = batch_labels.long().to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        results = vae_new.train_step(batch_data, batch_labels, optimizer)\n",
    "\n",
    "        # Lấy classification_logits từ model\n",
    "        with torch.no_grad():\n",
    "            _, _, _, classification_logits = vae_new(batch_data, batch_labels)\n",
    "\n",
    "        # Tạo lại one-hot label để tính Dice Loss\n",
    "        num_classes = classification_logits.shape[1]\n",
    "        y_true = F.one_hot(batch_labels, num_classes=num_classes).float()\n",
    "\n",
    "        # Tính Dice Loss\n",
    "        current_dice_loss = dice_loss(y_true, classification_logits)\n",
    "\n",
    "        # Tổng loss = KL + Recon + Classification + Dice\n",
    "        total_loss = results[\"total_loss\"] + current_dice_loss  # Thêm Dice Loss\n",
    "\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Cập nhật metrics\n",
    "        train_loss += total_loss.item()\n",
    "        rec_loss += results[\"recon_loss\"]\n",
    "        kl_loss_val += results[\"kl_loss\"]\n",
    "        classification_loss += results[\"cls_loss\"]\n",
    "        dice_loss_val += current_dice_loss.item()\n",
    "        accuracy += results[\"accuracy\"]\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    rec_loss /= len(train_loader)\n",
    "    kl_loss_val /= len(train_loader)\n",
    "    classification_loss /= len(train_loader)\n",
    "    dice_loss_val /= len(train_loader)\n",
    "    accuracy /= len(train_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, \"\n",
    "          f\"Total Loss: {train_loss:.4f}, \"\n",
    "          f\"Reconstruction Loss: {rec_loss:.4f}, \"\n",
    "          f\"KL Loss: {kl_loss_val:.4f}, \"\n",
    "          f\"Classification Loss: {classification_loss:.4f}, \"\n",
    "          f\"Dice Loss: {dice_loss_val:.4f}, \"\n",
    "          f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleDiffusionModel(nn.Module):\n",
    "    def __init__(self, latent_dim, time_steps=1000):\n",
    "        super().__init__()\n",
    "        self.time_steps = time_steps\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Tạo các beta_schedule tuyến tính\n",
    "        beta = torch.linspace(0.0001, 0.02, time_steps)\n",
    "        alpha = 1. - beta\n",
    "        alpha_bar = torch.cumprod(alpha, dim=0)\n",
    "\n",
    "        self.register_buffer('beta', beta)\n",
    "        self.register_buffer('alpha', alpha)\n",
    "        self.register_buffer('alpha_bar', alpha_bar)\n",
    "\n",
    "        # Mạng neural đơn giản để dự đoán nhiễu\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim + 1, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, latent_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, z, t):\n",
    "        noise = torch.randn_like(z)\n",
    "        \n",
    "        # Đảm bảo t là long type và shape phù hợp\n",
    "        if isinstance(t, torch.Tensor):\n",
    "            t = t.to(dtype=torch.long)\n",
    "        else:\n",
    "            t = torch.tensor([t], device=z.device, dtype=torch.long).expand(z.shape[0])\n",
    "\n",
    "        sqrt_alpha_bar = torch.sqrt(self.alpha_bar[t])[:, None]\n",
    "        sqrt_one_minus_alpha_bar = torch.sqrt(1 - self.alpha_bar[t])[:, None]\n",
    "        noisy_z = sqrt_alpha_bar * z + sqrt_one_minus_alpha_bar * noise\n",
    "\n",
    "        predicted_noise = self.model(torch.cat([noisy_z, t.unsqueeze(1)], dim=1))\n",
    "        loss = F.mse_loss(predicted_noise, noise)\n",
    "        return loss\n",
    "\n",
    "    def sample(self, z):\n",
    "        for i in reversed(range(self.time_steps)):\n",
    "            z = self.denoise_step(z, i)\n",
    "        return z\n",
    "    \n",
    "    def denoise_step(self, z, t):\n",
    "        timestep = t.item() if isinstance(t, torch.Tensor) else t\n",
    "        t_batch = torch.full((z.shape[0],), timestep, device=z.device, dtype=torch.long)\n",
    "\n",
    "        predicted_noise = self.model(torch.cat([z, t_batch.unsqueeze(1)], dim=1))\n",
    "\n",
    "        alpha = self.alpha[timestep]\n",
    "        alpha_bar = self.alpha_bar[timestep]\n",
    "        beta = self.beta[timestep]\n",
    "\n",
    "        z = (1 / torch.sqrt(alpha)) * (z - ((1 - alpha) / torch.sqrt(1 - alpha_bar)) * predicted_noise)\n",
    "        if timestep > 0:\n",
    "            noise = torch.randn_like(z)\n",
    "            z += torch.sqrt(beta) * noise\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    vae_new.encoder.eval()\n",
    "    dummy_input = torch.randn(1, input_dim).to(device)  # Thay input_dim theo đúng dữ liệu của bạn\n",
    "    dummy_labels = torch.zeros(1, dtype=torch.long).to(device)\n",
    "    z_mean, z_log_var, _ = vae_new.encoder(dummy_input, dummy_labels)\n",
    "    latent_dim = z_mean.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# latent_dim = vae.encoder[-1].out_features  # Kích thước latent z\n",
    "diffusion_model = SimpleDiffusionModel(latent_dim=latent_dim).to(device)\n",
    "diffusion_optimizer = torch.optim.Adam(diffusion_model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullPipelineModel(nn.Module):\n",
    "    def __init__(self, vae, diffusion, classifier):\n",
    "        super(FullPipelineModel, self).__init__()\n",
    "        self.vae = vae\n",
    "        self.diffusion = diffusion\n",
    "        self.classifier = classifier\n",
    "\n",
    "        self.total_loss = []\n",
    "        self.recon_loss = []\n",
    "        self.kl_loss = []\n",
    "        self.cls_loss = []\n",
    "        self.accuracy = []\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        z_mean, z_log_var, z = self.vae.encoder(x, labels)\n",
    "        reconstruction = self.vae.decoder(z)\n",
    "        z_diffused = self.diffusion.sample(z)  \n",
    "\n",
    "        classification_logits = self.classifier(z_diffused).float()\n",
    "        return reconstruction, z_mean, z_log_var, classification_logits\n",
    "\n",
    "    def train_step(self, x, labels, optimizer):\n",
    "        # Xử lý labels\n",
    "        if labels.dim() == 2 and labels.size(1) == 2:\n",
    "            labels = torch.argmax(labels, dim=1)\n",
    "        labels = labels.long().to(device)  # Đảm bảo kiểu đúng\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        reconstruction, z_mean, z_log_var, classification_logits = self.forward(x, labels)\n",
    "\n",
    "        # Reconstruction Loss\n",
    "        recon_loss = F.binary_cross_entropy_with_logits(reconstruction, x, reduction='mean')\n",
    "\n",
    "        # KL Divergence Loss\n",
    "        kl_loss = -0.5 * torch.mean(\n",
    "            torch.sum(1 + z_log_var - z_mean.pow(2) - z_log_var.exp(), dim=1)\n",
    "        )\n",
    "\n",
    "        # Classification Loss với class weights\n",
    "        ce_loss = F.cross_entropy(classification_logits, labels, weight=class_weights_tensor)\n",
    "\n",
    "        # Dice Loss\n",
    "        y_true = F.one_hot(labels, num_classes=classification_logits.shape[1]).float().to(device)\n",
    "        d_loss = dice_loss(y_true, classification_logits, class_weights=class_weights_tensor)\n",
    "\n",
    "        # Kết hợp CrossEntropy + Dice Loss\n",
    "        alpha = 0.5  # Điều chỉnh theo kết quả thực nghiệm\n",
    "        cls_loss = alpha * ce_loss + (1 - alpha) * d_loss\n",
    "\n",
    "        # Tổng loss\n",
    "        total_loss = recon_loss + kl_loss + cls_loss\n",
    "\n",
    "        # Backward\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Lưu lại metrics\n",
    "        self.total_loss.append(total_loss.item())\n",
    "        self.recon_loss.append(recon_loss.item())\n",
    "        self.kl_loss.append(kl_loss.item())\n",
    "        self.cls_loss.append(cls_loss.item())\n",
    "\n",
    "        # Tính accuracy\n",
    "        preds = torch.argmax(classification_logits, dim=1)\n",
    "        acc = (preds == labels).float().mean()\n",
    "        self.accuracy.append(acc.item())\n",
    "\n",
    "        return {\n",
    "            \"total_loss\": total_loss.item(),\n",
    "            \"recon_loss\": recon_loss.item(),\n",
    "            \"kl_loss\": kl_loss.item(),\n",
    "            \"cls_loss\": cls_loss.item(),\n",
    "            \"accuracy\": acc.item()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def train_diffusion(vae, diffusion_model, dataloader, optimizer, device, time_steps=1000, epochs=150):\n",
    "    diffusion_model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch_data, batch_labels in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            batch_data = batch_data.to(device)\n",
    "            batch_labels = batch_labels.squeeze().to(device)\n",
    "            if batch_labels.dim() > 1:  # nếu là one-hot hoặc có chiều phụ\n",
    "                batch_labels = batch_labels.argmax(dim=1)\n",
    "            with torch.no_grad():\n",
    "                z_mean, z_log_var, z = vae.encoder(batch_data, batch_labels)\n",
    "\n",
    "            t = torch.randint(0, time_steps, (z.shape[0],), device=device).long()\n",
    "            loss = diffusion_model(z, t)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Diffusion Loss: {total_loss/len(dataloader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, precision_score, recall_score\n",
    "\n",
    "def evaluate_diffusion_with_classifier(vae, diffusion_model, classifier, test_loader, device, time_steps=1000):\n",
    "    vae.eval()\n",
    "    diffusion_model.eval()\n",
    "    classifier.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_data, batch_labels in test_loader:\n",
    "            batch_data = batch_data.to(device)\n",
    "            batch_labels = batch_labels.squeeze().to(device)\n",
    "            if batch_labels.ndim == 3 and batch_labels.shape[1] == 1:\n",
    "                batch_labels = batch_labels.squeeze(1)  # bỏ chiều 1\n",
    "\n",
    "            if batch_labels.ndim == 2 and batch_labels.shape[1] == 2:\n",
    "                batch_labels = torch.argmax(batch_labels, dim=1)\n",
    "            elif batch_labels.ndim == 2 and batch_labels.shape[1] == 1:\n",
    "                batch_labels = batch_labels.squeeze(1)\n",
    "\n",
    "            z_mean, z_log_var, z = vae.encoder(batch_data, batch_labels)\n",
    "\n",
    "            t_forward = time_steps - 1\n",
    "            sqrt_alpha_bar = torch.sqrt(diffusion_model.alpha_bar[t_forward])\n",
    "            sqrt_one_minus_alpha_bar = torch.sqrt(1 - diffusion_model.alpha_bar[t_forward])\n",
    "            noisy_z = sqrt_alpha_bar * z + sqrt_one_minus_alpha_bar * torch.randn_like(z)\n",
    "\n",
    "            z_recovered = diffusion_model.sample(noisy_z)\n",
    "            logits = classifier(z_recovered)\n",
    "\n",
    "            # print(\"Logits shape:\", logits.shape)  # Debug\n",
    "\n",
    "            if len(logits.shape) == 1:\n",
    "                preds = (torch.sigmoid(logits) > 0.5).float()\n",
    "            elif len(logits.shape) == 2:\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected logits shape: {logits.shape}\")\n",
    "\n",
    "            # print(f\"preds shape: {preds.shape}, batch_labels shape: {batch_labels.shape}\")  # Debug\n",
    "            preds = preds.cpu().numpy().flatten()\n",
    "            labels = batch_labels.squeeze().cpu().numpy().flatten()\n",
    "\n",
    "            # Lưu lại cho metrics\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels)\n",
    "\n",
    "            correct += (preds == labels).sum()\n",
    "            total += len(labels)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(all_labels, all_preds))\n",
    "\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(all_labels, all_preds))\n",
    "\n",
    "    # Nếu bạn muốn trả về thêm các chỉ số cụ thể:\n",
    "    accuracy = sum(np.array(all_preds) == np.array(all_labels)) / len(all_labels)\n",
    "    f1 = f1_score(all_labels, all_preds, average='binary' if len(np.unique(all_labels)) == 2 else 'macro')\n",
    "    precision = precision_score(all_labels, all_preds, average='binary' if len(np.unique(all_labels)) == 2 else 'macro')\n",
    "    recall = recall_score(all_labels, all_preds, average='binary' if len(np.unique(all_labels)) == 2 else 'macro')\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"f1_score\": f1,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall\n",
    "    }\n",
    "    print(f\"Accuracy on recovered z: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# latent_dim = vae.encoder[-1].out_features\n",
    "diffusion_model = SimpleDiffusionModel(latent_dim=latent_dim).to(device)\n",
    "diffusion_optimizer = optim.Adam(diffusion_model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss: -15.7032 | Acc: 0.7632\n",
      "Epoch 2 | Loss: -13.0239 | Acc: 0.6842\n",
      "Epoch 3 | Loss: -13.1641 | Acc: 0.7368\n",
      "Epoch 4 | Loss: -14.5649 | Acc: 0.7632\n",
      "Epoch 5 | Loss: -13.7187 | Acc: 0.8684\n",
      "Epoch 6 | Loss: -13.8300 | Acc: 0.7632\n",
      "Epoch 7 | Loss: -14.2250 | Acc: 0.8421\n",
      "Epoch 8 | Loss: -14.5662 | Acc: 0.7895\n",
      "Epoch 9 | Loss: -12.8423 | Acc: 0.8947\n",
      "Epoch 10 | Loss: -16.5353 | Acc: 0.7895\n",
      "Epoch 11 | Loss: -15.3703 | Acc: 0.8947\n",
      "Epoch 12 | Loss: -12.8416 | Acc: 0.9211\n",
      "Epoch 13 | Loss: -14.3444 | Acc: 0.8684\n",
      "Epoch 14 | Loss: -12.8352 | Acc: 0.9211\n",
      "Epoch 15 | Loss: -11.0758 | Acc: 0.8684\n",
      "Epoch 16 | Loss: -13.4311 | Acc: 0.9474\n",
      "Epoch 17 | Loss: -15.7584 | Acc: 0.8947\n",
      "Epoch 18 | Loss: nan | Acc: 0.5000\n",
      "Epoch 19 | Loss: nan | Acc: 0.5000\n",
      "Epoch 20 | Loss: nan | Acc: 0.4737\n",
      "Epoch 21 | Loss: nan | Acc: 0.4474\n",
      "Epoch 22 | Loss: nan | Acc: 0.3947\n",
      "Epoch 23 | Loss: nan | Acc: 0.5000\n",
      "Epoch 24 | Loss: nan | Acc: 0.5526\n",
      "Epoch 25 | Loss: nan | Acc: 0.4474\n",
      "Epoch 26 | Loss: nan | Acc: 0.6053\n",
      "Epoch 27 | Loss: nan | Acc: 0.5526\n",
      "Epoch 28 | Loss: nan | Acc: 0.4211\n",
      "Epoch 29 | Loss: nan | Acc: 0.4737\n",
      "Epoch 30 | Loss: nan | Acc: 0.5526\n",
      "Epoch 31 | Loss: nan | Acc: 0.5263\n",
      "Epoch 32 | Loss: nan | Acc: 0.5789\n",
      "Epoch 33 | Loss: nan | Acc: 0.3684\n",
      "Epoch 34 | Loss: nan | Acc: 0.4211\n",
      "Epoch 35 | Loss: nan | Acc: 0.4474\n",
      "Epoch 36 | Loss: nan | Acc: 0.4211\n",
      "Epoch 37 | Loss: nan | Acc: 0.5789\n",
      "Epoch 38 | Loss: nan | Acc: 0.5789\n",
      "Epoch 39 | Loss: nan | Acc: 0.5000\n",
      "Epoch 40 | Loss: nan | Acc: 0.6053\n",
      "Epoch 41 | Loss: nan | Acc: 0.4474\n",
      "Epoch 42 | Loss: nan | Acc: 0.4737\n",
      "Epoch 43 | Loss: nan | Acc: 0.3684\n",
      "Epoch 44 | Loss: nan | Acc: 0.5526\n",
      "Epoch 45 | Loss: nan | Acc: 0.4737\n",
      "Epoch 46 | Loss: nan | Acc: 0.6053\n",
      "Epoch 47 | Loss: nan | Acc: 0.3947\n",
      "Epoch 48 | Loss: nan | Acc: 0.5789\n",
      "Epoch 49 | Loss: nan | Acc: 0.6842\n",
      "Epoch 50 | Loss: nan | Acc: 0.5263\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      1.00      0.97      4782\n",
      "           1       0.00      0.00      0.00       307\n",
      "\n",
      "    accuracy                           0.94      5089\n",
      "   macro avg       0.47      0.50      0.48      5089\n",
      "weighted avg       0.88      0.94      0.91      5089\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[4782    0]\n",
      " [ 307    0]]\n",
      "Accuracy on recovered z: 0.9397\n",
      "F1 Score: 0.0000\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Admin\\miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Admin\\miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Admin\\miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Giả sử bạn đã có sẵn các model con\n",
    "full_model = FullPipelineModel(vae=vae, diffusion=diffusion_model, classifier=classifier).to(device)\n",
    "optimizer = torch.optim.Adam(full_model.parameters(), lr=1e-4)\n",
    "\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    full_model.train()\n",
    "    for batch_data, batch_labels in train_loader:\n",
    "        batch_data = batch_data.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "\n",
    "        # Nếu label là one-hot encoded, chuyển thành dạng long\n",
    "        if batch_labels.dim() == 2 and batch_labels.size(1) == 2:\n",
    "            batch_labels = torch.argmax(batch_labels, dim=1)\n",
    "\n",
    "        metrics = full_model.train_step(batch_data, batch_labels, optimizer)\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Loss: {metrics['total_loss']:.4f} | Acc: {metrics['accuracy']:.4f}\")\n",
    "\n",
    "metrics = evaluate_diffusion_with_classifier(\n",
    "    vae=vae,\n",
    "    diffusion_model=diffusion_model,\n",
    "    classifier=classifier,\n",
    "    test_loader=test_loader,\n",
    "    device=device,\n",
    "    time_steps=100\n",
    ")\n",
    "\n",
    "print(f\"Accuracy on recovered z: {metrics['accuracy']:.4f}\")\n",
    "print(f\"F1 Score: {metrics['f1_score']:.4f}\")\n",
    "print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "print(f\"Recall: {metrics['recall']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # latent_dim = vae.encoder[-1].out_features\n",
    "# diffusion_model = SimpleDiffusionModel(latent_dim=latent_dim).to(device)\n",
    "# diffusion_optimizer = optim.Adam(diffusion_model.parameters(), lr=1e-3)\n",
    "\n",
    "# train_diffusion(vae_new, diffusion_model, train_loader, diffusion_optimizer, device, epochs=100)\n",
    "\n",
    "# evaluate_diffusion_with_classifier(vae_new, diffusion_model, vae.classifier, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
