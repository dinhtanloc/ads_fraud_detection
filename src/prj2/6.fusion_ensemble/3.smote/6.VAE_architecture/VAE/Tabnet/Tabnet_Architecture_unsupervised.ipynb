{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SymbolAlreadyExposedError",
     "evalue": "Symbol Zeros is already exposed as ().",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSymbolAlreadyExposedError\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Optional, Union, Tuple\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# import tensorflow_probability as tfp\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_addons\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtfa\u001b[39;00m\n",
      "File \u001b[1;32md:\\ads_test\\.conda\\lib\\site-packages\\tensorflow\\__init__.py:478\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;66;03m# Do an eager load for Keras' code so that any function/method that needs to\u001b[39;00m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;66;03m# happen at load time will trigger, eg registration of optimizers in the\u001b[39;00m\n\u001b[0;32m    475\u001b[0m \u001b[38;5;66;03m# SavedModel registry.\u001b[39;00m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;66;03m# See b/196254385 for more details.\u001b[39;00m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 478\u001b[0m   \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkeras.optimizers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mImportError\u001b[39;00m, \u001b[38;5;167;01mAttributeError\u001b[39;00m):\n\u001b[0;32m    480\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32md:\\ads_test\\.conda\\lib\\importlib\\__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    126\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\ads_test\\.conda\\lib\\site-packages\\keras\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"AUTOGENERATED. DO NOT EDIT.\"\"\"\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __internal__\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m activations\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m applications\n",
      "File \u001b[1;32md:\\ads_test\\.conda\\lib\\site-packages\\keras\\__internal__\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"AUTOGENERATED. DO NOT EDIT.\"\"\"\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layers\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m losses\n",
      "File \u001b[1;32md:\\ads_test\\.conda\\lib\\site-packages\\keras\\__internal__\\backend\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"AUTOGENERATED. DO NOT EDIT.\"\"\"\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _initialize_variables \u001b[38;5;28;01mas\u001b[39;00m initialize_variables\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m track_variable\n",
      "File \u001b[1;32md:\\ads_test\\.conda\\lib\\site-packages\\keras\\src\\__init__.py:21\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Implementation of the Keras API, the high-level API of TensorFlow.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03mDetailed documentation and user guides are available at\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03m[keras.io](https://keras.io).\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m applications\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m models\n",
      "File \u001b[1;32md:\\ads_test\\.conda\\lib\\site-packages\\keras\\src\\applications\\__init__.py:18\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Keras Applications are premade architectures with pre-trained weights.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapplications\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconvnext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConvNeXtBase\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapplications\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconvnext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConvNeXtLarge\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapplications\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconvnext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConvNeXtSmall\n",
      "File \u001b[1;32md:\\ads_test\\.conda\\lib\\site-packages\\keras\\src\\applications\\convnext.py:28\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m initializers\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layers\n",
      "File \u001b[1;32md:\\ads_test\\.conda\\lib\\site-packages\\keras\\src\\backend.py:35\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute_coordinator_utils \u001b[38;5;28;01mas\u001b[39;00m dc\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtensor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dtensor_api \u001b[38;5;28;01mas\u001b[39;00m dtensor\n\u001b[1;32m---> 35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras_tensor\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m control_flow_util\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m object_identity\n",
      "File \u001b[1;32md:\\ads_test\\.conda\\lib\\site-packages\\keras\\src\\engine\\keras_tensor.py:19\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Keras Input Tensor used to track functional API Topology.\"\"\"\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m object_identity\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# isort: off\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m structure\n",
      "File \u001b[1;32md:\\ads_test\\.conda\\lib\\site-packages\\keras\\src\\utils\\__init__.py:20\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Public Keras utilities.\"\"\"\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# isort: off\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Serialization related\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mserialization_lib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deserialize_keras_object\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mserialization_lib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m serialize_keras_object\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mobject_registration\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CustomObjectScope\n",
      "File \u001b[1;32md:\\ads_test\\.conda\\lib\\site-packages\\keras\\src\\saving\\serialization_lib.py:28\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m object_registration\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m serialization \u001b[38;5;28;01mas\u001b[39;00m legacy_serialization\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaved_model\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m in_tf_saved_model_scope\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m generic_utils\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# isort: off\u001b[39;00m\n",
      "File \u001b[1;32md:\\ads_test\\.conda\\lib\\site-packages\\keras\\src\\saving\\legacy\\saved_model\\utils.py:30\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_contextlib\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeneric_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LazyLoader\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayer_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CallFunctionSpec\n\u001b[0;32m     32\u001b[0m training_lib \u001b[38;5;241m=\u001b[39m LazyLoader(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_lib\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mglobals\u001b[39m(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.src.engine.training\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21muse_wrapped_call\u001b[39m(\n\u001b[0;32m     36\u001b[0m     layer, call_fn, call_spec, default_training_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, return_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     37\u001b[0m ):\n",
      "File \u001b[1;32md:\\ads_test\\.conda\\lib\\site-packages\\keras\\src\\utils\\layer_utils.py:26\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m initializers\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m io_utils\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# isort: off\u001b[39;00m\n",
      "File \u001b[1;32md:\\ads_test\\.conda\\lib\\site-packages\\keras\\src\\initializers\\__init__.py:23\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minitializers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m initializers\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minitializers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m initializers_v1\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m serialization_lib\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m serialization \u001b[38;5;28;01mas\u001b[39;00m legacy_serialization\n",
      "File \u001b[1;32md:\\ads_test\\.conda\\lib\\site-packages\\keras\\src\\initializers\\initializers_v1.py:32\u001b[0m\n\u001b[0;32m     29\u001b[0m _v1_glorot_uniform_initializer \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mglorot_uniform_initializer\n\u001b[0;32m     30\u001b[0m _v1_glorot_normal_initializer \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mglorot_normal_initializer\n\u001b[1;32m---> 32\u001b[0m \u001b[43mkeras_export\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkeras.initializers.Zeros\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkeras.initializers.zeros\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_v1_zeros_initializer\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m keras_export(v1\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.initializers.Ones\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.initializers.ones\u001b[39m\u001b[38;5;124m\"\u001b[39m])(\n\u001b[0;32m     36\u001b[0m     _v1_ones_initializer\n\u001b[0;32m     37\u001b[0m )\n\u001b[0;32m     38\u001b[0m keras_export(v1\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.initializers.Constant\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.initializers.constant\u001b[39m\u001b[38;5;124m\"\u001b[39m])(\n\u001b[0;32m     39\u001b[0m     _v1_constant_initializer\n\u001b[0;32m     40\u001b[0m )\n",
      "File \u001b[1;32md:\\ads_test\\.conda\\lib\\site-packages\\tensorflow\\python\\util\\tf_export.py:348\u001b[0m, in \u001b[0;36mapi_export.__call__\u001b[1;34m(self, func)\u001b[0m\n\u001b[0;32m    345\u001b[0m   \u001b[38;5;28mdelattr\u001b[39m(undecorated_f, api_names_attr_v1)\n\u001b[0;32m    347\u001b[0m _, undecorated_func \u001b[38;5;241m=\u001b[39m tf_decorator\u001b[38;5;241m.\u001b[39munwrap(func)\n\u001b[1;32m--> 348\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_attr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mundecorated_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapi_names_attr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_attr(undecorated_func, api_names_attr_v1, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_names_v1)\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_names:\n",
      "File \u001b[1;32md:\\ads_test\\.conda\\lib\\site-packages\\tensorflow\\python\\util\\tf_export.py:364\u001b[0m, in \u001b[0;36mapi_export.set_attr\u001b[1;34m(self, func, api_names_attr, names)\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m api_names_attr \u001b[38;5;129;01min\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[0;32m    363\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_allow_multiple_exports:\n\u001b[1;32m--> 364\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SymbolAlreadyExposedError(\n\u001b[0;32m    365\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSymbol \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is already exposed as \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m    366\u001b[0m         (func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;28mgetattr\u001b[39m(func, api_names_attr)))  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;28msetattr\u001b[39m(func, api_names_attr, names)\n",
      "\u001b[1;31mSymbolAlreadyExposedError\u001b[0m: Symbol Zeros is already exposed as ()."
     ]
    }
   ],
   "source": [
    "from typing import Optional, Union, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def identity(x):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow_addons in d:\\ads_test\\.conda\\lib\\site-packages (0.21.0)\n",
      "Requirement already satisfied: typeguard<3.0.0,>=2.7 in d:\\ads_test\\.conda\\lib\\site-packages (from tensorflow_addons) (2.13.3)\n",
      "Requirement already satisfied: packaging in d:\\ads_test\\.conda\\lib\\site-packages (from tensorflow_addons) (24.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Feature_1  Feature_2  Feature_3  Feature_4  Feature_5  Feature_6  \\\n",
      "0   0.374540   0.185133   0.261706   0.672703   0.571996   0.393636   \n",
      "1   0.950714   0.541901   0.246979   0.796681   0.805432   0.473436   \n",
      "2   0.731994   0.872946   0.906255   0.250468   0.760161   0.854547   \n",
      "3   0.598658   0.732225   0.249546   0.624874   0.153900   0.340004   \n",
      "4   0.156019   0.806561   0.271950   0.571746   0.149249   0.869650   \n",
      "\n",
      "   Feature_7  Feature_8  Feature_9  Feature_10  Feature_11  Feature_12  Target  \n",
      "0   0.648257   0.038799   0.720268    0.913578    0.373641    0.533031       1  \n",
      "1   0.172386   0.186773   0.687283    0.525360    0.332912    0.137899       1  \n",
      "2   0.872395   0.831246   0.095754    0.724910    0.176154    0.591243       0  \n",
      "3   0.613116   0.766768   0.922572    0.436048    0.607267    0.314786       0  \n",
      "4   0.157204   0.350643   0.568472    0.630035    0.476624    0.052349       0  \n",
      "Số lượng mẫu: 1000, Số lượng cột: 13\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)  \n",
    "\n",
    "num_samples = 1000\n",
    "\n",
    "data = {}\n",
    "for i in range(1, 13):\n",
    "    col_name = f\"Feature_{i}\"\n",
    "    data[col_name] = np.random.rand(num_samples)  \n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df['Target'] = np.random.randint(0, 2, size=num_samples) \n",
    "\n",
    "print(df.head())\n",
    "feature_columns = list(df.columns[:-1])  \n",
    "\n",
    "print(f\"Số lượng mẫu: {len(df)}, Số lượng cột: {len(df.columns)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GLUBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, units: Optional[int] = None,\n",
    "                 virtual_batch_size: Optional[int] = 128, \n",
    "                 momentum: Optional[float] = 0.02):\n",
    "        super(GLUBlock, self).__init__()\n",
    "        self.units = units\n",
    "        self.virtual_batch_size = virtual_batch_size\n",
    "        self.momentum = momentum\n",
    "        \n",
    "    def build(self, input_shape: tf.TensorShape):\n",
    "        if self.units is None:\n",
    "            self.units = input_shape[-1]\n",
    "            \n",
    "        self.fc_outout = tf.keras.layers.Dense(self.units, \n",
    "                                               use_bias=False)\n",
    "        self.bn_outout = tf.keras.layers.BatchNormalization(virtual_batch_size=self.virtual_batch_size, \n",
    "                                                            momentum=self.momentum)\n",
    "        \n",
    "        self.fc_gate = tf.keras.layers.Dense(self.units, \n",
    "                                             use_bias=False)\n",
    "        self.bn_gate = tf.keras.layers.BatchNormalization(virtual_batch_size=self.virtual_batch_size, \n",
    "                                                          momentum=self.momentum)\n",
    "        \n",
    "    def call(self, inputs: Union[tf.Tensor, np.ndarray], training: Optional[bool] = None):\n",
    "        output = self.bn_outout(self.fc_outout(inputs), \n",
    "                                training=training)\n",
    "        gate = self.bn_gate(self.fc_gate(inputs), \n",
    "                            training=training)\n",
    "    \n",
    "        return output * tf.keras.activations.sigmoid(gate) # GLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureTransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, units: Optional[int] = None, virtual_batch_size: Optional[int]=128, \n",
    "                 momentum: Optional[float] = 0.02, skip=False):\n",
    "        super(FeatureTransformerBlock, self).__init__()\n",
    "        self.units = units\n",
    "        self.virtual_batch_size = virtual_batch_size\n",
    "        self.momentum = momentum\n",
    "        self.skip = skip\n",
    "        \n",
    "    def build(self, input_shape: tf.TensorShape):\n",
    "        if self.units is None:\n",
    "            self.units = input_shape[-1]\n",
    "        \n",
    "        self.initial = GLUBlock(units = self.units, \n",
    "                                virtual_batch_size=self.virtual_batch_size, \n",
    "                                momentum=self.momentum)\n",
    "        self.residual =  GLUBlock(units = self.units, \n",
    "                                  virtual_batch_size=self.virtual_batch_size, \n",
    "                                  momentum=self.momentum)\n",
    "        \n",
    "    def call(self, inputs: Union[tf.Tensor, np.ndarray], training: Optional[bool] = None):\n",
    "        initial = self.initial(inputs, training=training)\n",
    "        \n",
    "        if self.skip == True:\n",
    "            initial += inputs\n",
    "\n",
    "        residual = self.residual(initial, training=training) # skip\n",
    "        \n",
    "        return (initial + residual) * np.sqrt(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentiveTransformer(tf.keras.layers.Layer):\n",
    "    def __init__(self, units: Optional[int] = None, virtual_batch_size: Optional[int] = 128, \n",
    "                 momentum: Optional[float] = 0.02):\n",
    "        super(AttentiveTransformer, self).__init__()\n",
    "        self.units = units\n",
    "        self.virtual_batch_size = virtual_batch_size\n",
    "        self.momentum = momentum\n",
    "        \n",
    "    def build(self, input_shape: tf.TensorShape):\n",
    "        if self.units is None:\n",
    "            self.units = input_shape[-1]\n",
    "            \n",
    "        self.fc = tf.keras.layers.Dense(self.units, \n",
    "                                        use_bias=False)\n",
    "        self.bn = tf.keras.layers.BatchNormalization(virtual_batch_size=self.virtual_batch_size, \n",
    "                                                     momentum=self.momentum)\n",
    "        \n",
    "    def call(self, inputs: Union[tf.Tensor, np.ndarray], priors: Optional[Union[tf.Tensor, np.ndarray]] = None, training: Optional[bool] = None) -> tf.Tensor:\n",
    "        feature = self.bn(self.fc(inputs), \n",
    "                          training=training)\n",
    "        if priors is None:\n",
    "            output = feature\n",
    "        else:\n",
    "            output = feature * priors\n",
    "        \n",
    "        return tfa.activations.sparsemax(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabNetStep(tf.keras.layers.Layer):\n",
    "    def __init__(self, units: Optional[int] = None, virtual_batch_size: Optional[int]=128, \n",
    "                 momentum: Optional[float] =0.02):\n",
    "        super(TabNetStep, self).__init__()\n",
    "        self.units = units\n",
    "        self.virtual_batch_size = virtual_batch_size\n",
    "        self.momentum = momentum\n",
    "        \n",
    "    def build(self, input_shape: tf.TensorShape):\n",
    "        if self.units is None:\n",
    "            self.units = input_shape[-1]\n",
    "        \n",
    "        self.unique = FeatureTransformerBlock(units = self.units, \n",
    "                                              virtual_batch_size=self.virtual_batch_size, \n",
    "                                              momentum=self.momentum,\n",
    "                                              skip=True)\n",
    "        self.attention = AttentiveTransformer(units = input_shape[-1], \n",
    "                                              virtual_batch_size=self.virtual_batch_size, \n",
    "                                              momentum=self.momentum)\n",
    "        \n",
    "    def call(self, inputs, shared, priors, training=None) -> Tuple[tf.Tensor]:  \n",
    "        split = self.unique(shared, training=training)\n",
    "        keys = self.attention(split, priors, training=training)\n",
    "        masked = keys * inputs\n",
    "        \n",
    "        return split, masked, keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabNetEncoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, units: int =1, \n",
    "                 n_steps: int = 3, \n",
    "                 n_features: int = 8,\n",
    "                 outputs: int = 1, \n",
    "                 gamma: float = 1.3,\n",
    "                 epsilon: float = 1e-8, \n",
    "                 sparsity: float = 1e-5, \n",
    "                 virtual_batch_size: Optional[int]=128, \n",
    "                 momentum: Optional[float] =0.02):\n",
    "        super(TabNetEncoder, self).__init__()\n",
    "        \n",
    "        self.units = units\n",
    "        self.n_steps = n_steps\n",
    "        self.n_features = n_features\n",
    "        self.virtual_batch_size = virtual_batch_size\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.momentum = momentum\n",
    "        self.sparsity = sparsity\n",
    "        \n",
    "    def build(self, input_shape: tf.TensorShape):            \n",
    "        self.bn = tf.keras.layers.BatchNormalization(virtual_batch_size=self.virtual_batch_size, \n",
    "                                                     momentum=self.momentum)\n",
    "        self.shared_block = FeatureTransformerBlock(units = self.n_features, \n",
    "                                                    virtual_batch_size=self.virtual_batch_size, \n",
    "                                                    momentum=self.momentum)        \n",
    "        self.initial_step = TabNetStep(units = self.n_features, \n",
    "                                       virtual_batch_size=self.virtual_batch_size, \n",
    "                                       momentum=self.momentum)\n",
    "        self.steps = [TabNetStep(units = self.n_features, \n",
    "                                 virtual_batch_size=self.virtual_batch_size, \n",
    "                                 momentum=self.momentum) for _ in range(self.n_steps)]\n",
    "        self.final = tf.keras.layers.Dense(units = self.units, \n",
    "                                           use_bias=False)\n",
    "    \n",
    "\n",
    "    def call(self, X: Union[tf.Tensor, np.ndarray], training: Optional[bool] = None) -> Tuple[tf.Tensor]:        \n",
    "        entropy_loss = 0.\n",
    "        encoded = 0.\n",
    "        output = 0.\n",
    "        importance = 0.\n",
    "        prior = tf.reduce_mean(tf.ones_like(X), axis=0)\n",
    "        \n",
    "        B = prior * self.bn(X, training=training)\n",
    "        shared = self.shared_block(B, training=training)\n",
    "        _, masked, keys = self.initial_step(B, shared, prior, training=training)\n",
    "\n",
    "        for step in self.steps:\n",
    "            entropy_loss += tf.reduce_mean(tf.reduce_sum(-keys * tf.math.log(keys + self.epsilon), axis=-1)) / tf.cast(self.n_steps, tf.float32)\n",
    "            prior *= (self.gamma - tf.reduce_mean(keys, axis=0))\n",
    "            importance += keys\n",
    "            \n",
    "            shared = self.shared_block(masked, training=training)\n",
    "            split, masked, keys = step(B, shared, prior, training=training)\n",
    "            features = tf.keras.activations.relu(split)\n",
    "            \n",
    "            output += features\n",
    "            encoded += split\n",
    "            \n",
    "        self.add_loss(self.sparsity * entropy_loss)\n",
    "          \n",
    "        prediction = self.final(output)\n",
    "        return prediction, encoded, importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabNetDecoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, units=1, \n",
    "                 n_steps = 3, \n",
    "                 n_features = 8,\n",
    "                 outputs = 1, \n",
    "                 gamma = 1.3,\n",
    "                 epsilon = 1e-8, \n",
    "                 sparsity = 1e-5, \n",
    "                 virtual_batch_size=128, \n",
    "                 momentum=0.02):\n",
    "        super(TabNetDecoder, self).__init__()\n",
    "        \n",
    "        self.units = units\n",
    "        self.n_steps = n_steps\n",
    "        self.n_features = n_features\n",
    "        self.virtual_batch_size = virtual_batch_size\n",
    "        self.momentum = momentum\n",
    "        \n",
    "    def build(self, input_shape: tf.TensorShape):\n",
    "        self.shared_block = FeatureTransformerBlock(units = self.n_features, \n",
    "                                                    virtual_batch_size=self.virtual_batch_size, \n",
    "                                                    momentum=self.momentum)\n",
    "        self.steps = [FeatureTransformerBlock(units = self.n_features,\n",
    "                                              virtual_batch_size=self.virtual_batch_size, \n",
    "                                              momentum=self.momentum) for _ in range(self.n_steps)]\n",
    "        self.fc = [tf.keras.layers.Dense(units = self.units) for _ in range(self.n_steps)]\n",
    "    \n",
    "\n",
    "    def call(self, X: Union[tf.Tensor, np.ndarray], training: Optional[bool] = None) -> tf.Tensor:\n",
    "        decoded = 0.\n",
    "        \n",
    "        for ftb, fc in zip(self.steps, self.fc):\n",
    "            shared = self.shared_block(X, training=training)\n",
    "            feature = ftb(shared, training=training)\n",
    "            output = fc(feature)\n",
    "            \n",
    "            decoded += output\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tab_net_autoencoder_9/tab_net_encoder_9/tab_net_step_3/attentive_transformer_3/dense_24/kernel:0', 'tab_net_autoencoder_9/tab_net_encoder_9/tab_net_step_3/attentive_transformer_3/batch_normalization_24/gamma:0', 'tab_net_autoencoder_9/tab_net_encoder_9/tab_net_step_3/attentive_transformer_3/batch_normalization_24/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tab_net_autoencoder_9/tab_net_encoder_9/tab_net_step_3/attentive_transformer_3/dense_24/kernel:0', 'tab_net_autoencoder_9/tab_net_encoder_9/tab_net_step_3/attentive_transformer_3/batch_normalization_24/gamma:0', 'tab_net_autoencoder_9/tab_net_encoder_9/tab_net_step_3/attentive_transformer_3/batch_normalization_24/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tab_net_autoencoder_9/tab_net_encoder_9/tab_net_step_3/attentive_transformer_3/dense_24/kernel:0', 'tab_net_autoencoder_9/tab_net_encoder_9/tab_net_step_3/attentive_transformer_3/batch_normalization_24/gamma:0', 'tab_net_autoencoder_9/tab_net_encoder_9/tab_net_step_3/attentive_transformer_3/batch_normalization_24/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tab_net_autoencoder_9/tab_net_encoder_9/tab_net_step_3/attentive_transformer_3/dense_24/kernel:0', 'tab_net_autoencoder_9/tab_net_encoder_9/tab_net_step_3/attentive_transformer_3/batch_normalization_24/gamma:0', 'tab_net_autoencoder_9/tab_net_encoder_9/tab_net_step_3/attentive_transformer_3/batch_normalization_24/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "4/4 [==============================] - 16s 7ms/step - loss: 1.3309\n",
      "Epoch 2/100\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 1.1500\n",
      "Epoch 3/100\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 1.0945\n",
      "Epoch 4/100\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 1.1289\n",
      "Epoch 5/100\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 1.0723\n",
      "Epoch 6/100\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.8916\n",
      "Epoch 7/100\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.9566\n",
      "Epoch 8/100\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.9176\n",
      "Epoch 9/100\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.8996\n",
      "Epoch 10/100\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.9487\n",
      "Epoch 11/100\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.9237\n",
      "Epoch 12/100\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.9244\n",
      "Epoch 13/100\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.9189\n",
      "Epoch 14/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.9490\n",
      "Epoch 15/100\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.8875\n",
      "Epoch 16/100\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.8386\n",
      "Epoch 17/100\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.8720\n",
      "Epoch 18/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.8661\n",
      "Epoch 19/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.8158\n",
      "Epoch 20/100\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.7744\n",
      "Epoch 21/100\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.8313\n",
      "Epoch 22/100\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.8237\n",
      "Epoch 23/100\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.8305\n",
      "Epoch 24/100\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.7962\n",
      "Epoch 25/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.8351\n",
      "Epoch 26/100\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.8519\n",
      "Epoch 27/100\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.8681\n",
      "Epoch 28/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.8464\n",
      "Epoch 29/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.8022\n",
      "Epoch 30/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.8338\n",
      "Epoch 31/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7696\n",
      "Epoch 32/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7818\n",
      "Epoch 33/100\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.7472\n",
      "Epoch 34/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.7785\n",
      "Epoch 35/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.7503\n",
      "Epoch 36/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7793\n",
      "Epoch 37/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7437\n",
      "Epoch 38/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.7595\n",
      "Epoch 39/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7410\n",
      "Epoch 40/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7422\n",
      "Epoch 41/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7672\n",
      "Epoch 42/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7555\n",
      "Epoch 43/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7304\n",
      "Epoch 44/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7490\n",
      "Epoch 45/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7687\n",
      "Epoch 46/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7561\n",
      "Epoch 47/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7704\n",
      "Epoch 48/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7368\n",
      "Epoch 49/100\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.7466\n",
      "Epoch 50/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7595\n",
      "Epoch 51/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7321\n",
      "Epoch 52/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7409\n",
      "Epoch 53/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7377\n",
      "Epoch 54/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7369\n",
      "Epoch 55/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7415\n",
      "Epoch 56/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7273\n",
      "Epoch 57/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7402\n",
      "Epoch 58/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7583\n",
      "Epoch 59/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7364\n",
      "Epoch 60/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7404\n",
      "Epoch 61/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7327\n",
      "Epoch 62/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7435\n",
      "Epoch 63/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.7191\n",
      "Epoch 64/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.7182\n",
      "Epoch 65/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7183\n",
      "Epoch 66/100\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.7285\n",
      "Epoch 67/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.7219\n",
      "Epoch 68/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.7302\n",
      "Epoch 69/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.7161\n",
      "Epoch 70/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.7307\n",
      "Epoch 71/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7237\n",
      "Epoch 72/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7245\n",
      "Epoch 73/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7120\n",
      "Epoch 74/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7116\n",
      "Epoch 75/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7090\n",
      "Epoch 76/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7155\n",
      "Epoch 77/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.7083\n",
      "Epoch 78/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7153\n",
      "Epoch 79/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7208\n",
      "Epoch 80/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7199\n",
      "Epoch 81/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.7112\n",
      "Epoch 82/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.7150\n",
      "Epoch 83/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.7059\n",
      "Epoch 84/100\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.7088\n",
      "Epoch 85/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.7095\n",
      "Epoch 86/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.7043\n",
      "Epoch 87/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.7093\n",
      "Epoch 88/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7056\n",
      "Epoch 89/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7055\n",
      "Epoch 90/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7057\n",
      "Epoch 91/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7129\n",
      "Epoch 92/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7068\n",
      "Epoch 93/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7044\n",
      "Epoch 94/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7143\n",
      "Epoch 95/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7075\n",
      "Epoch 96/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7114\n",
      "Epoch 97/100\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.7152\n",
      "Epoch 98/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.7027\n",
      "Epoch 99/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.6992\n",
      "Epoch 100/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7104\n",
      "Model: \"tab_net_autoencoder_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lambda_8 (Lambda)           multiple                  0         \n",
      "                                                                 \n",
      " tab_net_encoder_9 (TabNetE  multiple                  618       \n",
      " ncoder)                                                         \n",
      "                                                                 \n",
      " tab_net_decoder_9 (TabNetD  multiple                  300       \n",
      " ecoder)                                                         \n",
      "                                                                 \n",
      " batch_normalization_9 (Bat  multiple                  48        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         multiple                  0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 966 (3.77 KB)\n",
      "Trainable params: 678 (2.65 KB)\n",
      "Non-trainable params: 288 (1.12 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from typing import Optional, Union, Tuple\n",
    "\n",
    "class TabNetAutoencoder(tf.keras.Model):\n",
    "    def __init__(self, outputs: int = 1, \n",
    "                 inputs: int = 12,\n",
    "                 n_steps: int = 3, \n",
    "                 n_features: int = 8,\n",
    "                 gamma: float = 1.3, \n",
    "                 epsilon: float = 1e-8, \n",
    "                 sparsity: float = 1e-5, \n",
    "                 feature_column: Optional[tf.keras.layers.DenseFeatures] = None, \n",
    "                 virtual_batch_size: Optional[int] = None, \n",
    "                 momentum: Optional[float] = 0.02):\n",
    "        super(TabNetAutoencoder, self).__init__()\n",
    "        \n",
    "        self.outputs = outputs\n",
    "        self.inputs = inputs\n",
    "        self.n_steps = n_steps\n",
    "        self.n_features = n_features\n",
    "        self.feature_column = feature_column\n",
    "        self.virtual_batch_size = virtual_batch_size\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.momentum = momentum\n",
    "        self.sparsity = sparsity\n",
    "        \n",
    "        if feature_column is None:\n",
    "            self.feature = tf.keras.layers.Lambda(lambda x: x)\n",
    "        else:\n",
    "            self.feature = feature_column\n",
    "            \n",
    "        self.encoder = TabNetEncoder(units=outputs, \n",
    "                                    n_steps=n_steps, \n",
    "                                    n_features=n_features,\n",
    "                                    gamma=gamma, \n",
    "                                    epsilon=epsilon, \n",
    "                                    sparsity=sparsity,\n",
    "                                    virtual_batch_size=virtual_batch_size, \n",
    "                                    momentum=momentum)\n",
    "        \n",
    "        self.decoder = TabNetDecoder(units=inputs, \n",
    "                                     n_steps=n_steps, \n",
    "                                     n_features=n_features,\n",
    "                                     virtual_batch_size=virtual_batch_size, \n",
    "                                     momentum=momentum)\n",
    "        \n",
    "        self.bn = tf.keras.layers.BatchNormalization(momentum=momentum)\n",
    "        \n",
    "        self.do = tf.keras.layers.Dropout(0.25)\n",
    "\n",
    "    def call(self, X: Union[tf.Tensor, np.ndarray], training: Optional[bool] = None) -> tf.Tensor:\n",
    "        X = self.feature(X)\n",
    "        X = self.bn(X, training=training)\n",
    "        \n",
    "        M = self.do(tf.ones_like(X), training=training)\n",
    "        D = X * M\n",
    "        \n",
    "        output, encoded, importance = self.encoder(D)\n",
    "        prediction = tf.keras.activations.sigmoid(output)        \n",
    "        \n",
    "        T = X * (1 - M)\n",
    "        reconstruction = self.decoder(encoded)\n",
    "        \n",
    "        loss = tf.reduce_mean(tf.where(M != 0., tf.square(T - reconstruction), tf.zeros_like(reconstruction)))\n",
    "        self.add_loss(loss)\n",
    "        \n",
    "        return prediction\n",
    "    \n",
    "    def transform(self, X: Union[tf.Tensor, np.ndarray], training: Optional[bool] = None) -> tf.Tensor:\n",
    "        X = self.feature(X)\n",
    "        _, encoded, _, _, _ = self.encoder(X)\n",
    "        return encoded\n",
    "    \n",
    "    def explain(self, X: Union[tf.Tensor, np.ndarray], training: Optional[bool] = None) -> tf.Tensor:\n",
    "        X = self.feature(X)\n",
    "        _, _, importance, _, _ = self.encoder(X)\n",
    "        return importance\n",
    "\n",
    "feature_column = None  \n",
    "\n",
    "ae = TabNetAutoencoder(outputs=1, inputs=12, n_steps=3, n_features=2, feature_column=feature_column, virtual_batch_size=None)\n",
    "\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def dummy_loss(y, t):\n",
    "    return 0.\n",
    "\n",
    "ae.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.005), loss='binary_crossentropy')\n",
    "\n",
    "X_train = np.random.randn(100, 12)\n",
    "y_train = np.random.randint(0, 2, size=(100,))\n",
    "\n",
    "# Huấn luyện mô hình\n",
    "ae.fit(X_train,y_train, epochs=100)\n",
    "ae.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TabNetAutoencoder(tf.keras.Model):\n",
    "#     def __init__(self, outputs: int = 1, \n",
    "#                  inputs: int = 12,\n",
    "#                  n_steps: int  = 3, \n",
    "#                  n_features: int  = 8,\n",
    "#                  gamma: float = 1.3, \n",
    "#                  epsilon: float = 1e-8, \n",
    "#                  sparsity: float = 1e-5, \n",
    "#                  feature_column: Optional[tf.keras.layers.DenseFeatures] = None, \n",
    "#                  virtual_batch_size: Optional[int] = 128, \n",
    "#                  momentum: Optional[float] = 0.02):\n",
    "#         super(TabNetAutoencoder, self).__init__()\n",
    "        \n",
    "#         self.outputs = outputs\n",
    "#         self.inputs = inputs\n",
    "#         self.n_steps = n_steps\n",
    "#         self.n_features = n_features\n",
    "#         self.feature_column = feature_column\n",
    "#         self.virtual_batch_size = virtual_batch_size\n",
    "#         self.gamma = gamma\n",
    "#         self.epsilon = epsilon\n",
    "#         self.momentum = momentum\n",
    "#         self.sparsity = sparsity\n",
    "        \n",
    "#         if feature_column is None:\n",
    "#             self.feature = tf.keras.layers.Lambda(identity)\n",
    "#         else:\n",
    "#             self.feature = feature_column\n",
    "            \n",
    "#         self.encoder = TabNetEncoder(units=outputs, \n",
    "#                                     n_steps=n_steps, \n",
    "#                                     n_features = n_features,\n",
    "#                                     outputs=outputs, \n",
    "#                                     gamma=gamma, \n",
    "#                                     epsilon=epsilon, \n",
    "#                                     sparsity=sparsity,\n",
    "#                                     virtual_batch_size=self.virtual_batch_size, \n",
    "#                                     momentum=momentum)\n",
    "        \n",
    "#         self.decoder = TabNetDecoder(units=inputs, \n",
    "#                                      n_steps=n_steps, \n",
    "#                                      n_features = n_features,\n",
    "#                                      virtual_batch_size=self.virtual_batch_size, \n",
    "#                                      momentum=momentum)\n",
    "        \n",
    "#         self.bn = tf.keras.layers.BatchNormalization(virtual_batch_size=self.virtual_batch_size, \n",
    "#                                                      momentum=momentum)\n",
    "        \n",
    "#         self.do = tf.keras.layers.Dropout(0.25)\n",
    "\n",
    "#     def forward(self, X: Union[tf.Tensor, np.ndarray], training: Optional[bool] = None) -> Tuple[tf.Tensor]:\n",
    "#         X = self.feature(X)\n",
    "#         X = self.bn(X)\n",
    "        \n",
    "#         # training mask\n",
    "#         M = self.do(tf.ones_like(X), training=training)\n",
    "#         D = X*M\n",
    "        \n",
    "#         #encoder\n",
    "#         output, encoded, importance = self.encoder(D)\n",
    "#         prediction = tf.keras.activations.sigmoid(output)        \n",
    "        \n",
    "#         return prediction, encoded, importance, X, M\n",
    "    \n",
    "#     def call(self, X: Union[tf.Tensor, np.ndarray], training: Optional[bool] = None) -> tf.Tensor:\n",
    "#         # encode\n",
    "#         prediction, encoded, _, X, M = self.forward(X)\n",
    "#         T = X * (1 - M)\n",
    "\n",
    "#         #decode\n",
    "#         reconstruction = self.decoder(encoded)\n",
    "        \n",
    "#         #loss\n",
    "#         loss  = tf.reduce_mean(tf.where(M != 0., tf.square(T-reconstruction), tf.zeros_like(reconstruction)))\n",
    "        \n",
    "#         self.add_loss(loss)\n",
    "        \n",
    "#         return prediction\n",
    "    \n",
    "#     def transform(self, X: Union[tf.Tensor, np.ndarray], training: Optional[bool] = None) -> tf.Tensor:\n",
    "#         _, encoded, _, _, _ = self.forward(X)\n",
    "#         return encoded\n",
    "    \n",
    "#     def explain(self, X: Union[tf.Tensor, np.ndarray], training: Optional[bool] = None) -> tf.Tensor:\n",
    "#         _, _, importance, _, _ = self.forward(X)\n",
    "#         return importance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
