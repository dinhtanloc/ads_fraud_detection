{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0\n",
      "NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# print(torch.cuda.is_available())  \n",
    "# print(torch.cuda.current_device())  \n",
    "# print(torch.cuda.get_device_name(torch.cuda.current_device())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. Using GPU.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    # print(\"CUDA is available. Using GPU.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    # print(\"CUDA is not available. Using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from pytorch_tabnet.pretraining import TabNetPretrainer\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([700, 42]) torch.Size([700]) torch.Size([300, 42]) torch.Size([300])\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.DataFrame(np.random.rand(700, 42), columns=[f\"feature_{i}\" for i in range(42)])\n",
    "df_test = pd.DataFrame(np.random.rand(300, 42), columns=[f\"feature_{i}\" for i in range(42)])\n",
    "\n",
    "y_train = (np.random.rand(700) > 0.5).astype(np.float32)\n",
    "y_test = (np.random.rand(300) > 0.5).astype(np.float32)\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(df_train.values)\n",
    "X_test = scaler.transform(df_test.values)\n",
    "\n",
    "# y_train = y_train.values.astype(np.float32)\n",
    "# y_test = y_test.values.astype(np.float32)\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "# print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0.,\n",
       "        1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1.,\n",
       "        0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
       "        1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1.,\n",
       "        0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0.,\n",
       "        0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0.,\n",
       "        1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0.,\n",
       "        1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1.,\n",
       "        0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1.,\n",
       "        0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,\n",
       "        0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
       "        1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0.,\n",
       "        1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0.,\n",
       "        1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0.,\n",
       "        0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0.,\n",
       "        1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0.,\n",
       "        1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
       "        1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1.,\n",
       "        1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
       "        0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0.,\n",
       "        0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1.,\n",
       "        0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1.,\n",
       "        1., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,\n",
       "        1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1.,\n",
       "        1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0.,\n",
       "        0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0.,\n",
       "        1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0.,\n",
       "        1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0.,\n",
       "        1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1.,\n",
       "        1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1.,\n",
       "        0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.,\n",
       "        0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1.,\n",
       "        0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1.,\n",
       "        1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1.,\n",
       "        1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1.,\n",
       "        0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\miniconda3\\envs\\pytorch\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 60.91917| val_0_unsup_loss_numpy: 12.30090045928955|  0:00:00s\n",
      "epoch 1  | loss: 43.68689| val_0_unsup_loss_numpy: 10.328229904174805|  0:00:00s\n",
      "epoch 2  | loss: 32.23457| val_0_unsup_loss_numpy: 7.11821985244751|  0:00:00s\n",
      "epoch 3  | loss: 24.33612| val_0_unsup_loss_numpy: 5.316720008850098|  0:00:00s\n",
      "epoch 4  | loss: 17.89371| val_0_unsup_loss_numpy: 4.35368013381958|  0:00:00s\n",
      "epoch 5  | loss: 13.73012| val_0_unsup_loss_numpy: 4.090789794921875|  0:00:00s\n",
      "epoch 6  | loss: 11.33558| val_0_unsup_loss_numpy: 3.6928300857543945|  0:00:00s\n",
      "epoch 7  | loss: 9.38129 | val_0_unsup_loss_numpy: 2.977440118789673|  0:00:00s\n",
      "epoch 8  | loss: 7.46308 | val_0_unsup_loss_numpy: 2.6637299060821533|  0:00:01s\n",
      "epoch 9  | loss: 5.88853 | val_0_unsup_loss_numpy: 2.575589895248413|  0:00:01s\n",
      "epoch 10 | loss: 4.74838 | val_0_unsup_loss_numpy: 2.1205599308013916|  0:00:01s\n",
      "epoch 11 | loss: 4.06542 | val_0_unsup_loss_numpy: 1.9705699682235718|  0:00:01s\n",
      "epoch 12 | loss: 3.24445 | val_0_unsup_loss_numpy: 1.5197399854660034|  0:00:01s\n",
      "epoch 13 | loss: 2.61712 | val_0_unsup_loss_numpy: 1.3632099628448486|  0:00:01s\n",
      "epoch 14 | loss: 2.19361 | val_0_unsup_loss_numpy: 1.387279987335205|  0:00:01s\n",
      "epoch 15 | loss: 1.98115 | val_0_unsup_loss_numpy: 1.2516000270843506|  0:00:01s\n",
      "epoch 16 | loss: 1.78853 | val_0_unsup_loss_numpy: 1.2340400218963623|  0:00:01s\n",
      "epoch 17 | loss: 1.65363 | val_0_unsup_loss_numpy: 1.227720022201538|  0:00:01s\n",
      "epoch 18 | loss: 1.52662 | val_0_unsup_loss_numpy: 1.2268199920654297|  0:00:01s\n",
      "epoch 19 | loss: 1.40799 | val_0_unsup_loss_numpy: 1.1001299619674683|  0:00:01s\n",
      "epoch 20 | loss: 1.34607 | val_0_unsup_loss_numpy: 1.1790800094604492|  0:00:01s\n",
      "epoch 21 | loss: 1.28406 | val_0_unsup_loss_numpy: 1.0811400413513184|  0:00:01s\n",
      "epoch 22 | loss: 1.21929 | val_0_unsup_loss_numpy: 1.0599700212478638|  0:00:01s\n",
      "epoch 23 | loss: 1.19323 | val_0_unsup_loss_numpy: 1.0632599592208862|  0:00:01s\n",
      "epoch 24 | loss: 1.14821 | val_0_unsup_loss_numpy: 1.074079990386963|  0:00:01s\n",
      "epoch 25 | loss: 1.14929 | val_0_unsup_loss_numpy: 1.0989799499511719|  0:00:01s\n",
      "epoch 26 | loss: 1.13076 | val_0_unsup_loss_numpy: 1.0621999502182007|  0:00:01s\n",
      "epoch 27 | loss: 1.11749 | val_0_unsup_loss_numpy: 1.0591200590133667|  0:00:02s\n",
      "epoch 28 | loss: 1.10517 | val_0_unsup_loss_numpy: 1.052459955215454|  0:00:02s\n",
      "epoch 29 | loss: 1.08492 | val_0_unsup_loss_numpy: 1.034060001373291|  0:00:02s\n",
      "epoch 30 | loss: 1.07594 | val_0_unsup_loss_numpy: 1.024899959564209|  0:00:02s\n",
      "epoch 31 | loss: 1.07216 | val_0_unsup_loss_numpy: 1.0234400033950806|  0:00:02s\n",
      "epoch 32 | loss: 1.05121 | val_0_unsup_loss_numpy: 1.0217599868774414|  0:00:02s\n",
      "epoch 33 | loss: 1.05656 | val_0_unsup_loss_numpy: 1.0200200080871582|  0:00:02s\n",
      "epoch 34 | loss: 1.04914 | val_0_unsup_loss_numpy: 1.0199400186538696|  0:00:02s\n",
      "epoch 35 | loss: 1.05258 | val_0_unsup_loss_numpy: 1.0284899473190308|  0:00:02s\n",
      "epoch 36 | loss: 1.04163 | val_0_unsup_loss_numpy: 1.0240800380706787|  0:00:02s\n",
      "epoch 37 | loss: 1.03836 | val_0_unsup_loss_numpy: 1.0262199640274048|  0:00:02s\n",
      "epoch 38 | loss: 1.0289  | val_0_unsup_loss_numpy: 1.0183099508285522|  0:00:02s\n",
      "epoch 39 | loss: 1.03554 | val_0_unsup_loss_numpy: 1.0168700218200684|  0:00:02s\n",
      "epoch 40 | loss: 1.02389 | val_0_unsup_loss_numpy: 1.0114799737930298|  0:00:02s\n",
      "epoch 41 | loss: 1.03008 | val_0_unsup_loss_numpy: 1.0150099992752075|  0:00:02s\n",
      "epoch 42 | loss: 1.02249 | val_0_unsup_loss_numpy: 1.0073599815368652|  0:00:02s\n",
      "epoch 43 | loss: 1.02418 | val_0_unsup_loss_numpy: 1.0103399753570557|  0:00:02s\n",
      "epoch 44 | loss: 1.02291 | val_0_unsup_loss_numpy: 1.0090099573135376|  0:00:02s\n",
      "epoch 45 | loss: 1.03012 | val_0_unsup_loss_numpy: 1.0113199949264526|  0:00:03s\n",
      "epoch 46 | loss: 1.01952 | val_0_unsup_loss_numpy: 1.0093300342559814|  0:00:03s\n",
      "epoch 47 | loss: 1.01853 | val_0_unsup_loss_numpy: 1.0108699798583984|  0:00:03s\n",
      "epoch 48 | loss: 1.0201  | val_0_unsup_loss_numpy: 1.0082800388336182|  0:00:03s\n",
      "epoch 49 | loss: 1.01659 | val_0_unsup_loss_numpy: 1.009220004081726|  0:00:03s\n",
      "epoch 50 | loss: 1.01465 | val_0_unsup_loss_numpy: 1.0087800025939941|  0:00:03s\n",
      "epoch 51 | loss: 1.01398 | val_0_unsup_loss_numpy: 1.0091700553894043|  0:00:03s\n",
      "epoch 52 | loss: 1.01017 | val_0_unsup_loss_numpy: 1.0091500282287598|  0:00:03s\n",
      "\n",
      "Early stopping occurred at epoch 52 with best_epoch = 42 and best_val_0_unsup_loss_numpy = 1.0073599815368652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\miniconda3\\envs\\pytorch\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    }
   ],
   "source": [
    "tabnet_params = {\n",
    "    \"n_d\": 16,\n",
    "    \"n_a\": 16,\n",
    "    \"n_steps\": 3,\n",
    "    \"n_shared\": 2,\n",
    "    \"n_independent\": 2,\n",
    "    \"gamma\": 1.3,\n",
    "    \"epsilon\": 1e-15,\n",
    "    \"momentum\": 0.98,\n",
    "    \"mask_type\": \"sparsemax\",\n",
    "    \"lambda_sparse\": 1e-3,\n",
    "    \"device_name\": \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "}\n",
    "\n",
    "\n",
    "unsupervised_model = TabNetPretrainer(\n",
    "    optimizer_fn=torch.optim.Adam,\n",
    "    optimizer_params=dict(lr=2e-2),\n",
    "    **tabnet_params\n",
    ")\n",
    " \n",
    "\n",
    "unsupervised_model.fit(\n",
    "    X_train,\n",
    "    eval_set=[X_test],  \n",
    "    pretraining_ratio=0.8,\n",
    "    max_epochs=101,\n",
    "    patience=10,\n",
    "    batch_size=1024,\n",
    "    virtual_batch_size=128,\n",
    "    num_workers=0,\n",
    "    drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==============================================================================================================\n",
       "Layer (type:depth-idx)                                       Output Shape              Param #\n",
       "==============================================================================================================\n",
       "TabNetPretraining                                            [700, 42]                 --\n",
       "├─EmbeddingGenerator: 1-1                                    [700, 42]                 --\n",
       "├─TabNetEncoder: 1-2                                         [700, 16]                 --\n",
       "│    └─BatchNorm1d: 2-1                                      [700, 42]                 84\n",
       "│    └─FeatTransformer: 2-2                                  [700, 32]                 4,352\n",
       "│    │    └─GLU_Block: 3-1                                   [700, 32]                 4,992\n",
       "│    └─ModuleList: 2-12                                      --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-17                            --                        (recursive)\n",
       "│    └─FeatTransformer: 2-6                                  --                        (recursive)\n",
       "│    │    └─GLU_Block: 3-5                                   --                        (recursive)\n",
       "│    └─ModuleList: 2-12                                      --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-17                            --                        (recursive)\n",
       "│    └─FeatTransformer: 2-6                                  --                        (recursive)\n",
       "│    │    └─GLU_Block: 3-5                                   --                        (recursive)\n",
       "│    │    └─GLU_Block: 3-6                                   [700, 32]                 4,352\n",
       "│    └─ModuleList: 2-11                                      --                        (recursive)\n",
       "│    │    └─AttentiveTransformer: 3-7                        [700, 42]                 756\n",
       "│    └─ModuleList: 2-12                                      --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-8                             [700, 32]                 9,344\n",
       "│    │    └─FeatTransformer: 3-17                            --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-12                            --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-17                            --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-12                            --                        (recursive)\n",
       "│    └─ModuleList: 2-11                                      --                        (recursive)\n",
       "│    │    └─AttentiveTransformer: 3-13                       [700, 42]                 756\n",
       "│    └─ModuleList: 2-12                                      --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-14                            [700, 32]                 9,344\n",
       "│    │    └─FeatTransformer: 3-17                            --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-18                            --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-17                            --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-18                            --                        (recursive)\n",
       "│    └─ModuleList: 2-11                                      --                        (recursive)\n",
       "│    │    └─AttentiveTransformer: 3-19                       [700, 42]                 756\n",
       "│    └─ModuleList: 2-12                                      --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-20                            [700, 32]                 9,344\n",
       "├─TabNetDecoder: 1-3                                         [700, 42]                 --\n",
       "│    └─ModuleList: 2-13                                      --                        --\n",
       "│    │    └─FeatTransformer: 3-21                            [700, 16]                 1,152\n",
       "│    │    └─FeatTransformer: 3-25                            --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-23                            --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-24                            [700, 16]                 1,152\n",
       "│    │    └─FeatTransformer: 3-25                            --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-26                            --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-27                            [700, 16]                 1,152\n",
       "│    └─Linear: 2-14                                          [700, 42]                 672\n",
       "==============================================================================================================\n",
       "Total params: 70,736\n",
       "Trainable params: 70,736\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 30.70\n",
       "==============================================================================================================\n",
       "Input size (MB): 0.12\n",
       "Forward/backward pass size (MB): 15.50\n",
       "Params size (MB): 0.11\n",
       "Estimated Total Size (MB): 15.73\n",
       "=============================================================================================================="
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Truy cập vào mô hình TabNet bên trong\n",
    "from torchinfo import summary\n",
    "\n",
    "tabnet_model = unsupervised_model.network.to(device)\n",
    "\n",
    "summary(tabnet_model, input_size=X_train.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Encoder Summary:\n",
      "TabNetEncoder(\n",
      "  (initial_bn): BatchNorm1d(42, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  (initial_splitter): FeatTransformer(\n",
      "    (shared): GLU_Block(\n",
      "      (shared_layers): ModuleList(\n",
      "        (0): Linear(in_features=42, out_features=64, bias=False)\n",
      "        (1): Linear(in_features=32, out_features=64, bias=False)\n",
      "      )\n",
      "      (glu_layers): ModuleList(\n",
      "        (0): GLU_Layer(\n",
      "          (fc): Linear(in_features=42, out_features=64, bias=False)\n",
      "          (bn): GBN(\n",
      "            (bn): BatchNorm1d(64, eps=1e-05, momentum=0.98, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): GLU_Layer(\n",
      "          (fc): Linear(in_features=32, out_features=64, bias=False)\n",
      "          (bn): GBN(\n",
      "            (bn): BatchNorm1d(64, eps=1e-05, momentum=0.98, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (specifics): GLU_Block(\n",
      "      (glu_layers): ModuleList(\n",
      "        (0-1): 2 x GLU_Layer(\n",
      "          (fc): Linear(in_features=32, out_features=64, bias=False)\n",
      "          (bn): GBN(\n",
      "            (bn): BatchNorm1d(64, eps=1e-05, momentum=0.98, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (feat_transformers): ModuleList(\n",
      "    (0-2): 3 x FeatTransformer(\n",
      "      (shared): GLU_Block(\n",
      "        (shared_layers): ModuleList(\n",
      "          (0): Linear(in_features=42, out_features=64, bias=False)\n",
      "          (1): Linear(in_features=32, out_features=64, bias=False)\n",
      "        )\n",
      "        (glu_layers): ModuleList(\n",
      "          (0): GLU_Layer(\n",
      "            (fc): Linear(in_features=42, out_features=64, bias=False)\n",
      "            (bn): GBN(\n",
      "              (bn): BatchNorm1d(64, eps=1e-05, momentum=0.98, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (1): GLU_Layer(\n",
      "            (fc): Linear(in_features=32, out_features=64, bias=False)\n",
      "            (bn): GBN(\n",
      "              (bn): BatchNorm1d(64, eps=1e-05, momentum=0.98, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (specifics): GLU_Block(\n",
      "        (glu_layers): ModuleList(\n",
      "          (0-1): 2 x GLU_Layer(\n",
      "            (fc): Linear(in_features=32, out_features=64, bias=False)\n",
      "            (bn): GBN(\n",
      "              (bn): BatchNorm1d(64, eps=1e-05, momentum=0.98, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (att_transformers): ModuleList(\n",
      "    (0-2): 3 x AttentiveTransformer(\n",
      "      (fc): Linear(in_features=16, out_features=42, bias=False)\n",
      "      (bn): GBN(\n",
      "        (bn): BatchNorm1d(42, eps=1e-05, momentum=0.98, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (selector): Sparsemax()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "encoder = tabnet_model.encoder\n",
    "\n",
    "# print(\"\\nEncoder Summary:\")\n",
    "# print(encoder)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decoder Summary:\n",
      "TabNetDecoder(\n",
      "  (feat_transformers): ModuleList(\n",
      "    (0-2): 3 x FeatTransformer(\n",
      "      (shared): GLU_Block(\n",
      "        (shared_layers): ModuleList(\n",
      "          (0): Linear(in_features=16, out_features=32, bias=False)\n",
      "        )\n",
      "        (glu_layers): ModuleList(\n",
      "          (0): GLU_Layer(\n",
      "            (fc): Linear(in_features=16, out_features=32, bias=False)\n",
      "            (bn): GBN(\n",
      "              (bn): BatchNorm1d(32, eps=1e-05, momentum=0.98, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (specifics): GLU_Block(\n",
      "        (glu_layers): ModuleList(\n",
      "          (0): GLU_Layer(\n",
      "            (fc): Linear(in_features=16, out_features=32, bias=False)\n",
      "            (bn): GBN(\n",
      "              (bn): BatchNorm1d(32, eps=1e-05, momentum=0.98, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (reconstruction_layer): Linear(in_features=16, out_features=42, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "decoder = tabnet_model.decoder\n",
    "\n",
    "# print(\"\\nDecoder Summary:\")\n",
    "# print(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TabNet encoder trả về 2 giá trị.\n",
      "Đã xảy ra lỗi: 'list' object has no attribute 'shape'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_10484\\3356785120.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sample_input = torch.tensor(X_train[:5]).to(device)\n"
     ]
    }
   ],
   "source": [
    "sample_input = torch.tensor(X_train[:5]).to(device)  \n",
    "\n",
    "try:\n",
    "    result = tabnet_model.encoder(sample_input)\n",
    "    if isinstance(result, tuple):\n",
    "        # print(f'TabNet encoder trả về {len(result)} giá trị.')\n",
    "        for i, res in enumerate(result):\n",
    "            print(f'Giá trị {i + 1} shape: {res.shape}')\n",
    "    else:\n",
    "        print('TabNet encoder chỉ trả về một giá trị.')\n",
    "        # print(f'Giá trị shape: {result.shape}')\n",
    "except Exception as e:\n",
    "    print(f'Đã xảy ra lỗi: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(nn.Module):\n",
    "    def __init__(self, seed=1337):\n",
    "        super(Sampling, self).__init__()\n",
    "        self.seed = seed\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = z_mean.size(0)\n",
    "        dim = z_mean.size(1)\n",
    "        # # print(batch, dim)\n",
    "        epsilon = torch.randn(batch, dim, generator=torch.Generator().manual_seed(self.seed)).to(device)\n",
    "        return z_mean + torch.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE_Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(VAE_Encoder, self).__init__()\n",
    "        self.tabnet_encoder = tabnet_model.encoder\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(16, 128),  \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 96),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(96, latent_dim)\n",
    "        ).to(device)\n",
    "        self.fc_mean = nn.Linear(latent_dim, latent_dim).to(device)\n",
    "        self.fc_log_var = nn.Linear(latent_dim, latent_dim).to(device)\n",
    "        self.sampling = Sampling().to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        steps_output, _ = self.tabnet_encoder(x)\n",
    "        encoded = steps_output[-1]\n",
    "        # # print(\"Shape of encoded tensor:\", encoded.shape)\n",
    "        encoded = self.mlp(encoded)\n",
    "        z_mean = self.fc_mean(encoded)\n",
    "        z_log_var = self.fc_log_var(encoded)\n",
    "        z = self.sampling((z_mean, z_log_var))\n",
    "        # # print(f'Shape of z: {z.shape} - {z_log_var.shape} -{z_log_var.shape}')\n",
    "        return z_mean, z_log_var, z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE_Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim,encoded_dim, output_dim):\n",
    "        super(VAE_Decoder, self).__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 32),   \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 96),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(96, 96),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(96, encoded_dim),  \n",
    "        )\n",
    "        self.tabnet_decoder = tabnet_model.decoder\n",
    "        self.reshape = nn.Unflatten(1, (encoded_dim,))\n",
    "        self.output_dim=output_dim\n",
    "\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = F.relu(self.mlp(z))\n",
    "\n",
    "        # print(\"Shape before reshape:\", x.shape)\n",
    "        # x = self.reshape(x)\n",
    "        x = x[None, ...]\n",
    "\n",
    "        # print(\"Shape after reshape:\", x.shape)\n",
    "        # x = x.view(x.size(0), output_dim)\n",
    "        \n",
    "        output = self.tabnet_decoder(x)\n",
    "        # print(output.shape)\n",
    "        # print(\"Shape of output from tabnet_decoder:\", output.shape)\n",
    "        output = torch.sigmoid(output)\n",
    "        output = output.view(-1, self.output_dim)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data_range(tensor, name):\n",
    "    if not torch.all((tensor >= 0) & (tensor <= 1)):\n",
    "        # print(f\"{name} contains values outside the range [0, 1]\")\n",
    "        print(f\"{name} min: {tensor.min()}, max: {tensor.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE_Tabnet_MLPS(nn.Module):\n",
    "    def __init__(self, encoder, decoder, classifier):\n",
    "        super(VAE_Tabnet_MLPS, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.classifier = classifier\n",
    "        self.total_loss_tracker = []\n",
    "        self.reconstruction_loss_tracker = []\n",
    "        self.kl_loss_tracker = []\n",
    "        self.classification_loss_tracker = []\n",
    "        self.accuracy_tracker = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        z_mean, z_log_var, z = self.encoder(x)\n",
    "        reconstruction = self.decoder(z)\n",
    "        classification_output = self.classifier(z)\n",
    "        return reconstruction, z_mean, z_log_var, classification_output\n",
    "\n",
    "    def train_step(self, data, labels, optimizer):\n",
    "        optimizer.zero_grad()\n",
    "        # z_mean, z_log_var, z = self.encoder(data)\n",
    "        # reconstruction = self.decoder(z)\n",
    "        reconstruction, z_mean, z_log_var, classification_output = self.forward(data)\n",
    "        # # print('classifi',classification_output.shape)\n",
    "        # # print(check_data_range(data, 'data'))\n",
    "        # # print(check_data_range(reconstruction, 'reconstruction'))\n",
    "        # reconstruction_loss = torch.mean(\n",
    "        #     torch.sum(\n",
    "        #         F.binary_cross_entropy(reconstruction, data, reduction='none'),\n",
    "        #         dim=1\n",
    "        #     )\n",
    "        # )\n",
    "        reconstruction_loss = torch.mean(\n",
    "            torch.sum(\n",
    "                F.binary_cross_entropy_with_logits(reconstruction, data, reduction='none'),\n",
    "                dim=1\n",
    "                # dim=(1, 2)\n",
    "                )  \n",
    "        )\n",
    "        classification_loss = torch.mean(\n",
    "            torch.sum(\n",
    "                F.binary_cross_entropy_with_logits(classification_output, labels, reduction='none'),\n",
    "                # dim=1\n",
    "                # dim=(1, 2)\n",
    "                )  \n",
    "        )\n",
    "        kl_loss = -0.5 * torch.sum(1 + z_log_var - z_mean.pow(2) - z_log_var.exp(), dim=1)\n",
    "        kl_loss = torch.mean(torch.sum(kl_loss))\n",
    "        total_loss = reconstruction_loss + kl_loss + classification_loss\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        self.total_loss_tracker.append(total_loss.item())\n",
    "        self.reconstruction_loss_tracker.append(reconstruction_loss.item())\n",
    "        self.kl_loss_tracker.append(kl_loss.item())\n",
    "        self.classification_loss_tracker.append(classification_loss.item())\n",
    "\n",
    "        preds = torch.sigmoid(classification_output)\n",
    "        correct = ((preds > 0.5) == labels).float().sum()\n",
    "        accuracy = correct / labels.size(0)\n",
    "        self.accuracy_tracker.append(accuracy.item())\n",
    "\n",
    "        return {\n",
    "            \"loss\": total_loss.item(),\n",
    "            \"reconstruction_loss\": reconstruction_loss.item(),\n",
    "            \"kl_loss\": kl_loss.item(),\n",
    "            \"classification_loss\": classification_loss.item(),\n",
    "            \"accuracy\": accuracy.item()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 64\n",
    "encoded_dim = 16\n",
    "output_dim = X_train.shape[1]\n",
    "input_dim = X_train.shape[1]\n",
    "# print(input_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(SimpleClassifier, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, output_dim),\n",
    "            nn.Sigmoid()  \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # # print('input: ',x.shape)\n",
    "        output = self.fc(x)\n",
    "        output = output.view(-1)\n",
    "        # # print('output',output.shape)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SimpleClassifier(latent_dim, output_dim=1).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input size: torch.Size([32, 64])\n",
      "Output size: torch.Size([32])\n",
      "Output: tensor([0.4634, 0.4261, 0.4812, 0.4578, 0.4735, 0.4766, 0.4581, 0.4933, 0.4660,\n",
      "        0.4409, 0.4369, 0.4910, 0.4414, 0.4114, 0.4475, 0.4577, 0.4821, 0.4838,\n",
      "        0.4424, 0.4795, 0.4572, 0.4620, 0.4737, 0.4427, 0.4271, 0.4568, 0.4764,\n",
      "        0.4853, 0.4551, 0.4886, 0.4910, 0.4757])\n"
     ]
    }
   ],
   "source": [
    "def check_output(model, input_tensor):\n",
    "    with torch.no_grad():  \n",
    "        output = model(input_tensor)\n",
    "        # print(f\"Input size: {input_tensor.size()}\")\n",
    "        # print(f\"Output size: {output.size()}\")\n",
    "        # print(f\"Output: {output}\")\n",
    "\n",
    "model = SimpleClassifier(latent_dim, output_dim=1)\n",
    "\n",
    "input_tensor = torch.randn(32,latent_dim)  \n",
    "\n",
    "check_output(model, input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Summary:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==============================================================================================================\n",
       "Layer (type:depth-idx)                                       Output Shape              Param #\n",
       "==============================================================================================================\n",
       "VAE_Encoder                                                  [32, 64]                  --\n",
       "├─TabNetEncoder: 1-1                                         [32, 16]                  --\n",
       "│    └─BatchNorm1d: 2-1                                      [32, 42]                  84\n",
       "│    └─FeatTransformer: 2-2                                  [32, 32]                  4,352\n",
       "│    │    └─GLU_Block: 3-1                                   [32, 32]                  4,992\n",
       "│    └─ModuleList: 2-12                                      --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-17                            --                        (recursive)\n",
       "│    └─FeatTransformer: 2-6                                  --                        (recursive)\n",
       "│    │    └─GLU_Block: 3-5                                   --                        (recursive)\n",
       "│    └─ModuleList: 2-12                                      --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-17                            --                        (recursive)\n",
       "│    └─FeatTransformer: 2-6                                  --                        (recursive)\n",
       "│    │    └─GLU_Block: 3-5                                   --                        (recursive)\n",
       "│    │    └─GLU_Block: 3-6                                   [32, 32]                  4,352\n",
       "│    └─ModuleList: 2-11                                      --                        (recursive)\n",
       "│    │    └─AttentiveTransformer: 3-7                        [32, 42]                  756\n",
       "│    └─ModuleList: 2-12                                      --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-8                             [32, 32]                  9,344\n",
       "│    │    └─FeatTransformer: 3-17                            --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-12                            --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-17                            --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-12                            --                        (recursive)\n",
       "│    └─ModuleList: 2-11                                      --                        (recursive)\n",
       "│    │    └─AttentiveTransformer: 3-13                       [32, 42]                  756\n",
       "│    └─ModuleList: 2-12                                      --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-14                            [32, 32]                  9,344\n",
       "│    │    └─FeatTransformer: 3-17                            --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-18                            --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-17                            --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-18                            --                        (recursive)\n",
       "│    └─ModuleList: 2-11                                      --                        (recursive)\n",
       "│    │    └─AttentiveTransformer: 3-19                       [32, 42]                  756\n",
       "│    └─ModuleList: 2-12                                      --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-20                            [32, 32]                  9,344\n",
       "├─Sequential: 1-2                                            [32, 64]                  --\n",
       "│    └─Linear: 2-13                                          [32, 128]                 2,176\n",
       "│    └─ReLU: 2-14                                            [32, 128]                 --\n",
       "│    └─Linear: 2-15                                          [32, 128]                 16,512\n",
       "│    └─ReLU: 2-16                                            [32, 128]                 --\n",
       "│    └─Linear: 2-17                                          [32, 96]                  12,384\n",
       "│    └─ReLU: 2-18                                            [32, 96]                  --\n",
       "│    └─Linear: 2-19                                          [32, 64]                  6,208\n",
       "├─Linear: 1-3                                                [32, 64]                  4,160\n",
       "├─Linear: 1-4                                                [32, 64]                  4,160\n",
       "├─Sampling: 1-5                                              [32, 64]                  --\n",
       "==============================================================================================================\n",
       "Total params: 110,416\n",
       "Trainable params: 110,416\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 2.73\n",
       "==============================================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 0.74\n",
       "Params size (MB): 0.28\n",
       "Estimated Total Size (MB): 1.03\n",
       "=============================================================================================================="
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae_encoder = VAE_Encoder(latent_dim=latent_dim)\n",
    "# print(\"Encoder Summary:\")\n",
    "# vae_encoder.to(device)\n",
    "\n",
    "summary(vae_encoder, input_size=(32, input_dim), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded shape: torch.Size([800, 16])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(800, 42).to(device)\n",
    "steps_output, _ = tabnet_model.encoder(x)\n",
    "encoded = steps_output[-1]\n",
    "# print(f\"Encoded shape: {encoded.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of encoder output: [torch.Size([800, 16]), torch.Size([800, 16]), torch.Size([800, 16])]\n",
      "Decoder shape: torch.Size([800, 42])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.randn(800, 42).to(device)  # Đầu vào có kích thước (batch_size, features)\n",
    "\n",
    "steps_output, _ = tabnet_model.encoder(x)\n",
    "# print(\"Shape of encoder output:\", [output.shape for output in steps_output])\n",
    "\n",
    "decoder_input = steps_output[-1]  \n",
    "decoder_input = decoder_input[None, ...]\n",
    "try:\n",
    "    decoder_output = tabnet_model.decoder(decoder_input)\n",
    "    # print(f\"Decoder shape: {decoder_output.shape}\")\n",
    "except ValueError as e:\n",
    "    # print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder Summary:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==============================================================================================================\n",
       "Layer (type:depth-idx)                                       Output Shape              Param #\n",
       "==============================================================================================================\n",
       "VAE_Decoder                                                  [32, 42]                  --\n",
       "├─Sequential: 1-1                                            [32, 16]                  --\n",
       "│    └─Linear: 2-1                                           [32, 32]                  2,080\n",
       "│    └─ReLU: 2-2                                             [32, 32]                  --\n",
       "│    └─Linear: 2-3                                           [32, 96]                  3,168\n",
       "│    └─ReLU: 2-4                                             [32, 96]                  --\n",
       "│    └─Linear: 2-5                                           [32, 96]                  9,312\n",
       "│    └─ReLU: 2-6                                             [32, 96]                  --\n",
       "│    └─Linear: 2-7                                           [32, 16]                  1,552\n",
       "├─TabNetDecoder: 1-2                                         [32, 42]                  --\n",
       "│    └─ModuleList: 2-8                                       --                        --\n",
       "│    │    └─FeatTransformer: 3-1                             [32, 16]                  1,152\n",
       "│    │    └─FeatTransformer: 3-2                             --                        1,152\n",
       "│    │    └─FeatTransformer: 3-3                             --                        (recursive)\n",
       "│    └─Linear: 2-9                                           [32, 42]                  672\n",
       "==============================================================================================================\n",
       "Total params: 19,728\n",
       "Trainable params: 19,728\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.57\n",
       "==============================================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 0.10\n",
       "Params size (MB): 0.07\n",
       "Estimated Total Size (MB): 0.18\n",
       "=============================================================================================================="
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae_decoder = VAE_Decoder(latent_dim=latent_dim, encoded_dim=encoded_dim, output_dim=output_dim).to(device)\n",
    "# print(\"Decoder Summary:\")\n",
    "summary(vae_decoder, input_size=(32, latent_dim), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===================================================================================================================\n",
       "Layer (type:depth-idx)                                            Output Shape              Param #\n",
       "===================================================================================================================\n",
       "VAE_Tabnet_MLPS                                                   [32, 42]                  --\n",
       "├─VAE_Encoder: 1-1                                                [32, 64]                  --\n",
       "│    └─TabNetEncoder: 2-1                                         [32, 16]                  --\n",
       "│    │    └─BatchNorm1d: 3-1                                      [32, 42]                  84\n",
       "│    │    └─FeatTransformer: 3-2                                  [32, 32]                  9,344\n",
       "│    │    └─ModuleList: 3-12                                      --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-6                                  --                        (recursive)\n",
       "│    │    └─ModuleList: 3-12                                      --                        (recursive)\n",
       "│    │    └─FeatTransformer: 3-6                                  --                        (recursive)\n",
       "│    │    └─ModuleList: 3-11                                      --                        (recursive)\n",
       "│    │    └─ModuleList: 3-12                                      --                        (recursive)\n",
       "│    │    └─ModuleList: 3-11                                      --                        (recursive)\n",
       "│    │    └─ModuleList: 3-12                                      --                        (recursive)\n",
       "│    │    └─ModuleList: 3-11                                      --                        (recursive)\n",
       "│    │    └─ModuleList: 3-12                                      --                        (recursive)\n",
       "│    └─Sequential: 2-2                                            [32, 64]                  --\n",
       "│    │    └─Linear: 3-13                                          [32, 128]                 2,176\n",
       "│    │    └─ReLU: 3-14                                            [32, 128]                 --\n",
       "│    │    └─Linear: 3-15                                          [32, 128]                 16,512\n",
       "│    │    └─ReLU: 3-16                                            [32, 128]                 --\n",
       "│    │    └─Linear: 3-17                                          [32, 96]                  12,384\n",
       "│    │    └─ReLU: 3-18                                            [32, 96]                  --\n",
       "│    │    └─Linear: 3-19                                          [32, 64]                  6,208\n",
       "│    └─Linear: 2-3                                                [32, 64]                  4,160\n",
       "│    └─Linear: 2-4                                                [32, 64]                  4,160\n",
       "│    └─Sampling: 2-5                                              [32, 64]                  --\n",
       "├─VAE_Decoder: 1-2                                                [32, 42]                  --\n",
       "│    └─Sequential: 2-6                                            [32, 16]                  --\n",
       "│    │    └─Linear: 3-20                                          [32, 32]                  2,080\n",
       "│    │    └─ReLU: 3-21                                            [32, 32]                  --\n",
       "│    │    └─Linear: 3-22                                          [32, 96]                  3,168\n",
       "│    │    └─ReLU: 3-23                                            [32, 96]                  --\n",
       "│    │    └─Linear: 3-24                                          [32, 96]                  9,312\n",
       "│    │    └─ReLU: 3-25                                            [32, 96]                  --\n",
       "│    │    └─Linear: 3-26                                          [32, 16]                  1,552\n",
       "│    └─TabNetDecoder: 2-7                                         [32, 42]                  --\n",
       "│    │    └─ModuleList: 3-27                                      --                        2,432\n",
       "│    │    └─Linear: 3-28                                          [32, 42]                  672\n",
       "├─SimpleClassifier: 1-3                                           [32]                      --\n",
       "│    └─Sequential: 2-8                                            [32, 1]                   --\n",
       "│    │    └─Linear: 3-29                                          [32, 64]                  4,160\n",
       "│    │    └─ReLU: 3-30                                            [32, 64]                  --\n",
       "│    │    └─Linear: 3-31                                          [32, 32]                  2,080\n",
       "│    │    └─ReLU: 3-32                                            [32, 32]                  --\n",
       "│    │    └─Linear: 3-33                                          [32, 1]                   33\n",
       "│    │    └─Sigmoid: 3-34                                         [32, 1]                   --\n",
       "===================================================================================================================\n",
       "Total params: 136,417\n",
       "Trainable params: 136,417\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 3.51\n",
       "===================================================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 0.87\n",
       "Params size (MB): 0.38\n",
       "Estimated Total Size (MB): 1.26\n",
       "==================================================================================================================="
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae = VAE_Tabnet_MLPS(encoder=vae_encoder, decoder=vae_decoder,classifier=classifier).to(device)\n",
    "summary(vae, input_size=(32, input_dim), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 61.4475, Reconstruction Loss: 30.6784, KL Loss: 7.7328, Classification Loss: 23.0363, Accuracy: 0.5002\n",
      "Epoch 2/10, Loss: 58.9608, Reconstruction Loss: 30.6223, KL Loss: 5.3508, Classification Loss: 22.9877, Accuracy: 0.5006\n",
      "Epoch 3/10, Loss: 57.0393, Reconstruction Loss: 30.5961, KL Loss: 3.4634, Classification Loss: 22.9797, Accuracy: 0.4996\n",
      "Epoch 4/10, Loss: 55.3225, Reconstruction Loss: 30.5742, KL Loss: 1.8678, Classification Loss: 22.8805, Accuracy: 0.5008\n",
      "Epoch 5/10, Loss: 54.2315, Reconstruction Loss: 30.5489, KL Loss: 0.8267, Classification Loss: 22.8559, Accuracy: 0.4992\n",
      "Epoch 6/10, Loss: 53.6408, Reconstruction Loss: 30.5359, KL Loss: 0.3066, Classification Loss: 22.7983, Accuracy: 0.5006\n",
      "Epoch 7/10, Loss: 53.4424, Reconstruction Loss: 30.5177, KL Loss: 0.1460, Classification Loss: 22.7787, Accuracy: 0.5004\n",
      "Epoch 8/10, Loss: 53.3401, Reconstruction Loss: 30.5035, KL Loss: 0.0918, Classification Loss: 22.7447, Accuracy: 0.5000\n",
      "Epoch 9/10, Loss: 53.2159, Reconstruction Loss: 30.4835, KL Loss: 0.0597, Classification Loss: 22.6727, Accuracy: 0.4998\n",
      "Epoch 10/10, Loss: 53.1513, Reconstruction Loss: 30.4765, KL Loss: 0.0455, Classification Loss: 22.6293, Accuracy: 0.4994\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.0001\n",
    "optimizer = optim.Adam(vae.parameters(), lr=learning_rate)\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    vae.train()\n",
    "    train_loss = 0\n",
    "    rec_loss = 0\n",
    "    kl_loss = 0\n",
    "    classification_loss = 0\n",
    "    accuracy = 0\n",
    "\n",
    "    for batch_data, batch_labels in train_loader:\n",
    "        batch_data = batch_data.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "        results = vae.train_step(batch_data, batch_labels, optimizer)\n",
    "        \n",
    "        train_loss += results[\"loss\"]\n",
    "        rec_loss += results[\"reconstruction_loss\"]\n",
    "        kl_loss += results[\"kl_loss\"]\n",
    "        classification_loss += results[\"classification_loss\"]\n",
    "        accuracy += results[\"accuracy\"]\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    rec_loss /= len(train_loader)\n",
    "    kl_loss /= len(train_loader)\n",
    "    classification_loss /= len(train_loader)\n",
    "    accuracy /= len(train_loader)\n",
    "\n",
    "    # print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {train_loss:.4f}, Reconstruction Loss: {rec_loss:.4f}, KL Loss: {kl_loss:.4f}, Classification Loss: {classification_loss:.4f}, Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in vae.encoder.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 53.1120, Reconstruction Loss: 30.4644, KL Loss: 0.0400, Classification Loss: 22.6076, Accuracy: 0.4992\n",
      "Epoch 2/10, Loss: 53.0209, Reconstruction Loss: 30.4537, KL Loss: 0.0387, Classification Loss: 22.5284, Accuracy: 0.5002\n",
      "Epoch 3/10, Loss: 52.9978, Reconstruction Loss: 30.4485, KL Loss: 0.0430, Classification Loss: 22.5062, Accuracy: 0.4990\n",
      "Epoch 4/10, Loss: 52.9031, Reconstruction Loss: 30.4347, KL Loss: 0.0404, Classification Loss: 22.4280, Accuracy: 0.4998\n",
      "Epoch 5/10, Loss: 52.9155, Reconstruction Loss: 30.4229, KL Loss: 0.0395, Classification Loss: 22.4531, Accuracy: 0.4998\n",
      "Epoch 6/10, Loss: 52.8807, Reconstruction Loss: 30.4122, KL Loss: 0.0409, Classification Loss: 22.4276, Accuracy: 0.5000\n",
      "Epoch 7/10, Loss: 52.8405, Reconstruction Loss: 30.4025, KL Loss: 0.0428, Classification Loss: 22.3952, Accuracy: 0.5002\n",
      "Epoch 8/10, Loss: 52.8140, Reconstruction Loss: 30.3933, KL Loss: 0.0420, Classification Loss: 22.3787, Accuracy: 0.5000\n",
      "Epoch 9/10, Loss: 52.7752, Reconstruction Loss: 30.3814, KL Loss: 0.0417, Classification Loss: 22.3520, Accuracy: 0.5006\n",
      "Epoch 10/10, Loss: 52.7125, Reconstruction Loss: 30.3661, KL Loss: 0.0423, Classification Loss: 22.3041, Accuracy: 0.4992\n"
     ]
    }
   ],
   "source": [
    "vae_new = VAE_Tabnet_MLPS(vae.encoder, vae.decoder, vae.classifier).to(device)\n",
    "for param in vae_new.encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, vae_new.parameters()), lr=learning_rate)\n",
    "for epoch in range(num_epochs):\n",
    "    vae_new.train()\n",
    "    train_loss = 0\n",
    "    rec_loss = 0\n",
    "    kl_loss = 0\n",
    "    classification_loss = 0\n",
    "    accuracy = 0\n",
    "\n",
    "    for batch_data, batch_labels in train_loader:\n",
    "        batch_data = batch_data.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "        results = vae.train_step(batch_data, batch_labels, optimizer)\n",
    "        \n",
    "        train_loss += results[\"loss\"]\n",
    "        rec_loss += results[\"reconstruction_loss\"]\n",
    "        kl_loss += results[\"kl_loss\"]\n",
    "        classification_loss += results[\"classification_loss\"]\n",
    "        accuracy += results[\"accuracy\"]\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    rec_loss /= len(train_loader)\n",
    "    kl_loss /= len(train_loader)\n",
    "    classification_loss /= len(train_loader)\n",
    "    accuracy /= len(train_loader)\n",
    "\n",
    "    # print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {train_loss:.4f}, Reconstruction Loss: {rec_loss:.4f}, KL Loss: {kl_loss:.4f}, Classification Loss: {classification_loss:.4f}, Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conbr_block(nn.Module):\n",
    "    def __init__(self, in_layer, out_layer, kernel_size, stride, dilation):\n",
    "        super(conbr_block, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_layer, out_layer, kernel_size=kernel_size, stride=stride, dilation = dilation, padding = 3, bias=True)\n",
    "        self.bn = nn.BatchNorm1d(out_layer)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn(x)\n",
    "        out = self.relu(x)\n",
    "        \n",
    "        return out       \n",
    "\n",
    "class se_block(nn.Module):\n",
    "    def __init__(self,in_layer, out_layer):\n",
    "        super(se_block, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(in_layer, out_layer//8, kernel_size=1, padding=0)\n",
    "        self.conv2 = nn.Conv1d(out_layer//8, in_layer, kernel_size=1, padding=0)\n",
    "        self.fc = nn.Linear(1,out_layer//8)\n",
    "        self.fc2 = nn.Linear(out_layer//8,out_layer)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self,x):\n",
    "\n",
    "        x_se = nn.functional.adaptive_avg_pool1d(x,1)\n",
    "        x_se = self.conv1(x_se)\n",
    "        x_se = self.relu(x_se)\n",
    "        x_se = self.conv2(x_se)\n",
    "        x_se = self.sigmoid(x_se)\n",
    "        \n",
    "        x_out = torch.add(x, x_se)\n",
    "        return x_out\n",
    "\n",
    "class re_block(nn.Module):\n",
    "    def __init__(self, in_layer, out_layer, kernel_size, dilation):\n",
    "        super(re_block, self).__init__()\n",
    "        \n",
    "        self.cbr1 = conbr_block(in_layer,out_layer, kernel_size, 1, dilation)\n",
    "        self.cbr2 = conbr_block(out_layer,out_layer, kernel_size, 1, dilation)\n",
    "        self.seblock = se_block(out_layer, out_layer)\n",
    "    \n",
    "    def forward(self,x):\n",
    "\n",
    "        x_re = self.cbr1(x)\n",
    "        x_re = self.cbr2(x_re)\n",
    "        x_re = self.seblock(x_re)\n",
    "        x_out = torch.add(x, x_re)\n",
    "        return x_out          \n",
    "\n",
    "class UNET_1D(nn.Module):\n",
    "    def __init__(self, input_dim, layer_n, kernel_size, depth):\n",
    "        super(UNET_1D, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.layer_n = layer_n\n",
    "        self.kernel_size = kernel_size\n",
    "        self.depth = depth\n",
    "\n",
    "        # AvgPool layers\n",
    "        self.AvgPool1D1 = nn.AvgPool1d(input_dim, stride=5)\n",
    "        self.AvgPool1D2 = nn.AvgPool1d(input_dim, stride=25)\n",
    "        self.AvgPool1D3 = nn.AvgPool1d(input_dim, stride=125)\n",
    "\n",
    "        # Encoder layers\n",
    "        self.layer1 = self.down_layer(self.input_dim, self.layer_n, self.kernel_size, 1, 2)\n",
    "        self.layer2 = self.down_layer(self.layer_n, int(self.layer_n*2), self.kernel_size, 5, 2)\n",
    "        self.layer3 = self.down_layer(int(self.layer_n*2)+int(self.input_dim), int(self.layer_n*3), self.kernel_size, 5, 2)\n",
    "        self.layer4 = self.down_layer(int(self.layer_n*3)+int(self.input_dim), int(self.layer_n*4), self.kernel_size, 5, 2)\n",
    "        self.layer5 = self.down_layer(int(self.layer_n*4)+int(self.input_dim), int(self.layer_n*5), self.kernel_size, 4, 2)\n",
    "\n",
    "        # Decoder layers\n",
    "        self.cbr_up1 = conbr_block(int(self.layer_n*7), int(self.layer_n*3), self.kernel_size, 1, 1)\n",
    "        self.cbr_up2 = conbr_block(int(self.layer_n*5), int(self.layer_n*2), self.kernel_size, 1, 1)\n",
    "        self.cbr_up3 = conbr_block(int(self.layer_n*3), self.layer_n, self.kernel_size, 1, 1)\n",
    "\n",
    "        self.upsample = nn.Upsample(scale_factor=5, mode='nearest')\n",
    "        self.upsample1 = nn.Upsample(scale_factor=5, mode='nearest')\n",
    "\n",
    "        self.outcov = nn.Conv1d(self.layer_n, 11, kernel_size=self.kernel_size, stride=1, padding=3)\n",
    "\n",
    "    def down_layer(self, input_layer, out_layer, kernel, stride, depth):\n",
    "        layers = []\n",
    "        # Layer đầu tiên\n",
    "        layers.append(conbr_block(input_layer, out_layer, kernel, stride, 1))\n",
    "        # Các re_block tiếp theo\n",
    "        for _ in range(depth):\n",
    "            layers.append(re_block(out_layer, out_layer, kernel, 1))\n",
    "        return nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, x, t_emb):\n",
    "        pool_x1 = self.AvgPool1D1(x)\n",
    "        pool_x2 = self.AvgPool1D2(x)\n",
    "        pool_x3 = self.AvgPool1D3(x)\n",
    "\n",
    "        ############# Encoder #####################\n",
    "        # layer1\n",
    "        x_enc = x\n",
    "        for layer in self.layer1:\n",
    "            if isinstance(layer, conbr_block):\n",
    "                x_enc = layer(x_enc, t_emb)\n",
    "            else:\n",
    "                x_enc = layer(x_enc)  # nếu có lớp khác không cần t_emb\n",
    "\n",
    "        out_0 = x_enc\n",
    "\n",
    "        # layer2\n",
    "        x_enc = out_0\n",
    "        for layer in self.layer2:\n",
    "            if isinstance(layer, conbr_block):\n",
    "                x_enc = layer(x_enc, t_emb)\n",
    "            else:\n",
    "                x_enc = layer(x_enc)\n",
    "\n",
    "        out_1 = x_enc\n",
    "\n",
    "        # layer3\n",
    "        x = torch.cat([out_1, pool_x1], dim=1)\n",
    "        x_enc = x\n",
    "        for layer in self.layer3:\n",
    "            if isinstance(layer, conbr_block):\n",
    "                x_enc = layer(x_enc, t_emb)\n",
    "            else:\n",
    "                x_enc = layer(x_enc)\n",
    "\n",
    "        out_2 = x_enc\n",
    "\n",
    "        # layer4\n",
    "        x = torch.cat([out_2, pool_x2], dim=1)\n",
    "        x_enc = x\n",
    "        for layer in self.layer4:\n",
    "            if isinstance(layer, conbr_block):\n",
    "                x_enc = layer(x_enc, t_emb)\n",
    "            else:\n",
    "                x_enc = layer(x_enc)\n",
    "\n",
    "        x = x_enc\n",
    "\n",
    "        ############# Decoder ####################\n",
    "        up = self.upsample1(x)\n",
    "        up = torch.cat([up, out_2], dim=1)\n",
    "        up = self.cbr_up1(up, t_emb)\n",
    "\n",
    "        up = self.upsample(up)\n",
    "        up = torch.cat([up, out_1], dim=1)\n",
    "        up = self.cbr_up2(up, t_emb)\n",
    "\n",
    "        up = self.upsample(up)\n",
    "        up = torch.cat([up, out_0], dim=1)\n",
    "        up = self.cbr_up3(up, t_emb)\n",
    "\n",
    "        out = self.outcov(up)\n",
    "        noise_pred = out.squeeze(1)  # shape: [B, latent_dim]\n",
    "        return noise_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "class conbr_block(nn.Module):\n",
    "    def __init__(self, in_layer, out_layer, kernel_size, stride, dilation):\n",
    "        super(conbr_block, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_layer, out_layer, kernel_size=kernel_size,\n",
    "                               stride=stride, dilation=dilation, padding=3, bias=True)\n",
    "        self.bn = nn.BatchNorm1d(out_layer)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # Time embedding projection\n",
    "        self.time_mlp = nn.Linear(128, out_layer)\n",
    "\n",
    "    def forward(self, x, t_emb):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn(x)\n",
    "        t_emb = self.time_mlp(t_emb)\n",
    "        # print('conbr_block t_emb shape:', t_emb.shape)\n",
    "        # # print(t_emb.shape, x.shape)  # Check shapes before broadcasting\n",
    "        t_emb_proj = t_emb.unsqueeze(-1) \n",
    "        # print('t_emb_proj shape:', t_emb_proj.shape)\n",
    "        \n",
    "        # print(t_emb_proj.shape, x.shape)  # Project time embedding to match out_layer\n",
    "        x = x + t_emb_proj  # broadcasting\n",
    "        out = self.relu(x)\n",
    "        # print('conbr_block out shape:', out.shape)\n",
    "        return out\n",
    "\n",
    "\n",
    "class re_block(nn.Module):\n",
    "    def __init__(self, in_layer, out_layer, kernel_size, dilation):\n",
    "        super(re_block, self).__init__()\n",
    "        self.cbr1 = conbr_block(in_layer, out_layer, kernel_size, 1, dilation)\n",
    "        self.cbr2 = conbr_block(out_layer, out_layer, kernel_size, 1, dilation)\n",
    "        self.seblock = se_block(out_layer, out_layer)\n",
    "\n",
    "    def forward(self, x, t_emb):\n",
    "        x_re = self.cbr1(x, t_emb)\n",
    "        x_re = self.cbr2(x_re, t_emb)\n",
    "        x_re = self.seblock(x_re)\n",
    "\n",
    "        # Resize x_re nếu temporal dim không khớp\n",
    "        if x_re.shape[-1] != x.shape[-1]:\n",
    "            x_re = F.interpolate(x_re, size=x.shape[-1], mode='nearest')\n",
    "\n",
    "        x_out = torch.add(x, x_re)\n",
    "        return x_out\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=10000):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, time):\n",
    "        device = time.device\n",
    "        half_d = self.d_model // 2\n",
    "        embeddings = math.log(10000) / (half_d - 1)\n",
    "        embeddings = torch.exp(torch.arange(half_d, device=device) * -embeddings)\n",
    "        embeddings = time[:, None] * embeddings[None, :]\n",
    "        embeddings = torch.cat([torch.sin(embeddings), torch.cos(embeddings)], dim=-1)\n",
    "        return embeddings \n",
    "\n",
    "class UNET_1D_Diffusion(nn.Module):\n",
    "    def __init__(self, latent_dim, time_emb_dim=128):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        # Time embedding\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            PositionalEncoding(time_emb_dim),\n",
    "            nn.Linear(time_emb_dim, time_emb_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(time_emb_dim, time_emb_dim)\n",
    "        )\n",
    "\n",
    "        # Encoder\n",
    "        self.enc1_cbr = conbr_block(1, 64, kernel_size=5, stride=1, dilation=1)\n",
    "        self.enc1_re = re_block(64, 64, kernel_size=5, dilation=1)\n",
    "\n",
    "        self.enc2_cbr = conbr_block(64, 128, kernel_size=5, stride=5, dilation=1)\n",
    "        self.enc2_re = re_block(128, 128, kernel_size=5, dilation=1)\n",
    "\n",
    "        self.enc3_cbr = conbr_block(128, 192, kernel_size=5, stride=5, dilation=1)\n",
    "        self.enc3_re = re_block(192, 192, kernel_size=5, dilation=1)\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck_cbr = conbr_block(192, 256, kernel_size=3, stride=4, dilation=1)\n",
    "        self.bottleneck_re = re_block(256, 256, kernel_size=3, dilation=1)\n",
    "\n",
    "        # Decoder\n",
    "        self.dec3_up = nn.Upsample(scale_factor=5, mode='nearest')\n",
    "        self.dec3_cbr = conbr_block(256 + 192, 192, kernel_size=3, stride=1, dilation=1)\n",
    "        self.dec3_re = re_block(192, 192, kernel_size=3, dilation=1)\n",
    "\n",
    "        self.dec2_up = nn.Upsample(scale_factor=5, mode='nearest')\n",
    "        self.dec2_cbr = conbr_block(192 + 128, 128, kernel_size=5, stride=1, dilation=1)\n",
    "        self.dec2_re = re_block(128, 128, kernel_size=5, dilation=1)\n",
    "\n",
    "        self.dec1_up = nn.Upsample(scale_factor=5, mode='nearest')\n",
    "        self.dec1_cbr = conbr_block(128 + 64, 64, kernel_size=5, stride=1, dilation=1)\n",
    "        self.dec1_re = re_block(64, 64, kernel_size=5, dilation=1)\n",
    "\n",
    "        self.final_conv = nn.Conv1d(64, 1, kernel_size=1)\n",
    "\n",
    "        self.final_linear = nn.Linear(362, latent_dim) \n",
    "\n",
    "        # Beta schedule\n",
    "        beta = torch.linspace(0.0001, 0.02, 1000)\n",
    "        alpha = 1. - beta\n",
    "        alpha_bar = torch.cumprod(alpha, dim=0)\n",
    "        self.register_buffer('beta', beta)\n",
    "        self.register_buffer('alpha', alpha)\n",
    "        self.register_buffer('alpha_bar', alpha_bar)\n",
    "\n",
    "    def forward(self, z_t, t):\n",
    "        # Embedding timestep\n",
    "        t_emb = self.time_mlp(t)\n",
    "        # print('t_emb shape diffusion:', t_emb.shape)\n",
    "        # print('z_t shape:', z_t.shape)\n",
    "\n",
    "        # Thêm chiều channel\n",
    "        x = z_t.unsqueeze(1)  # shape: [B, 1, latent_dim]\n",
    "        # print('x shape:', x.shape)\n",
    "\n",
    "        # Encoder\n",
    "        e1 = self.enc1_cbr(x, t_emb)\n",
    "        e1 = self.enc1_re(e1, t_emb)\n",
    "        # # print('e1 shape:', e1.shape)\n",
    "        # print('e1 diffusion shape:', e1.shape)\n",
    "\n",
    "        e2 = self.enc2_cbr(e1, t_emb)\n",
    "        # print(\"lỗi 1\")\n",
    "        e2 = self.enc2_re(e2, t_emb)\n",
    "        # print(\"lỗi 2\")\n",
    "        # print('e2 diffusion shape:', e2.shape)\n",
    "        # print('t_emv', t_emb.shape)\n",
    "\n",
    "        e3 = self.enc3_cbr(e2, t_emb)\n",
    "        # print('lỗi 3')\n",
    "        e3 = self.enc3_re(e3, t_emb)\n",
    "        # print('e3 diffusion shape:', e3.shape)\n",
    "        # print('lỗi 4')\n",
    "        b = self.bottleneck_cbr(e3, t_emb)\n",
    "        # print('bottleneck shape:', b.shape)\n",
    "        # print('lỗi 5')\n",
    "        b = self.bottleneck_re(b, t_emb)\n",
    "\n",
    "        # Decoder\n",
    "        up3 = self.dec3_up(b)\n",
    "        # print('up3 shape:', up3.shape)\n",
    "        # print('e3 shape:', e3.shape)\n",
    "        if e3.shape[-1] != up3.shape[-1]:\n",
    "            e3 = F.interpolate(e3, size=up3.shape[-1], mode='nearest')\n",
    "        up3 = torch.cat([up3, e3], dim=1)\n",
    "        d3 = self.dec3_cbr(up3, t_emb)\n",
    "        d3 = self.dec3_re(d3, t_emb)\n",
    "\n",
    "        up2 = self.dec2_up(d3)\n",
    "        # print('up2 shape:', up2.shape)\n",
    "        # print('e2 shape:', e2.shape)\n",
    "        if e2.shape[-1] != up2.shape[-1]:\n",
    "            e2 = F.interpolate(e2, size=up2.shape[-1], mode='nearest')\n",
    "        up2 = torch.cat([up2, e2], dim=1)\n",
    "        d2 = self.dec2_cbr(up2, t_emb)\n",
    "        d2 = self.dec2_re(d2, t_emb)\n",
    "\n",
    "        up1 = self.dec1_up(d2)\n",
    "\n",
    "        # print('up1 shape:', up1.shape)\n",
    "        # print('e1 shape:', e1.shape)\n",
    "        if e1.shape[-1] != up1.shape[-1]:\n",
    "            e1 = F.interpolate(e1, size=up1.shape[-1], mode='nearest')\n",
    "        up1 = torch.cat([up1, e1], dim=1)\n",
    "        d1 = self.dec1_cbr(up1, t_emb)\n",
    "        d1 = self.dec1_re(d1, t_emb)\n",
    "\n",
    "        out = self.final_conv(d1).squeeze(1)  # shape: [B, latent_dim]\n",
    "        out = self.final_linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleDiffusionModel(nn.Module):\n",
    "    def __init__(self, latent_dim, time_steps=1000):\n",
    "        super().__init__()\n",
    "        self.time_steps = time_steps\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Tạo các beta_schedule tuyến tính\n",
    "        beta = torch.linspace(0.0001, 0.02, time_steps)\n",
    "        alpha = 1. - beta\n",
    "        alpha_bar = torch.cumprod(alpha, dim=0)\n",
    "\n",
    "        self.register_buffer('beta', beta)\n",
    "        self.register_buffer('alpha', alpha)\n",
    "        self.register_buffer('alpha_bar', alpha_bar)\n",
    "\n",
    "        # Mạng neural đơn giản để dự đoán nhiễu\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim + 1, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, latent_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, z, t):\n",
    "        noise = torch.randn_like(z)\n",
    "        \n",
    "        # Đảm bảo t là long type và shape phù hợp\n",
    "        if isinstance(t, torch.Tensor):\n",
    "            t = t.to(dtype=torch.long)\n",
    "        else:\n",
    "            t = torch.tensor([t], device=z.device, dtype=torch.long).expand(z.shape[0])\n",
    "\n",
    "        sqrt_alpha_bar = torch.sqrt(self.alpha_bar[t])[:, None]\n",
    "        sqrt_one_minus_alpha_bar = torch.sqrt(1 - self.alpha_bar[t])[:, None]\n",
    "        noisy_z = sqrt_alpha_bar * z + sqrt_one_minus_alpha_bar * noise\n",
    "\n",
    "        predicted_noise = self.model(torch.cat([noisy_z, t.unsqueeze(1)], dim=1))\n",
    "        loss = F.mse_loss(predicted_noise, noise)\n",
    "        return loss\n",
    "\n",
    "    def sample(self, num_samples):\n",
    "        z = torch.randn(num_samples, self.latent_dim).to(next(self.parameters()).device)\n",
    "        for i in reversed(range(self.time_steps)):\n",
    "            t = torch.full((num_samples,), i, device=z.device, dtype=torch.long)\n",
    "            z = self.denoise_step(z, t)\n",
    "        return z\n",
    "    \n",
    "    def denoise_step(self, z, t):\n",
    "        timestep = t.item() if isinstance(t, torch.Tensor) else t\n",
    "        t_batch = torch.full((z.shape[0],), timestep, device=z.device, dtype=torch.long)\n",
    "\n",
    "        predicted_noise = self.model(torch.cat([z, t_batch.unsqueeze(1)], dim=1))\n",
    "\n",
    "        alpha = self.alpha[timestep]\n",
    "        alpha_bar = self.alpha_bar[timestep]\n",
    "        beta = self.beta[timestep]\n",
    "\n",
    "        z = (1 / torch.sqrt(alpha)) * (z - ((1 - alpha) / torch.sqrt(1 - alpha_bar)) * predicted_noise)\n",
    "        if timestep > 0:\n",
    "            noise = torch.randn_like(z)\n",
    "            z += torch.sqrt(beta) * noise\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thay đoạn lỗi này:\n",
    "# latent_dim = vae.encoder[-1].out_features\n",
    "\n",
    "# Bằng đoạn này:\n",
    "with torch.no_grad():\n",
    "    vae_new.encoder.eval()\n",
    "    dummy_input = torch.randn(1, input_dim).to(device)  # Thay input_dim theo đúng dữ liệu của bạn\n",
    "    z_mean, z_log_var, _ = vae_new.encoder(dummy_input)\n",
    "    latent_dim = z_mean.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# latent_dim = vae.encoder[-1].out_features  # Kích thước latent z\n",
    "diffusion_model = SimpleDiffusionModel(latent_dim=latent_dim).to(device)\n",
    "diffusion_optimizer = torch.optim.Adam(diffusion_model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def train_diffusion(vae, diffusion_model, dataloader, optimizer, device, time_steps=1000, epochs=20):\n",
    "    diffusion_model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch_data, _ in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            batch_data = batch_data.to(device)\n",
    "            with torch.no_grad():\n",
    "                z_mean, _, z = vae.encoder(batch_data)\n",
    "\n",
    "            # Forward diffusion\n",
    "            t = torch.randint(0, time_steps, (z.shape[0],), device=device).long()\n",
    "            sqrt_alpha_bar = torch.sqrt(diffusion_model.alpha_bar[t])[:, None]\n",
    "            sqrt_one_minus_alpha_bar = torch.sqrt(1 - diffusion_model.alpha_bar[t])[:, None]\n",
    "            noisy_z = sqrt_alpha_bar * z + sqrt_one_minus_alpha_bar * torch.randn_like(z)\n",
    "\n",
    "            predicted_noise = diffusion_model(noisy_z, t)\n",
    "\n",
    "            # Resize predicted_noise về đúng latent_dim nếu cần\n",
    "            if predicted_noise.shape[-1] != z.shape[-1]:\n",
    "                predicted_noise = F.interpolate(\n",
    "                    predicted_noise.unsqueeze(1),\n",
    "                    size=z.shape[-1],\n",
    "                    mode='linear',\n",
    "                    align_corners=False\n",
    "                ).squeeze(1)\n",
    "\n",
    "            loss = F.mse_loss(predicted_noise, z)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # print(f\"[Diffusion Train] Epoch {epoch+1}, Loss: {total_loss/len(dataloader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "# def train_diffusion(vae, diffusion_model, dataloader, optimizer, device, time_steps=1000, epochs=20):\n",
    "#     diffusion_model.train()\n",
    "#     for epoch in range(epochs):\n",
    "#         total_loss = 0\n",
    "#         for batch_data, _ in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "#             batch_data = batch_data.to(device)\n",
    "#             with torch.no_grad():\n",
    "#                 z_mean, z_log_var, z = vae.encoder(batch_data)\n",
    "\n",
    "#             t = torch.randint(0, time_steps, (z.shape[0],), device=device).long()\n",
    "#             loss = diffusion_model(z, t)\n",
    "\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#             total_loss += loss.item()\n",
    "\n",
    "#         # print(f\"Epoch {epoch+1}, Diffusion Loss: {total_loss/len(dataloader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_diffusion_with_classifier(vae, diffusion_model, classifier, test_loader, device, time_steps=1000):\n",
    "    diffusion_model.eval()\n",
    "    classifier.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_data, batch_labels in test_loader:\n",
    "            batch_data = batch_data.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "\n",
    "            # Lấy z thật từ encoder\n",
    "            z_mean, _, z = vae.encoder(batch_data)\n",
    "\n",
    "            # Forward diffusion\n",
    "            noisy_z = torch.sqrt(diffusion_model.alpha_bar[-1]) * z \\\n",
    "                      + torch.sqrt(1 - diffusion_model.alpha_bar[-1]) * torch.randn_like(z)\n",
    "\n",
    "            # Reverse diffusion\n",
    "            z_recovered = noisy_z\n",
    "            for t in reversed(range(time_steps)):\n",
    "                t_tensor = torch.full((z.shape[0],), t, device=z.device, dtype=torch.long)\n",
    "                noise_pred = diffusion_model(z_recovered, t_tensor)\n",
    "                alpha = diffusion_model.alpha[t]\n",
    "                alpha_bar = diffusion_model.alpha_bar[t]\n",
    "                beta = diffusion_model.beta[t]\n",
    "\n",
    "                noise_term = torch.sqrt(beta) * torch.randn_like(z_recovered) if t > 0 else 0\n",
    "                z_recovered = (1 / torch.sqrt(alpha)) * (z_recovered - ((1 - alpha) / torch.sqrt(1 - alpha_bar)) * noise_pred) + noise_term\n",
    "\n",
    "            print('z_recovered shape:', z_recovered)\n",
    "            # Phân loại\n",
    "            logits = classifier(z_recovered)\n",
    "            preds = torch.sigmoid(logits)\n",
    "            correct += (preds == batch_labels).sum().item()\n",
    "            total += batch_labels.size(0)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(accuracy)\n",
    "    print(f\"[Diffusion → Classifier] Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate_diffusion_with_classifier(vae, diffusion_model, classifier, test_loader, device, time_steps=1000):\n",
    "#     vae.eval()\n",
    "#     diffusion_model.eval()\n",
    "#     classifier.eval()\n",
    "    \n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for batch_data, batch_labels in test_loader:\n",
    "#             batch_data = batch_data.to(device)\n",
    "#             batch_labels = batch_labels.to(device)\n",
    "\n",
    "#             # Lấy z từ encoder\n",
    "#             z_mean, z_log_var, z = vae.encoder(batch_data)\n",
    "\n",
    "#             # Forward diffusion\n",
    "#             t_forward = time_steps - 1\n",
    "#             sqrt_alpha_bar = torch.sqrt(diffusion_model.alpha_bar[t_forward])\n",
    "#             sqrt_one_minus_alpha_bar = torch.sqrt(1 - diffusion_model.alpha_bar[t_forward])\n",
    "#             noisy_z = sqrt_alpha_bar * z + sqrt_one_minus_alpha_bar * torch.randn_like(z)\n",
    "\n",
    "#             # Reverse diffusion (hoàn nhiễu)\n",
    "#             z_recovered = noisy_z\n",
    "#             for t in reversed(range(time_steps)):\n",
    "#                 z_recovered = diffusion_model.denoise_step(z_recovered, t)\n",
    "\n",
    "#             # Phân loại trên z đã hoàn nhiễu\n",
    "#             logits = classifier(z_recovered)\n",
    "\n",
    "#             # Kiểm tra shape của logits\n",
    "#             # print(\"Logits shape:\", logits.shape)  # Debug\n",
    "\n",
    "#             if len(logits.shape) == 1:\n",
    "#                 # Trường hợp: binary classification với output shape [batch_size]\n",
    "#                 preds = (torch.sigmoid(logits) > 0.5).float()\n",
    "#             elif len(logits.shape) == 2:\n",
    "#                 # Trường hợp: multi-class classification\n",
    "#                 preds = torch.argmax(logits, dim=1)\n",
    "#             else:\n",
    "#                 raise ValueError(f\"Unexpected logits shape: {logits.shape}\")\n",
    "\n",
    "#             # Cập nhật accuracy\n",
    "#             if len(batch_labels.shape) == 2 and batch_labels.shape[1] == 1:\n",
    "#                 batch_labels = batch_labels.squeeze(1)  # về shape [batch_size]\n",
    "\n",
    "#             correct += (preds == batch_labels).sum().item()\n",
    "#             total += batch_labels.size(0)\n",
    "\n",
    "#     accuracy = correct / total\n",
    "#     # print(f\"Accuracy on recovered z: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████| 22/22 [00:00<00:00, 92.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Diffusion Loss: 42.7793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|██████████| 22/22 [00:00<00:00, 111.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Diffusion Loss: 4.0952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|██████████| 22/22 [00:00<00:00, 108.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Diffusion Loss: 1.3671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|██████████| 22/22 [00:00<00:00, 109.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Diffusion Loss: 1.0407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|██████████| 22/22 [00:00<00:00, 108.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Diffusion Loss: 0.9676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|██████████| 22/22 [00:00<00:00, 107.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Diffusion Loss: 0.9269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|██████████| 22/22 [00:00<00:00, 105.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Diffusion Loss: 0.8888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|██████████| 22/22 [00:00<00:00, 105.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Diffusion Loss: 0.8463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|██████████| 22/22 [00:00<00:00, 108.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Diffusion Loss: 0.8085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|██████████| 22/22 [00:00<00:00, 101.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Diffusion Loss: 0.7707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|██████████| 22/22 [00:00<00:00, 100.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Diffusion Loss: 0.7214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|██████████| 22/22 [00:00<00:00, 118.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Diffusion Loss: 0.6964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|██████████| 22/22 [00:00<00:00, 116.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Diffusion Loss: 0.6584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|██████████| 22/22 [00:00<00:00, 119.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Diffusion Loss: 0.6505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|██████████| 22/22 [00:00<00:00, 117.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Diffusion Loss: 0.6098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|██████████| 22/22 [00:00<00:00, 112.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Diffusion Loss: 0.5598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20: 100%|██████████| 22/22 [00:00<00:00, 112.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Diffusion Loss: 0.5508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20: 100%|██████████| 22/22 [00:00<00:00, 122.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Diffusion Loss: 0.5294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20: 100%|██████████| 22/22 [00:00<00:00, 112.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Diffusion Loss: 0.5032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20: 100%|██████████| 22/22 [00:00<00:00, 110.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Diffusion Loss: 0.4683\n",
      "Logits shape: torch.Size([32])\n",
      "Logits shape: torch.Size([32])\n",
      "Logits shape: torch.Size([32])\n",
      "Logits shape: torch.Size([32])\n",
      "Logits shape: torch.Size([32])\n",
      "Logits shape: torch.Size([32])\n",
      "Logits shape: torch.Size([32])\n",
      "Logits shape: torch.Size([32])\n",
      "Logits shape: torch.Size([32])\n",
      "Logits shape: torch.Size([12])\n",
      "Accuracy on recovered z: 0.5433\n"
     ]
    }
   ],
   "source": [
    "# latent_dim = vae.encoder[-1].out_features\n",
    "diffusion_model = SimpleDiffusionModel(latent_dim=latent_dim).to(device)\n",
    "diffusion_optimizer = optim.Adam(diffusion_model.parameters(), lr=1e-3)\n",
    "\n",
    "train_diffusion(vae_new, diffusion_model, train_loader, diffusion_optimizer, device)\n",
    "\n",
    "evaluate_diffusion_with_classifier(vae, diffusion_model, vae.classifier, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████| 22/22 [00:00<00:00, 27.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Diffusion Train] Epoch 1, Loss: 0.9704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|██████████| 22/22 [00:00<00:00, 35.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Diffusion Train] Epoch 2, Loss: 0.8516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|██████████| 22/22 [00:00<00:00, 37.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Diffusion Train] Epoch 3, Loss: 0.7556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|██████████| 22/22 [00:00<00:00, 37.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Diffusion Train] Epoch 4, Loss: 0.7143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|██████████| 22/22 [00:00<00:00, 37.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Diffusion Train] Epoch 5, Loss: 0.6931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|██████████| 22/22 [00:00<00:00, 37.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Diffusion Train] Epoch 6, Loss: 0.6435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|██████████| 22/22 [00:00<00:00, 37.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Diffusion Train] Epoch 7, Loss: 0.6558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|██████████| 22/22 [00:00<00:00, 36.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Diffusion Train] Epoch 8, Loss: 0.6401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|██████████| 22/22 [00:00<00:00, 37.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Diffusion Train] Epoch 9, Loss: 0.6601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|██████████| 22/22 [00:00<00:00, 36.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Diffusion Train] Epoch 10, Loss: 0.6458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|██████████| 22/22 [00:00<00:00, 36.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Diffusion Train] Epoch 11, Loss: 0.6311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|██████████| 22/22 [00:00<00:00, 36.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Diffusion Train] Epoch 12, Loss: 0.6599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|██████████| 22/22 [00:00<00:00, 37.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Diffusion Train] Epoch 13, Loss: 0.6445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|██████████| 22/22 [00:00<00:00, 35.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Diffusion Train] Epoch 14, Loss: 0.6276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|██████████| 22/22 [00:00<00:00, 37.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Diffusion Train] Epoch 15, Loss: 0.6316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|██████████| 22/22 [00:00<00:00, 36.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Diffusion Train] Epoch 16, Loss: 0.6384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20: 100%|██████████| 22/22 [00:00<00:00, 37.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Diffusion Train] Epoch 17, Loss: 0.6527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20: 100%|██████████| 22/22 [00:00<00:00, 39.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Diffusion Train] Epoch 18, Loss: 0.6400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20: 100%|██████████| 22/22 [00:00<00:00, 35.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Diffusion Train] Epoch 19, Loss: 0.6421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20: 100%|██████████| 22/22 [00:00<00:00, 36.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Diffusion Train] Epoch 20, Loss: 0.6438\n",
      "z_recovered shape: tensor([[-583.2983,  -61.8582, -222.6907,  ..., -283.3321, -190.9614,\n",
      "         -271.3030],\n",
      "        [ 160.2209,  288.3057, -313.7933,  ...,  116.0769, -196.0297,\n",
      "          -73.1510],\n",
      "        [ 259.6774, -160.9802,  251.6780,  ...,  530.2748,   65.6217,\n",
      "          427.5295],\n",
      "        ...,\n",
      "        [-166.6937,  352.1508, -419.4474,  ..., -155.3312,  -23.6157,\n",
      "         -361.4874],\n",
      "        [-295.3281,  396.7336,  -63.2645,  ...,  124.9102,  173.7991,\n",
      "           93.7586],\n",
      "        [-447.0365, -138.5770,  140.8850,  ..., -548.7076,   90.9476,\n",
      "          -75.5248]], device='cuda:0')\n",
      "z_recovered shape: tensor([[ 241.3752,    3.4590,  -95.2587,  ...,  351.7063,  -73.9934,\n",
      "         -120.2489],\n",
      "        [-195.7940,  206.0456,   79.7677,  ...,  219.9612, -114.5185,\n",
      "          -23.5802],\n",
      "        [  35.7229, -283.4267, -190.9855,  ..., -361.0707, -197.1773,\n",
      "         -225.5519],\n",
      "        ...,\n",
      "        [  39.6922, -250.0571,   73.0073,  ...,  -81.4518,  -18.2253,\n",
      "         -279.2038],\n",
      "        [-138.5351,  -89.5000,  222.5870,  ...,  -37.5031,  314.3814,\n",
      "           11.8396],\n",
      "        [-330.9377, -254.8739,  -84.3224,  ...,  297.0221, -278.6479,\n",
      "          -71.0723]], device='cuda:0')\n",
      "z_recovered shape: tensor([[-145.4674, -311.7408,   53.4709,  ..., -145.8177,  -89.9429,\n",
      "         -440.0643],\n",
      "        [ 264.9193,  146.9820, -121.3149,  ...,  115.5477,  389.9006,\n",
      "         -124.3240],\n",
      "        [  11.8098,  281.5427, -335.1575,  ..., -349.0340,  -81.8291,\n",
      "           20.2285],\n",
      "        ...,\n",
      "        [  76.5831,  171.4584,   74.5287,  ...,  280.8249,  472.3115,\n",
      "          262.2635],\n",
      "        [-121.4518, -353.1824,  112.7070,  ...,  182.9729,  284.3640,\n",
      "           77.0698],\n",
      "        [ 275.5214,  288.7381,  -39.4801,  ...,  521.2913,   64.9693,\n",
      "           28.2134]], device='cuda:0')\n",
      "z_recovered shape: tensor([[-382.6688,  -19.2950,   36.6161,  ..., -117.5401,  -70.9280,\n",
      "          138.2854],\n",
      "        [-287.2974, -243.1978, -185.8261,  ..., -327.5891,   -1.3753,\n",
      "         -277.9627],\n",
      "        [  -8.1870,  -68.2567,    7.1356,  ..., -184.5671,  -52.6999,\n",
      "          -83.6272],\n",
      "        ...,\n",
      "        [ 247.2477, -194.3006,   36.0891,  ...,  156.8848,  -43.3516,\n",
      "         -491.2330],\n",
      "        [ -27.5956, -135.0925,  -66.1836,  ..., -365.0491, -127.2950,\n",
      "         -346.9594],\n",
      "        [-197.4310,  234.2578,  149.3403,  ...,  443.8274,  686.7519,\n",
      "           74.5475]], device='cuda:0')\n",
      "z_recovered shape: tensor([[ 120.5863,  322.0980,    2.3660,  ...,    2.9547, -443.0523,\n",
      "          -25.2308],\n",
      "        [ -46.6275,  326.9521, -313.8351,  ...,  114.0383, -214.3606,\n",
      "           19.8569],\n",
      "        [-333.3729,  -86.9677, -138.8474,  ...,  175.6463,  -78.1341,\n",
      "          230.9668],\n",
      "        ...,\n",
      "        [ -17.2378,  -20.8655,  -75.4046,  ...,  101.3508,  252.1560,\n",
      "         -181.9646],\n",
      "        [  36.1995,  412.0203, -404.2921,  ...,  -90.2683,  -83.7218,\n",
      "         -182.8300],\n",
      "        [-494.6088, -406.3966, -140.7255,  ..., -232.6253,  195.3091,\n",
      "           93.1545]], device='cuda:0')\n",
      "z_recovered shape: tensor([[-285.4941, -373.5955,  -49.4971,  ...,   40.8870, -275.3986,\n",
      "          -22.1123],\n",
      "        [ 253.9574, -303.6262, -158.4311,  ..., -161.5942,  174.4038,\n",
      "         -107.3271],\n",
      "        [-167.1310,  265.9844, -219.8932,  ...,  587.0156,  185.5530,\n",
      "         -133.3423],\n",
      "        ...,\n",
      "        [-494.7032,  338.3905, -338.6839,  ..., -282.5419, -147.0472,\n",
      "            4.4858],\n",
      "        [  55.9732,  248.5897, -163.1289,  ...,  145.9797, -649.1637,\n",
      "          110.3867],\n",
      "        [-262.1817,  -39.6144, -168.0031,  ...,  -44.6687,  107.4681,\n",
      "         -159.0128]], device='cuda:0')\n",
      "z_recovered shape: tensor([[-141.8878, -198.3055,  167.7540,  ..., -188.0434,  -65.0720,\n",
      "         -327.8804],\n",
      "        [  60.2197,   43.8182, -573.7842,  ..., -130.7669,  -33.0503,\n",
      "          -45.0380],\n",
      "        [-176.4262,  -52.8889, -203.9915,  ..., -155.0214,  -76.9637,\n",
      "         -555.6862],\n",
      "        ...,\n",
      "        [ -58.3428,  319.5226,  149.4844,  ...,  -57.7635,  319.3979,\n",
      "         -247.0417],\n",
      "        [-447.7632,  -79.4622,  190.3044,  ..., -313.2666,  271.6742,\n",
      "           24.6706],\n",
      "        [-331.5999,  110.7245,  100.4875,  ...,  -32.3416, -299.2617,\n",
      "         -369.6585]], device='cuda:0')\n",
      "z_recovered shape: tensor([[-415.9833, -140.1981, -130.1812,  ...,   88.5535, -183.1929,\n",
      "         -337.7817],\n",
      "        [  57.9816,  -44.2258,  -40.9867,  ..., -478.7401,  289.4466,\n",
      "         -116.9702],\n",
      "        [ 223.2656,  114.9752, -222.7410,  ...,  105.9786,  109.7240,\n",
      "         -159.4785],\n",
      "        ...,\n",
      "        [ -70.3097,   17.2367, -233.5051,  ..., -147.9747, -138.2712,\n",
      "         -159.9983],\n",
      "        [-275.9500,  -94.0909, -131.3378,  ..., -329.5865,  127.0225,\n",
      "           46.2483],\n",
      "        [ 426.1353, -188.2594, -146.0055,  ...,  246.0294,   54.2895,\n",
      "         -292.9736]], device='cuda:0')\n",
      "z_recovered shape: tensor([[-1017.2764,   169.9065,  -286.0094,  ...,  -358.4250,    77.4704,\n",
      "            -2.7179],\n",
      "        [ -137.1122,    66.9881,  -124.3138,  ...,   404.9724,   695.6849,\n",
      "          -185.1712],\n",
      "        [ -203.4621,  -305.4358,    19.0357,  ...,  -583.5957,  -302.1255,\n",
      "           -68.7911],\n",
      "        ...,\n",
      "        [ -574.3590,    26.6916,  -290.2054,  ...,    55.1865,    -9.2552,\n",
      "           227.0415],\n",
      "        [   15.2372,  -146.1003,   287.3018,  ...,  -103.4242,   -16.2156,\n",
      "          -433.2785],\n",
      "        [   88.9619,   146.8172,  -468.5460,  ...,    -8.0724,  -301.4161,\n",
      "            -3.6816]], device='cuda:0')\n",
      "z_recovered shape: tensor([[-3.2443e+01, -2.0335e+01, -2.0185e+01,  2.0089e+02, -2.1140e+02,\n",
      "         -4.1925e+01, -1.5008e+01,  6.8011e+01,  2.5919e+02, -2.2194e+01,\n",
      "          8.0034e+01, -1.2470e+01,  3.4428e+02, -5.6071e+01, -3.4942e+02,\n",
      "          2.5277e+02, -1.9106e+02,  4.5874e+02, -1.0627e+02, -5.6522e+01,\n",
      "          4.0032e+02,  4.2603e+02, -1.5270e+02,  5.5568e+02, -1.3984e+01,\n",
      "          4.1100e+02,  1.6924e+02,  3.8328e+02,  3.6832e+02,  9.2267e+01,\n",
      "          4.2768e+02,  3.9443e+02,  1.0818e+02,  2.7337e+02,  7.3530e+01,\n",
      "          2.3171e+02,  3.4656e+02,  1.7150e+01,  3.7437e+02, -1.6307e+01,\n",
      "          2.4380e+02, -1.1026e+02,  1.9505e+02,  4.5440e+02,  7.8295e+01,\n",
      "          1.3183e+02,  3.7293e+02,  2.1664e+02, -1.5319e+02, -1.0496e+02,\n",
      "          1.4494e+02,  2.3307e+02, -2.9645e+02, -5.6136e+01,  1.7068e+01,\n",
      "         -1.0677e+02, -2.8468e+02, -2.6207e+02, -2.0105e+01,  3.7471e+02,\n",
      "          2.1452e+02,  2.3593e+02, -5.1289e+01, -4.9024e+00],\n",
      "        [-2.4973e+02, -1.4037e+02,  1.9667e+02, -2.2061e+01, -1.2481e+02,\n",
      "          2.5097e+02, -1.5273e+02,  1.5104e+01,  9.5930e+02, -1.4611e+02,\n",
      "          1.7246e+02, -1.8967e+01,  5.5776e+00,  2.4750e+02,  1.6840e+02,\n",
      "         -1.8426e+01, -3.9965e+02,  3.1035e+02, -6.9806e+01,  3.9628e+01,\n",
      "          2.4454e+02,  2.8040e+01, -1.8377e+02,  2.2338e+02,  1.0526e+02,\n",
      "         -7.2411e+01,  3.6942e+02,  5.3734e+02,  2.3897e+02,  1.2077e+02,\n",
      "          3.2318e+02, -2.8268e+02,  3.6996e+02, -3.5135e+02, -4.8132e+01,\n",
      "          1.4019e+02,  2.6112e+02,  1.7178e+02,  8.7916e+01,  2.4968e+02,\n",
      "         -4.7967e+02, -4.3028e+02, -2.7376e+01, -2.3422e+02, -4.2468e+02,\n",
      "         -4.7149e+02,  1.0906e+02,  3.7307e+02,  1.1857e+02, -2.8303e+02,\n",
      "         -2.5378e+01,  8.3063e+00, -2.8456e+02, -8.6434e+01, -3.4550e+01,\n",
      "         -1.7339e+02,  1.0419e+01,  5.1293e+02,  4.9777e+02, -3.3724e+02,\n",
      "          2.3425e+02, -9.7666e+01,  1.2466e+02, -3.3961e+01],\n",
      "        [-7.5845e+01,  5.5353e+01, -1.1790e+02, -1.7607e+02,  2.9144e+02,\n",
      "         -9.6427e+01, -4.4964e+01,  2.6567e+02,  5.4716e+01, -2.7846e+02,\n",
      "         -1.1756e+01,  2.1372e+02,  7.3802e+01,  8.7583e+01, -3.9454e+02,\n",
      "          3.2873e+01, -3.6175e+02,  2.1861e+02, -5.4029e+01,  1.8817e+02,\n",
      "          2.5827e+02,  1.9289e+02,  9.4418e+01, -2.4207e+01, -3.6157e+02,\n",
      "         -1.5654e+02, -1.0315e+02,  2.1783e+02,  9.0544e+01, -3.8272e+02,\n",
      "          2.8775e+02, -1.2676e+02, -9.8640e+00, -1.4736e+02,  9.9837e+01,\n",
      "          1.5824e+02, -8.8646e+01, -1.4228e+02, -1.2372e+00, -1.3726e+02,\n",
      "         -1.8466e+01, -5.4533e+01, -4.1581e+02, -1.0982e+02, -1.8459e+02,\n",
      "         -2.3550e+02,  5.1619e+01,  3.7015e+02,  8.4650e+01,  7.9094e+01,\n",
      "         -3.5089e+01, -5.1875e+01, -3.2053e+02, -1.2628e+02, -1.3646e+02,\n",
      "          3.0689e+02, -1.5625e+02, -1.0944e+02,  1.9404e+02, -5.8969e+01,\n",
      "          2.3928e+02,  9.7700e+00,  1.2057e+02, -1.2448e+02],\n",
      "        [ 3.4394e+02,  5.3131e-01,  1.0334e+02, -1.7181e+01,  1.9533e+02,\n",
      "         -1.8948e+02, -1.2917e+02,  9.2310e+01,  4.2738e+02, -1.8056e+02,\n",
      "          6.0458e+01,  2.0214e+02,  1.5639e+02,  2.1967e+02, -6.0375e+01,\n",
      "         -2.0548e+00, -2.1254e+02,  2.3821e+02, -8.4746e+01,  3.7275e+01,\n",
      "         -2.6727e+02,  2.6345e+02, -5.7498e+01, -1.6905e+01,  1.6679e+02,\n",
      "          4.9750e+01,  2.0132e+02,  1.1612e+02,  6.6622e+01, -3.7845e+00,\n",
      "         -3.3658e+01,  4.0861e+01,  2.9556e+01,  1.9752e+02,  2.7014e+02,\n",
      "          1.3138e+02, -6.9241e+00,  7.0630e+01,  2.6030e+02,  1.6877e+02,\n",
      "          2.2867e+02,  2.7250e+01, -3.2703e+01,  1.1247e+01,  1.2429e+02,\n",
      "         -7.0243e+01,  3.2535e+02,  2.9983e+02,  1.0439e+02,  1.8118e+02,\n",
      "          1.3551e+02,  4.6656e+00,  3.0501e+02,  1.2332e+02, -1.8650e+02,\n",
      "         -8.1748e+01,  7.4111e+00,  4.3022e+01,  1.6366e+02,  1.4217e+02,\n",
      "         -2.3624e+01,  2.6222e+02,  1.8855e+02, -4.7255e+01],\n",
      "        [-4.9469e+02,  2.5369e+02, -1.5455e+02, -3.0732e+02, -5.8441e+02,\n",
      "          1.7698e+02,  1.1342e+02,  6.2116e+02,  3.3854e+02,  2.6835e+02,\n",
      "          1.5131e+02, -2.5075e+02, -3.8904e+02,  1.7791e+02, -3.2555e+01,\n",
      "         -1.2560e+02,  4.1231e+01,  6.9756e+02,  1.7125e+02, -1.2266e+00,\n",
      "          1.9221e+02, -8.5891e+01,  3.6533e+02, -3.6671e+02, -2.2751e+02,\n",
      "         -3.3164e+01,  3.4121e+02,  2.5295e+02,  2.2171e+02,  5.6281e+01,\n",
      "          4.4812e+02,  2.2229e+02,  1.2804e+02, -3.1515e+02, -2.0739e+01,\n",
      "         -3.2593e+02, -2.0626e+02, -1.5324e+01,  2.8448e+02, -1.1839e+02,\n",
      "         -3.8924e+02,  3.2448e+02, -1.3749e+02,  1.4075e+02, -2.3358e+02,\n",
      "         -3.4467e+02,  3.0183e+02,  2.9848e+02,  1.2037e+02, -2.3587e+02,\n",
      "          6.6790e+01,  6.6031e+01, -1.8706e+02,  1.9826e+02,  1.0782e+01,\n",
      "          2.7015e+02, -2.7809e+02,  2.2649e+00, -1.2684e+02, -2.8132e+02,\n",
      "          4.4815e+02, -8.6773e+00,  4.1320e+02,  1.6120e+02],\n",
      "        [-1.2553e+02,  2.4079e+01,  2.7767e+02, -1.0021e+02, -3.2248e+02,\n",
      "         -4.8030e+01, -1.4288e+02,  3.6620e+02,  1.2869e+02,  2.8807e+02,\n",
      "         -1.9537e+02, -2.1011e+02,  7.1265e+01,  4.1606e+00, -2.8028e+02,\n",
      "          1.6378e+02, -3.6176e+02,  3.0095e+02, -1.0099e+02,  2.6588e+01,\n",
      "         -1.0280e+02,  1.3745e+02, -1.2752e+02, -2.0171e+02,  7.2232e+01,\n",
      "          1.8977e+02,  3.7856e+02,  1.7560e+02,  3.4269e+02,  2.1315e+02,\n",
      "          6.8727e+01,  2.4545e+02, -6.8314e+01, -3.6385e+02,  1.6117e+02,\n",
      "         -2.7754e+02, -4.3255e+02, -7.8364e+01,  1.0941e+02, -2.6286e+01,\n",
      "          1.9537e+02,  2.3643e+02, -1.6804e+02, -2.0617e+02, -5.5522e+02,\n",
      "         -4.1598e+02,  5.7414e+02,  2.2461e+02, -5.8481e+01,  7.5537e+01,\n",
      "         -8.3825e+01, -1.2957e+02,  1.6043e+01,  8.8848e+01,  2.6695e+02,\n",
      "          2.9746e+02,  2.4850e+02, -3.3181e+02,  9.8592e+01,  1.6024e+02,\n",
      "          1.2340e+02,  8.2120e+01, -2.0429e+02,  8.4848e+01],\n",
      "        [ 3.9307e+01, -2.0227e+02,  1.9032e+02, -1.9387e+02,  4.7678e+01,\n",
      "         -9.2124e+01, -3.3985e+02,  3.8549e+02,  4.1836e+02,  7.3974e+01,\n",
      "          1.2683e+02, -1.7610e+00,  1.4061e+02,  2.5780e+02, -1.4691e+02,\n",
      "         -1.8596e+02,  3.1774e+01,  4.5055e+02, -1.7220e+01,  1.7639e+02,\n",
      "         -1.1307e+02,  7.4776e+01,  9.2838e+01,  4.8979e+01,  8.4118e+01,\n",
      "          4.0501e+02,  3.9630e+02,  3.9110e+02,  5.3614e+02,  1.9904e+02,\n",
      "          2.2683e+02,  1.5213e+01, -2.8603e+02,  2.5505e+02,  3.0978e+02,\n",
      "         -8.0471e+01,  5.0107e+01,  2.8776e+02,  5.8682e+02,  3.0870e+02,\n",
      "          5.2780e+02,  4.5422e+02, -2.2649e+02, -2.0081e+01, -2.0576e+02,\n",
      "         -3.6140e+02,  4.0650e+02,  4.2190e+02,  5.7640e+01,  3.5685e+02,\n",
      "          3.2099e+02,  1.6248e+02,  1.6877e+02, -1.0682e+02, -2.5950e+02,\n",
      "         -2.6390e+02, -2.3056e+02, -2.8607e+01, -5.5906e+01, -8.7766e+00,\n",
      "          2.5853e+02,  4.0947e+02,  2.0637e+02, -2.1019e+02],\n",
      "        [-5.8461e+02,  4.8343e+01, -1.9034e+02, -1.8931e+02,  1.2392e+02,\n",
      "          1.7266e+01, -3.9110e+02,  8.2008e+01,  8.2523e+02, -4.4184e+01,\n",
      "          1.4888e+02, -1.1473e+02,  9.5245e+01,  2.1442e+02, -1.4686e+02,\n",
      "          3.3588e+01, -1.1214e+02,  3.6442e+02, -9.4770e+01,  8.2949e+01,\n",
      "         -1.5635e+02, -3.2710e+02,  3.6013e+02,  3.6468e+02, -6.7078e+01,\n",
      "          1.5357e+02,  3.9016e+02,  2.3918e+02,  1.7509e+02, -2.8769e+02,\n",
      "          3.2527e+02, -2.5002e+02,  2.8337e+01,  4.9825e+01,  2.5849e+02,\n",
      "          8.2613e+01,  3.4351e+02, -1.2000e+02,  2.0314e+02,  1.1812e+02,\n",
      "          5.1148e+00,  2.9499e+01, -1.9893e+02, -3.9166e+01, -1.5280e+02,\n",
      "         -4.2394e+02, -9.1742e+01,  6.3003e+02,  1.4067e+02, -3.7321e+02,\n",
      "         -4.2493e+02,  1.6961e+02,  2.3014e+02, -6.9082e+02,  3.9009e+01,\n",
      "         -2.1355e+01, -1.2247e+02, -1.6209e+02,  2.6409e+02, -6.2448e+01,\n",
      "          3.4078e+02, -4.3023e+02, -1.5347e+02, -1.0875e+01],\n",
      "        [-4.9602e+02,  5.6449e+01, -9.8325e+01,  3.6248e+02, -2.5505e+02,\n",
      "         -1.9728e+02, -5.5623e+01,  4.8107e+02,  9.1996e+01,  3.0293e+02,\n",
      "          5.4644e+01, -1.0233e+02,  5.3325e+01,  4.1272e+02,  8.0451e+01,\n",
      "          1.9151e+02,  1.8609e+02,  3.1065e+02,  1.7181e+02, -4.1892e+01,\n",
      "          2.8265e+02,  3.2986e+02, -1.5792e+02,  1.3677e+02,  3.0868e+02,\n",
      "          5.1902e+02, -7.6280e+01,  4.4913e+02,  4.1699e+02,  5.2404e+02,\n",
      "         -8.8918e+01, -2.6091e+01,  3.6263e+02, -1.0306e+02,  9.5057e+01,\n",
      "         -1.0838e+01, -8.2648e+01,  4.4272e+02, -1.9197e+00, -1.2749e+02,\n",
      "          4.6949e+01,  3.2768e+02,  3.6491e+02,  1.9695e+02, -4.3631e+02,\n",
      "         -6.9419e+01,  1.5970e+02,  3.3596e+02, -5.3563e+01,  9.0671e+01,\n",
      "          5.1840e+01,  5.8936e+01, -3.9531e+01,  4.2842e+02,  4.5232e+02,\n",
      "          2.3531e+02,  8.3413e+01, -3.2035e+01,  3.1919e+02, -6.9915e+01,\n",
      "         -2.3671e+02, -1.5029e+02,  3.3141e+02,  5.7281e+01],\n",
      "        [-2.7541e+02, -8.0926e+01,  3.3288e+01, -4.6440e+01,  2.0622e+02,\n",
      "         -1.4167e+02, -8.4949e+00, -1.5623e+02, -6.4948e+01,  2.0363e+02,\n",
      "         -2.8999e+02, -3.5969e+01,  3.1238e+02, -8.6685e+00, -1.1251e+02,\n",
      "         -7.2749e+00,  7.4452e+01, -2.6544e+01,  1.9420e+02, -2.6553e+02,\n",
      "          4.6197e+01, -1.5454e+02,  4.5856e+01,  3.2535e+02,  1.1349e+01,\n",
      "          3.2634e+01,  3.6809e+02, -1.4066e+01,  2.7860e+02, -3.0538e+01,\n",
      "          3.3597e+01,  1.5220e+02,  5.3662e+02,  6.8204e+01, -6.7038e+01,\n",
      "          1.6920e+02,  1.9451e+02, -1.9245e+02,  2.6018e+02,  2.7590e+02,\n",
      "          2.3938e+01,  6.1935e+01,  2.6346e+02, -7.9911e+01, -1.5386e+02,\n",
      "         -2.4229e+02,  1.2408e+02,  2.5659e+02, -1.0966e+02, -1.0405e+02,\n",
      "          9.5508e+01, -2.3006e+02,  5.0422e+01,  8.6855e+01,  2.0141e+02,\n",
      "          2.6906e+01,  3.4434e+02, -8.2204e+01,  6.4602e+01,  3.3834e+01,\n",
      "          5.6337e+01,  5.0548e+00, -1.2187e+02, -4.5980e+01],\n",
      "        [-2.9040e+02, -1.5754e+02, -5.9589e+01, -9.7231e+01, -1.9952e+02,\n",
      "          3.6379e+01, -1.3872e+01,  1.2487e+02,  3.4280e+02,  1.2194e+01,\n",
      "         -2.3607e+02, -1.0546e+02,  2.7162e+02,  1.3346e+02,  3.7303e+01,\n",
      "          2.6761e+02,  6.9204e+01, -1.4717e+02, -4.0351e+01,  2.1826e+02,\n",
      "          4.2579e+01,  1.2068e+02,  2.6272e+02,  1.2074e+02, -2.5958e+02,\n",
      "          2.7512e+02, -1.3216e+02,  4.7460e+02,  2.6713e+02,  2.0385e+02,\n",
      "          3.7969e+02, -2.1655e+02, -1.0801e+02,  8.1123e+00,  1.8744e+02,\n",
      "          9.0169e+01,  2.5920e+02,  2.6094e+02,  8.8120e+01,  1.6983e+02,\n",
      "          8.0029e+01,  4.0220e+02, -2.4011e+02,  4.9422e+01, -2.4047e+02,\n",
      "         -5.6781e+02,  1.5406e+01,  5.2506e+01, -6.3269e+01,  4.4086e+00,\n",
      "          3.0510e+01, -1.3562e+02, -7.0395e+01,  1.0406e+02,  2.7023e+01,\n",
      "          2.2031e+02, -1.2414e+02,  5.6366e+00, -4.3439e+02, -1.4562e+02,\n",
      "         -6.2930e+01, -2.9943e+00,  1.8871e+02, -2.3784e+02],\n",
      "        [-4.8703e+02,  2.0579e+02, -8.1129e+01, -1.9552e+02,  1.5882e+02,\n",
      "         -9.6517e+01,  1.1500e+02,  4.2482e+01,  4.0810e+02,  1.1799e+02,\n",
      "          1.3979e+02,  2.5703e+02, -1.0177e+01,  5.2588e+02, -5.0451e+01,\n",
      "         -8.4238e+01,  2.0955e+02,  5.4053e+02,  1.7339e+02,  3.2465e+02,\n",
      "          2.1427e+02,  1.8894e+02,  1.4291e+02, -1.0237e+02,  5.0549e+02,\n",
      "          2.5536e+02,  1.7916e+02,  7.9881e+01,  3.5198e+02, -6.2978e+01,\n",
      "          2.0100e+02,  1.1450e+02, -2.0792e+02,  2.6275e+02, -1.1056e+02,\n",
      "         -1.8420e+01, -4.2287e+01,  3.9334e+00,  4.8434e+02,  6.2352e+01,\n",
      "          1.6596e+02,  1.4667e+01,  1.5977e+02, -6.8430e+01,  9.7072e+01,\n",
      "          1.6956e+02,  2.4416e+02,  4.6506e+02,  7.1618e+00, -1.7155e+02,\n",
      "         -2.1535e+02,  2.0669e+02, -2.1615e+01, -1.4279e+02,  1.7835e+00,\n",
      "          1.6932e+02, -2.9809e+02, -1.9986e+02,  3.1964e+01,  2.2523e+02,\n",
      "          3.5217e+02,  6.7735e+01,  1.2975e+02,  3.2593e+02]], device='cuda:0')\n",
      "0.0\n",
      "[Diffusion → Classifier] Accuracy: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# Dummy input để lấy latent_dim\n",
    "with torch.no_grad():\n",
    "    dummy_input = torch.randn(1, input_dim).to(device)  # input_dim = số chiều đầu vào của VAE encoder\n",
    "    z_mean, _, _ = vae.encoder(dummy_input)\n",
    "    latent_dim = z_mean.shape[1]\n",
    "\n",
    "# Khởi tạo base UNET_1D với input_dim = latent_dim\n",
    "unet_base = UNET_1D(\n",
    "    input_dim=latent_dim,\n",
    "    layer_n=32,               # có thể điều chỉnh theo nhu cầu\n",
    "    kernel_size=5,\n",
    "    depth=2\n",
    ")\n",
    "\n",
    "# Wrap bằng lớp hỗ trợ diffusion\n",
    "diffusion_model = UNET_1D_Diffusion(latent_dim=latent_dim).to(device)\n",
    "\n",
    "# Optimizer\n",
    "diffusion_optimizer = optim.Adam(diffusion_model.parameters(), lr=1e-3)\n",
    "\n",
    "# Huấn luyện mô hình diffusion\"\n",
    "train_diffusion(vae_new, diffusion_model, train_loader, diffusion_optimizer, device)\n",
    "\n",
    "# Đánh giá độ chính xác phân loại trên z đã phục hồi\n",
    "evaluate_diffusion_with_classifier(vae, diffusion_model, vae.classifier, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
