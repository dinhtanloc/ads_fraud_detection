{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Optional, Union, Tuple\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mK\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "from typing import Optional, Union, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "K.clear_session()\n",
    "tf.config.run_functions_eagerly(True)\n",
    "# import tensorflow_probability as tfp\n",
    "from keras import ops\n",
    "import tensorflow_addons as tfa\n",
    "# from keras.activations import gelu as keras_gelu\n",
    "\n",
    "import pandas as pd\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "@tf.function\n",
    "def identity(x):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Feature_1  Feature_2  Feature_3  Feature_4  Feature_5  Feature_6  \\\n",
      "0   0.374540   0.185133   0.261706   0.672703   0.571996   0.393636   \n",
      "1   0.950714   0.541901   0.246979   0.796681   0.805432   0.473436   \n",
      "2   0.731994   0.872946   0.906255   0.250468   0.760161   0.854547   \n",
      "3   0.598658   0.732225   0.249546   0.624874   0.153900   0.340004   \n",
      "4   0.156019   0.806561   0.271950   0.571746   0.149249   0.869650   \n",
      "\n",
      "   Feature_7  Feature_8  Feature_9  Feature_10  Feature_11  Feature_12  Target  \n",
      "0   0.648257   0.038799   0.720268    0.913578    0.373641    0.533031       1  \n",
      "1   0.172386   0.186773   0.687283    0.525360    0.332912    0.137899       1  \n",
      "2   0.872395   0.831246   0.095754    0.724910    0.176154    0.591243       0  \n",
      "3   0.613116   0.766768   0.922572    0.436048    0.607267    0.314786       0  \n",
      "4   0.157204   0.350643   0.568472    0.630035    0.476624    0.052349       0  \n",
      "Số lượng mẫu: 1000, Số lượng cột: 13\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)  \n",
    "\n",
    "num_samples = 1000\n",
    "\n",
    "data = {}\n",
    "for i in range(1, 13):\n",
    "    col_name = f\"Feature_{i}\"\n",
    "    data[col_name] = np.random.rand(num_samples)  \n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df['Target'] = np.random.randint(0, 2, size=num_samples) \n",
    "\n",
    "print(df.head())\n",
    "feature_columns = list(df.columns[:-1])  \n",
    "\n",
    "print(f\"Số lượng mẫu: {len(df)}, Số lượng cột: {len(df.columns)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GLUBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, units: Optional[int] = None,\n",
    "                 momentum: Optional[float] = 0.02):\n",
    "        super(GLUBlock, self).__init__()\n",
    "        self.units = units\n",
    "        self.momentum = momentum\n",
    "        \n",
    "    def build(self, input_shape: tf.TensorShape):\n",
    "        if self.units is None:\n",
    "            self.units = input_shape[-1]\n",
    "            \n",
    "        self.fc_outout = tf.keras.layers.Dense(self.units, \n",
    "                                               use_bias=False)\n",
    "        self.bn_outout = tf.keras.layers.BatchNormalization(\n",
    "                                                            momentum=self.momentum)\n",
    "        \n",
    "        self.fc_gate = tf.keras.layers.Dense(self.units, \n",
    "                                             use_bias=False)\n",
    "        self.bn_gate = tf.keras.layers.BatchNormalization(\n",
    "                                                          momentum=self.momentum)\n",
    "        \n",
    "    def call(self, inputs: Union[tf.Tensor, np.ndarray], training: Optional[bool] = None):\n",
    "        output = self.bn_outout(self.fc_outout(inputs), \n",
    "                                training=training)\n",
    "        gate = self.bn_gate(self.fc_gate(inputs), \n",
    "                            training=training)\n",
    "    \n",
    "        return output * tf.keras.activations.sigmoid(gate) # GLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureTransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, units: Optional[int] = None, \n",
    "                 momentum: Optional[float] = 0.02, skip=False):\n",
    "        super(FeatureTransformerBlock, self).__init__()\n",
    "        self.units = units\n",
    "        # self.virtual_batch_size = virtual_batch_size\n",
    "        self.momentum = momentum\n",
    "        self.skip = skip\n",
    "        \n",
    "    def build(self, input_shape: tf.TensorShape):\n",
    "        if self.units is None:\n",
    "            self.units = input_shape[-1]\n",
    "        \n",
    "        self.initial = GLUBlock(units = self.units, \n",
    "                                # virtual_batch_size=self.virtual_batch_size, \n",
    "                                momentum=self.momentum)\n",
    "        self.residual =  GLUBlock(units = self.units, \n",
    "                                #   virtual_batch_size=self.virtual_batch_size, \n",
    "                                  momentum=self.momentum)\n",
    "        \n",
    "    def call(self, inputs: Union[tf.Tensor, np.ndarray], training: Optional[bool] = None):\n",
    "        initial = self.initial(inputs, training=training)\n",
    "        \n",
    "        if self.skip == True:\n",
    "            initial += inputs\n",
    "\n",
    "        residual = self.residual(initial, training=training) # skip\n",
    "        \n",
    "        return (initial + residual) * np.sqrt(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentiveTransformer(tf.keras.layers.Layer):\n",
    "    def __init__(self, units: Optional[int] = None, \n",
    "                 momentum: Optional[float] = 0.02):\n",
    "        super(AttentiveTransformer, self).__init__()\n",
    "        self.units = units\n",
    "        # self.virtual_batch_size = virtual_batch_size\n",
    "        self.momentum = momentum\n",
    "        \n",
    "    def build(self, input_shape: tf.TensorShape):\n",
    "        if self.units is None:\n",
    "            self.units = input_shape[-1]\n",
    "            \n",
    "        self.fc = tf.keras.layers.Dense(self.units, \n",
    "                                        use_bias=False)\n",
    "        self.bn = tf.keras.layers.BatchNormalization(\n",
    "                                                     momentum=self.momentum)\n",
    "        \n",
    "    def call(self, inputs: Union[tf.Tensor, np.ndarray], priors: Optional[Union[tf.Tensor, np.ndarray]] = None, training: Optional[bool] = None) -> tf.Tensor:\n",
    "        feature = self.bn(self.fc(inputs), \n",
    "                          training=training)\n",
    "        if priors is None:\n",
    "            output = feature\n",
    "        else:\n",
    "            output = feature * priors\n",
    "        \n",
    "        return tfa.activations.sparsemax(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabNetStep(tf.keras.layers.Layer):\n",
    "    def __init__(self, units: Optional[int] = None, \n",
    "                 momentum: Optional[float] =0.02):\n",
    "        super(TabNetStep, self).__init__()\n",
    "        self.units = units\n",
    "        # self.virtual_batch_size = virtual_batch_size\n",
    "        self.momentum = momentum\n",
    "        \n",
    "    def build(self, input_shape: tf.TensorShape):\n",
    "        if self.units is None:\n",
    "            self.units = input_shape[-1]\n",
    "        \n",
    "        self.unique = FeatureTransformerBlock(units = self.units, \n",
    "                                            #   virtual_batch_size=self.virtual_batch_size, \n",
    "                                              momentum=self.momentum,\n",
    "                                              skip=True)\n",
    "        self.attention = AttentiveTransformer(units = input_shape[-1], \n",
    "                                            #   virtual_batch_size=self.virtual_batch_size, \n",
    "                                              momentum=self.momentum)\n",
    "        \n",
    "    def call(self, inputs, shared, priors, training=None) -> Tuple[tf.Tensor]:  \n",
    "        split = self.unique(shared, training=training)\n",
    "        keys = self.attention(split, priors, training=training)\n",
    "        masked = keys * inputs\n",
    "        \n",
    "        return split, masked, keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabNetEncoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, units: int =1, \n",
    "                 n_steps: int = 3, \n",
    "                 n_features: int = 8,\n",
    "                 outputs: int = 1, \n",
    "                 gamma: float = 1.3,\n",
    "                 epsilon: float = 1e-8, \n",
    "                 sparsity: float = 1e-5, \n",
    "                #  virtual_batch_size: Optional[int]=128, \n",
    "                 momentum: Optional[float] =0.02):\n",
    "        super(TabNetEncoder, self).__init__()\n",
    "        \n",
    "        self.units = units\n",
    "        self.n_steps = n_steps\n",
    "        self.n_features = n_features\n",
    "        # self.virtual_batch_size = virtual_batch_size\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.momentum = momentum\n",
    "        self.sparsity = sparsity\n",
    "        \n",
    "    def build(self, input_shape: tf.TensorShape):            \n",
    "        self.bn = tf.keras.layers.BatchNormalization(\n",
    "                                                     momentum=self.momentum)\n",
    "        self.shared_block = FeatureTransformerBlock(units = self.n_features, \n",
    "                                                    momentum=self.momentum)        \n",
    "        self.initial_step = TabNetStep(units = self.n_features, \n",
    "                                       momentum=self.momentum)\n",
    "        self.steps = [TabNetStep(units = self.n_features, \n",
    "                                 momentum=self.momentum) for _ in range(self.n_steps)]\n",
    "        self.final = tf.keras.layers.Dense(units = self.units, \n",
    "                                           use_bias=False)\n",
    "    \n",
    "\n",
    "    def call(self, X: Union[tf.Tensor, np.ndarray], training: Optional[bool] = None) -> Tuple[tf.Tensor]:        \n",
    "        entropy_loss = 0.\n",
    "        encoded = 0.\n",
    "        output = 0.\n",
    "        importance = 0.\n",
    "        prior = tf.reduce_mean(tf.ones_like(X), axis=0)\n",
    "        \n",
    "        B = prior * self.bn(X, training=training)\n",
    "        shared = self.shared_block(B, training=training)\n",
    "        _, masked, keys = self.initial_step(B, shared, prior, training=training)\n",
    "\n",
    "        for step in self.steps:\n",
    "            entropy_loss += tf.reduce_mean(tf.reduce_sum(-keys * tf.math.log(keys + self.epsilon), axis=-1)) / tf.cast(self.n_steps, tf.float32)\n",
    "            prior *= (self.gamma - tf.reduce_mean(keys, axis=0))\n",
    "            importance += keys\n",
    "            \n",
    "            shared = self.shared_block(masked, training=training)\n",
    "            split, masked, keys = step(B, shared, prior, training=training)\n",
    "            features = tf.keras.activations.relu(split)\n",
    "            \n",
    "            output += features\n",
    "            encoded += split\n",
    "            \n",
    "        self.add_loss(self.sparsity * entropy_loss)\n",
    "          \n",
    "        prediction = self.final(output)\n",
    "        return prediction, encoded, importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabNetDecoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, units=1, \n",
    "                 n_steps = 3, \n",
    "                 n_features = 8,\n",
    "                 outputs = 1, \n",
    "                 gamma = 1.3,\n",
    "                 epsilon = 1e-8, \n",
    "                 sparsity = 1e-5, \n",
    "                #  virtual_batch_size=128, \n",
    "                 momentum=0.02):\n",
    "        super(TabNetDecoder, self).__init__()\n",
    "        \n",
    "        self.units = units\n",
    "        self.n_steps = n_steps\n",
    "        self.n_features = n_features\n",
    "        # self.virtual_batch_size = virtual_batch_size\n",
    "        self.momentum = momentum\n",
    "        \n",
    "    def build(self, input_shape: tf.TensorShape):\n",
    "        self.shared_block = FeatureTransformerBlock(units = self.n_features, \n",
    "                                                    # virtual_batch_size=self.virtual_batch_size, \n",
    "                                                    momentum=self.momentum)\n",
    "        self.steps = [FeatureTransformerBlock(units = self.n_features,\n",
    "                                            #   virtual_batch_size=self.virtual_batch_size, \n",
    "                                              momentum=self.momentum) for _ in range(self.n_steps)]\n",
    "        self.fc = [tf.keras.layers.Dense(units = self.units) for _ in range(self.n_steps)]\n",
    "    \n",
    "\n",
    "    def call(self, X: Union[tf.Tensor, np.ndarray], training: Optional[bool] = None) -> tf.Tensor:\n",
    "        decoded = 0.\n",
    "        \n",
    "        for ftb, fc in zip(self.steps, self.fc):\n",
    "            shared = self.shared_block(X, training=training)\n",
    "            feature = ftb(shared, training=training)\n",
    "            output = fc(feature)\n",
    "            \n",
    "            decoded += output\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.seed_generator = keras.random.SeedGenerator(1337)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = ops.shape(z_mean)[0]\n",
    "        dim = ops.shape(z_mean)[1]\n",
    "        epsilon = keras.random.normal(shape=(batch, dim), seed=self.seed_generator)\n",
    "        return z_mean + ops.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\SimSwap\\.conda\\lib\\site-packages\\tensorflow\\python\\data\\ops\\structured_function.py:265: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Exception encountered when calling layer 'tab_net_autoencoder' (type TabNetAutoencoder).\n\nmodule 'keras' has no attribute 'random'\n\nCall arguments received by layer 'tab_net_autoencoder' (type TabNetAutoencoder):\n  • X=tf.Tensor(shape=(32, 12), dtype=float32)\n  • training=True",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 155\u001b[0m\n\u001b[0;32m    152\u001b[0m y_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m100\u001b[39m,))\n\u001b[0;32m    154\u001b[0m \u001b[38;5;66;03m# Huấn luyện mô hình\u001b[39;00m\n\u001b[1;32m--> 155\u001b[0m \u001b[43mae\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    156\u001b[0m ae\u001b[38;5;241m.\u001b[39msummary()\n",
      "File \u001b[1;32md:\\SimSwap\\.conda\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[1;32mIn[20], line 91\u001b[0m, in \u001b[0;36mTabNetAutoencoder.call\u001b[1;34m(self, X, training)\u001b[0m\n\u001b[0;32m     89\u001b[0m z_mean \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmean_layer(encoded)\n\u001b[0;32m     90\u001b[0m z_log_var \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogvar_layer(encoded)\n\u001b[1;32m---> 91\u001b[0m z \u001b[38;5;241m=\u001b[39m Sampling()([z_mean, z_log_var])\n\u001b[0;32m     92\u001b[0m \u001b[38;5;66;03m# prediction = tf.keras.activations.sigmoid(output)\u001b[39;00m\n\u001b[0;32m     93\u001b[0m T \u001b[38;5;241m=\u001b[39m X \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m M)\n",
      "Cell \u001b[1;32mIn[19], line 6\u001b[0m, in \u001b[0;36mSampling.__init__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m----> 6\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseed_generator \u001b[38;5;241m=\u001b[39m \u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241m.\u001b[39mSeedGenerator(\u001b[38;5;241m1337\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: Exception encountered when calling layer 'tab_net_autoencoder' (type TabNetAutoencoder).\n\nmodule 'keras' has no attribute 'random'\n\nCall arguments received by layer 'tab_net_autoencoder' (type TabNetAutoencoder):\n  • X=tf.Tensor(shape=(32, 12), dtype=float32)\n  • training=True"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from typing import Optional, Union, Tuple\n",
    "\n",
    "class TabNetAutoencoder(tf.keras.Model):\n",
    "    def __init__(self, outputs: int = 1, \n",
    "                 inputs: int = 12,\n",
    "                 n_steps: int = 3, \n",
    "                 n_features: int = 8,\n",
    "                 gamma: float = 1.3, \n",
    "                 epsilon: float = 1e-8, \n",
    "                 sparsity: float = 1e-5, \n",
    "                 feature_column: Optional[tf.keras.layers.Dense] = None, \n",
    "                #  virtual_batch_size: Optional[int] = None, \n",
    "                 momentum: Optional[float] = 0.02):\n",
    "        super(TabNetAutoencoder, self).__init__()\n",
    "        \n",
    "        self.outputs = outputs\n",
    "        self.inputs = inputs\n",
    "        self.n_steps = n_steps\n",
    "        self.n_features = n_features\n",
    "        self.feature_column = feature_column\n",
    "        # self.virtual_batch_size = virtual_batch_size\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.momentum = momentum\n",
    "        self.sparsity = sparsity\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "        self.classification_loss_tracker = keras.metrics.Mean(name=\"classification_loss\")\n",
    "        \n",
    "        if feature_column is None:\n",
    "            self.feature = tf.keras.layers.Lambda(lambda x: x)\n",
    "        else:\n",
    "            self.feature = feature_column\n",
    "            \n",
    "        self.encoder = TabNetEncoder(units=outputs, \n",
    "                                    n_steps=n_steps, \n",
    "                                    n_features=n_features,\n",
    "                                    gamma=gamma, \n",
    "                                    epsilon=epsilon, \n",
    "                                    sparsity=sparsity,\n",
    "                                    # virtual_batch_size=virtual_batch_size, \n",
    "                                    momentum=momentum)\n",
    "        self.mlp1=tf.keras.Sequential([\n",
    "                tf.keras.layers.Dense(64, activation='relu'), \n",
    "                tf.keras.layers.Dense(32, activation='relu') \n",
    "            ])\n",
    "        \n",
    "        self.decoder = TabNetDecoder(units=inputs, \n",
    "                                     n_steps=n_steps, \n",
    "                                     n_features=n_features,\n",
    "                                    #  virtual_batch_size=virtual_batch_size, \n",
    "                                     momentum=momentum)\n",
    "        self.mlp2=tf.keras.Sequential([\n",
    "                tf.keras.layers.Dense(32, activation='relu'), \n",
    "                tf.keras.layers.Dense(64, activation='relu') \n",
    "            ])\n",
    "        \n",
    "        self.mean_layer = tf.keras.layers.Dense(64)\n",
    "        self.logvar_layer = tf.keras.layers.Dense(64)\n",
    "        \n",
    "        self.bn = tf.keras.layers.BatchNormalization(momentum=momentum)\n",
    "        \n",
    "        self.do = tf.keras.layers.Dropout(0.25)\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "            self.classification_loss_tracker\n",
    "        ]\n",
    "    \n",
    "    def call(self, X: Union[tf.Tensor, np.ndarray], training: Optional[bool] = None) -> tf.Tensor:\n",
    "        X = self.feature(X)\n",
    "        X = self.bn(X, training=training)\n",
    "        \n",
    "        M = self.do(tf.ones_like(X), training=training)\n",
    "        D = X * M\n",
    "        with tf.GradientTape() as tape:\n",
    "            output, encoded, importance = self.encoder(D)\n",
    "            encoded=self.mlp1(encoded)\n",
    "            z_mean = self.mean_layer(encoded)\n",
    "            z_log_var = self.logvar_layer(encoded)\n",
    "            z = Sampling()([z_mean, z_log_var])\n",
    "            # prediction = tf.keras.activations.sigmoid(output)\n",
    "            T = X * (1 - M)\n",
    "            reconstruction = self.decoder(z)\n",
    "            classification_output = self.classifier(z)\n",
    "            \n",
    "            reconstruction_loss = keras.losses.binary_crossentropy(X, reconstruction)\n",
    "            reconstruction_loss = tf.reduce_mean(tf.reduce_sum(reconstruction_loss, axis=1))      \n",
    "\n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "            print(classification_output.shape,y_train.shape)\n",
    "                \n",
    "                # Classification loss\n",
    "            classification_loss = keras.losses.binary_crossentropy(y_train, classification_output)\n",
    "            print('hello')\n",
    "            classification_loss = tf.reduce_mean(classification_loss)\n",
    "            \n",
    "            # Total loss\n",
    "            total_loss = reconstruction_loss + kl_loss + classification_loss  \n",
    "        \n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "\n",
    "        # Update loss metrics\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        self.classification_loss_tracker.update_state(classification_loss)\n",
    "\n",
    "        \n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "            \"classification_loss\": self.classification_loss_tracker.result()\n",
    "        }\n",
    "    \n",
    "    def transform(self, X: Union[tf.Tensor, np.ndarray], training: Optional[bool] = None) -> tf.Tensor:\n",
    "        X = self.feature(X)\n",
    "        _, encoded, _, _, _ = self.encoder(X)\n",
    "        return encoded\n",
    "    \n",
    "    def explain(self, X: Union[tf.Tensor, np.ndarray], training: Optional[bool] = None) -> tf.Tensor:\n",
    "        X = self.feature(X)\n",
    "        _, _, importance, _, _ = self.encoder(X)\n",
    "        return importance\n",
    "\n",
    "feature_column = None  \n",
    "\n",
    "ae = TabNetAutoencoder(outputs=1, inputs=12, n_steps=3, n_features=2, feature_column=feature_column)\n",
    "\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def dummy_loss(y, t):\n",
    "    return 0.\n",
    "\n",
    "ae.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.005), loss='binary_crossentropy')\n",
    "\n",
    "X_train = np.random.randn(100, 12)\n",
    "y_train = np.random.randint(0, 2, size=(100,))\n",
    "\n",
    "# Huấn luyện mô hình\n",
    "ae.fit(X_train,y_train, epochs=100)\n",
    "ae.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TabNetAutoencoder(tf.keras.Model):\n",
    "#     def __init__(self, outputs: int = 1, \n",
    "#                  inputs: int = 12,\n",
    "#                  n_steps: int  = 3, \n",
    "#                  n_features: int  = 8,\n",
    "#                  gamma: float = 1.3, \n",
    "#                  epsilon: float = 1e-8, \n",
    "#                  sparsity: float = 1e-5, \n",
    "#                  feature_column: Optional[tf.keras.layers.DenseFeatures] = None, \n",
    "#                  virtual_batch_size: Optional[int] = 128, \n",
    "#                  momentum: Optional[float] = 0.02):\n",
    "#         super(TabNetAutoencoder, self).__init__()\n",
    "        \n",
    "#         self.outputs = outputs\n",
    "#         self.inputs = inputs\n",
    "#         self.n_steps = n_steps\n",
    "#         self.n_features = n_features\n",
    "#         self.feature_column = feature_column\n",
    "#         self.virtual_batch_size = virtual_batch_size\n",
    "#         self.gamma = gamma\n",
    "#         self.epsilon = epsilon\n",
    "#         self.momentum = momentum\n",
    "#         self.sparsity = sparsity\n",
    "        \n",
    "#         if feature_column is None:\n",
    "#             self.feature = tf.keras.layers.Lambda(identity)\n",
    "#         else:\n",
    "#             self.feature = feature_column\n",
    "            \n",
    "#         self.encoder = TabNetEncoder(units=outputs, \n",
    "#                                     n_steps=n_steps, \n",
    "#                                     n_features = n_features,\n",
    "#                                     outputs=outputs, \n",
    "#                                     gamma=gamma, \n",
    "#                                     epsilon=epsilon, \n",
    "#                                     sparsity=sparsity,\n",
    "#                                     virtual_batch_size=self.virtual_batch_size, \n",
    "#                                     momentum=momentum)\n",
    "        \n",
    "#         self.decoder = TabNetDecoder(units=inputs, \n",
    "#                                      n_steps=n_steps, \n",
    "#                                      n_features = n_features,\n",
    "#                                      virtual_batch_size=self.virtual_batch_size, \n",
    "#                                      momentum=momentum)\n",
    "        \n",
    "#         self.bn = tf.keras.layers.BatchNormalization(virtual_batch_size=self.virtual_batch_size, \n",
    "#                                                      momentum=momentum)\n",
    "        \n",
    "#         self.do = tf.keras.layers.Dropout(0.25)\n",
    "\n",
    "#     def forward(self, X: Union[tf.Tensor, np.ndarray], training: Optional[bool] = None) -> Tuple[tf.Tensor]:\n",
    "#         X = self.feature(X)\n",
    "#         X = self.bn(X)\n",
    "        \n",
    "#         # training mask\n",
    "#         M = self.do(tf.ones_like(X), training=training)\n",
    "#         D = X*M\n",
    "        \n",
    "#         #encoder\n",
    "#         output, encoded, importance = self.encoder(D)\n",
    "#         prediction = tf.keras.activations.sigmoid(output)        \n",
    "        \n",
    "#         return prediction, encoded, importance, X, M\n",
    "    \n",
    "#     def call(self, X: Union[tf.Tensor, np.ndarray], training: Optional[bool] = None) -> tf.Tensor:\n",
    "#         # encode\n",
    "#         prediction, encoded, _, X, M = self.forward(X)\n",
    "#         T = X * (1 - M)\n",
    "\n",
    "#         #decode\n",
    "#         reconstruction = self.decoder(encoded)\n",
    "        \n",
    "#         #loss\n",
    "#         loss  = tf.reduce_mean(tf.where(M != 0., tf.square(T-reconstruction), tf.zeros_like(reconstruction)))\n",
    "        \n",
    "#         self.add_loss(loss)\n",
    "        \n",
    "#         return prediction\n",
    "    \n",
    "#     def transform(self, X: Union[tf.Tensor, np.ndarray], training: Optional[bool] = None) -> tf.Tensor:\n",
    "#         _, encoded, _, _, _ = self.forward(X)\n",
    "#         return encoded\n",
    "    \n",
    "#     def explain(self, X: Union[tf.Tensor, np.ndarray], training: Optional[bool] = None) -> tf.Tensor:\n",
    "#         _, _, importance, _, _ = self.forward(X)\n",
    "#         return importance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
