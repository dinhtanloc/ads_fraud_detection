{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b1aa9a8-fdc3-47d7-a89a-8f5d01655b00",
   "metadata": {},
   "source": [
    "## General information\n",
    "\n",
    "In this kernel I work with IEEE Fraud Detection competition.\n",
    "\n",
    "EEE-CIS works across a variety of AI and machine learning areas, including deep neural networks, fuzzy systems, evolutionary computation, and swarm intelligence. Today they’re partnering with the world’s leading payment service company, Vesta Corporation, seeking the best solutions for fraud prevention industry, and now you are invited to join the challenge.\n",
    "\n",
    "We have a binary classification problem with a heavy imbalance which is an inherent property of such problems.\n",
    "At first I'll explore the data and try to find valuable insights, maybe I'll do some feature engineering and then it wil be time to build models.\n",
    "\n",
    "![](https://cis.ieee.org/images/files/slideshow/abstract01.jpg)\n",
    "\n",
    "*Work in progress*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1b90be3-d0ab-4198-ad56-1acc7b788f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\Data\\ads_fraud_detection\n"
     ]
    }
   ],
   "source": [
    "import sys,os\n",
    "__script_path=os.path.abspath(globals().get('__file__','.'))\n",
    "__script_dir = os.path.dirname(__script_path)\n",
    "root_dir = os.path.abspath(f'{__script_dir}/../../..')\n",
    "print(root_dir)\n",
    "for lib in [root_dir][::-1]:\n",
    "    if lib in sys.path:\n",
    "        sys.path.remove(lib)\n",
    "    sys.path.insert(0,lib)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f331d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:/Users/PC/Data/ads_fraud_detection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\Data\\ads_fraud_detection\\.conda\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\Users\\PC\\Data\\ads_fraud_detection\\.conda\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "c:\\Users\\PC\\Data\\ads_fraud_detection\\.conda\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.12.0\n"
     ]
    }
   ],
   "source": [
    "from config.config import *\n",
    "from libs.common import *\n",
    "#init_notebook_mode(connected=True)\n",
    "print(tf.__version__) # requires version >= 1.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c883be51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: JOBLIB_TEMP_FOLDER=/tmp\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import NuSVR, SVR\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "pd.options.display.precision = 15\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import time\n",
    "import datetime\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold, GroupKFold, GridSearchCV, train_test_split, TimeSeriesSplit\n",
    "from sklearn import metrics\n",
    "from sklearn import linear_model\n",
    "import gc\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# import eli5\n",
    "import shap\n",
    "from IPython.display import HTML\n",
    "import json\n",
    "# import altair as alt\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# alt.renderers.enable('notebook')\n",
    "\n",
    "%env JOBLIB_TEMP_FOLDER=/tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b97348d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import json\n",
    "import gc\n",
    "from numba import jit\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostRegressor, CatBoostClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "# import altair as alt\n",
    "# from altair.vega import v5\n",
    "from IPython.display import HTML\n",
    "\n",
    "# using ideas from this kernel: https://www.kaggle.com/notslush/altair-visualization-2018-stackoverflow-survey\n",
    "def prepare_altair():\n",
    "    \"\"\"\n",
    "    Helper function to prepare altair for working.\n",
    "    \"\"\"\n",
    "\n",
    "    vega_url = 'https://cdn.jsdelivr.net/npm/vega@' + v5.SCHEMA_VERSION\n",
    "    vega_lib_url = 'https://cdn.jsdelivr.net/npm/vega-lib'\n",
    "    vega_lite_url = 'https://cdn.jsdelivr.net/npm/vega-lite@' + alt.SCHEMA_VERSION\n",
    "    vega_embed_url = 'https://cdn.jsdelivr.net/npm/vega-embed@3'\n",
    "    noext = \"?noext\"\n",
    "    \n",
    "    paths = {\n",
    "        'vega': vega_url + noext,\n",
    "        'vega-lib': vega_lib_url + noext,\n",
    "        'vega-lite': vega_lite_url + noext,\n",
    "        'vega-embed': vega_embed_url + noext\n",
    "    }\n",
    "    \n",
    "    workaround = f\"\"\"    requirejs.config({{\n",
    "        baseUrl: 'https://cdn.jsdelivr.net/npm/',\n",
    "        paths: {paths}\n",
    "    }});\n",
    "    \"\"\"\n",
    "    \n",
    "    return workaround\n",
    "    \n",
    "\n",
    "def add_autoincrement(render_func):\n",
    "    # Keep track of unique <div/> IDs\n",
    "    cache = {}\n",
    "    def wrapped(chart, id=\"vega-chart\", autoincrement=True):\n",
    "        if autoincrement:\n",
    "            if id in cache:\n",
    "                counter = 1 + cache[id]\n",
    "                cache[id] = counter\n",
    "            else:\n",
    "                cache[id] = 0\n",
    "            actual_id = id if cache[id] == 0 else id + '-' + str(cache[id])\n",
    "        else:\n",
    "            if id not in cache:\n",
    "                cache[id] = 0\n",
    "            actual_id = id\n",
    "        return render_func(chart, id=actual_id)\n",
    "    # Cache will stay outside and \n",
    "    return wrapped\n",
    "           \n",
    "\n",
    "@add_autoincrement\n",
    "def render(chart, id=\"vega-chart\"):\n",
    "    \"\"\"\n",
    "    Helper function to plot altair visualizations.\n",
    "    \"\"\"\n",
    "    chart_str = \"\"\"\n",
    "    <div id=\"{id}\"></div><script>\n",
    "    require([\"vega-embed\"], function(vg_embed) {{\n",
    "        const spec = {chart};     \n",
    "        vg_embed(\"#{id}\", spec, {{defaultStyle: true}}).catch(console.warn);\n",
    "        console.log(\"anything?\");\n",
    "    }});\n",
    "    console.log(\"really...anything?\");\n",
    "    </script>\n",
    "    \"\"\"\n",
    "    return HTML(\n",
    "        chart_str.format(\n",
    "            id=id,\n",
    "            chart=json.dumps(chart) if isinstance(chart, dict) else chart.to_json(indent=None)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                c_prec = df[col].apply(lambda x: np.finfo(x).precision).max()\n",
    "                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max and c_prec == np.finfo(np.float32).precision:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "    \n",
    "\n",
    "@jit\n",
    "def fast_auc(y_true, y_prob):\n",
    "    \"\"\"\n",
    "    fast roc_auc computation: https://www.kaggle.com/c/microsoft-malware-prediction/discussion/76013\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_true = y_true[np.argsort(y_prob)]\n",
    "    nfalse = 0\n",
    "    auc = 0\n",
    "    n = len(y_true)\n",
    "    for i in range(n):\n",
    "        y_i = y_true[i]\n",
    "        nfalse += (1 - y_i)\n",
    "        auc += y_i * nfalse\n",
    "    auc /= (nfalse * (n - nfalse))\n",
    "    return auc\n",
    "\n",
    "\n",
    "def eval_auc(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Fast auc eval function for lgb.\n",
    "    \"\"\"\n",
    "    return 'auc', fast_auc(y_true, y_pred), True\n",
    "\n",
    "\n",
    "def group_mean_log_mae(y_true, y_pred, types, floor=1e-9):\n",
    "    \"\"\"\n",
    "    Fast metric computation for this competition: https://www.kaggle.com/c/champs-scalar-coupling\n",
    "    Code is from this kernel: https://www.kaggle.com/uberkinder/efficient-metric\n",
    "    \"\"\"\n",
    "    maes = (y_true-y_pred).abs().groupby(types).mean()\n",
    "    return np.log(maes.map(lambda x: max(x, floor))).mean()\n",
    "    \n",
    "\n",
    "def train_model_regression(X, X_test, y, params, folds=None, model_type='lgb', eval_metric='mae', columns=None, plot_feature_importance=False, model=None,\n",
    "                               verbose=10000, early_stopping_rounds=200, n_estimators=50000, splits=None, n_folds=3):\n",
    "    \"\"\"\n",
    "    A function to train a variety of regression models.\n",
    "    Returns dictionary with oof predictions, test predictions, scores and, if necessary, feature importances.\n",
    "    \n",
    "    :params: X - training data, can be pd.DataFrame or np.ndarray (after normalizing)\n",
    "    :params: X_test - test data, can be pd.DataFrame or np.ndarray (after normalizing)\n",
    "    :params: y - target\n",
    "    :params: folds - folds to split data\n",
    "    :params: model_type - type of model to use\n",
    "    :params: eval_metric - metric to use\n",
    "    :params: columns - columns to use. If None - use all columns\n",
    "    :params: plot_feature_importance - whether to plot feature importance of LGB\n",
    "    :params: model - sklearn model, works only for \"sklearn\" model type\n",
    "    \n",
    "    \"\"\"\n",
    "    columns = X.columns if columns is None else columns\n",
    "    X_test = X_test[columns]\n",
    "    splits = folds.split(X) if splits is None else splits\n",
    "    n_splits = folds.n_splits if splits is None else n_folds\n",
    "    \n",
    "    # to set up scoring parameters\n",
    "    metrics_dict = {'mae': {'lgb_metric_name': 'mae',\n",
    "                        'catboost_metric_name': 'MAE',\n",
    "                        'sklearn_scoring_function': metrics.mean_absolute_error},\n",
    "                    'group_mae': {'lgb_metric_name': 'mae',\n",
    "                        'catboost_metric_name': 'MAE',\n",
    "                        'scoring_function': group_mean_log_mae},\n",
    "                    'mse': {'lgb_metric_name': 'mse',\n",
    "                        'catboost_metric_name': 'MSE',\n",
    "                        'sklearn_scoring_function': metrics.mean_squared_error}\n",
    "                    }\n",
    "\n",
    "    \n",
    "    result_dict = {}\n",
    "    \n",
    "    # out-of-fold predictions on train data\n",
    "    oof = np.zeros(len(X))\n",
    "    \n",
    "    # averaged predictions on train data\n",
    "    prediction = np.zeros(len(X_test))\n",
    "    \n",
    "    # list of scores on folds\n",
    "    scores = []\n",
    "    feature_importance = pd.DataFrame()\n",
    "    \n",
    "    # split and train on folds\n",
    "    for fold_n, (train_index, valid_index) in enumerate(splits):\n",
    "        if verbose:\n",
    "            print(f'Fold {fold_n + 1} started at {time.ctime()}')\n",
    "        if type(X) == np.ndarray:\n",
    "            X_train, X_valid = X[columns][train_index], X[columns][valid_index]\n",
    "            y_train, y_valid = y[train_index], y[valid_index]\n",
    "        else:\n",
    "            X_train, X_valid = X[columns].iloc[train_index], X[columns].iloc[valid_index]\n",
    "            y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
    "            \n",
    "        if model_type == 'lgb':\n",
    "            model = lgb.LGBMRegressor(**params, n_estimators = n_estimators, n_jobs = -1)\n",
    "            model.fit(X_train, y_train, \n",
    "                    eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric=metrics_dict[eval_metric]['lgb_metric_name'],\n",
    "                    verbose=verbose, early_stopping_rounds=early_stopping_rounds)\n",
    "            \n",
    "            y_pred_valid = model.predict(X_valid)\n",
    "            y_pred = model.predict(X_test, num_iteration=model.best_iteration_)\n",
    "            \n",
    "        if model_type == 'xgb':\n",
    "            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X.columns)\n",
    "            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X.columns)\n",
    "\n",
    "            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n",
    "            model = xgb.train(dtrain=train_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=200, verbose_eval=verbose, params=params)\n",
    "            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n",
    "            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n",
    "        \n",
    "        if model_type == 'sklearn':\n",
    "            model = model\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            y_pred_valid = model.predict(X_valid).reshape(-1,)\n",
    "            score = metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid)\n",
    "            print(f'Fold {fold_n}. {eval_metric}: {score:.4f}.')\n",
    "            print('')\n",
    "            \n",
    "            y_pred = model.predict(X_test).reshape(-1,)\n",
    "        \n",
    "        if model_type == 'cat':\n",
    "            model = CatBoostRegressor(iterations=20000,  eval_metric=metrics_dict[eval_metric]['catboost_metric_name'], **params,\n",
    "                                      loss_function=metrics_dict[eval_metric]['catboost_metric_name'])\n",
    "            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)\n",
    "\n",
    "            y_pred_valid = model.predict(X_valid)\n",
    "            y_pred = model.predict(X_test)\n",
    "        \n",
    "        oof[valid_index] = y_pred_valid.reshape(-1,)\n",
    "        if eval_metric != 'group_mae':\n",
    "            scores.append(metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid))\n",
    "        else:\n",
    "            scores.append(metrics_dict[eval_metric]['scoring_function'](y_valid, y_pred_valid, X_valid['type']))\n",
    "\n",
    "        prediction += y_pred    \n",
    "        \n",
    "        if model_type == 'lgb' and plot_feature_importance:\n",
    "            # feature importance\n",
    "            fold_importance = pd.DataFrame()\n",
    "            fold_importance[\"feature\"] = columns\n",
    "            fold_importance[\"importance\"] = model.feature_importances_\n",
    "            fold_importance[\"fold\"] = fold_n + 1\n",
    "            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n",
    "\n",
    "    prediction /= n_splits\n",
    "    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n",
    "    \n",
    "    result_dict['oof'] = oof\n",
    "    result_dict['prediction'] = prediction\n",
    "    result_dict['scores'] = scores\n",
    "    \n",
    "    if model_type == 'lgb':\n",
    "        if plot_feature_importance:\n",
    "            feature_importance[\"importance\"] /= n_splits\n",
    "            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n",
    "                by=\"importance\", ascending=False)[:50].index\n",
    "\n",
    "            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n",
    "\n",
    "            plt.figure(figsize=(16, 12));\n",
    "            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n",
    "            plt.title('LGB Features (avg over folds)');\n",
    "            \n",
    "            result_dict['feature_importance'] = feature_importance\n",
    "        \n",
    "    return result_dict\n",
    "    \n",
    "\n",
    "\n",
    "def train_model_classification(X, X_test, y, params, folds, model_type='lgb', eval_metric='auc', columns=None, plot_feature_importance=False, model=None,\n",
    "                               verbose=10000, early_stopping_rounds=200, n_estimators=50000, splits=None, n_folds=3, averaging='usual', n_jobs=-1):\n",
    "    \"\"\"\n",
    "    A function to train a variety of classification models.\n",
    "    Returns dictionary with oof predictions, test predictions, scores and, if necessary, feature importances.\n",
    "    \n",
    "    :params: X - training data, can be pd.DataFrame or np.ndarray (after normalizing)\n",
    "    :params: X_test - test data, can be pd.DataFrame or np.ndarray (after normalizing)\n",
    "    :params: y - target\n",
    "    :params: folds - folds to split data\n",
    "    :params: model_type - type of model to use\n",
    "    :params: eval_metric - metric to use\n",
    "    :params: columns - columns to use. If None - use all columns\n",
    "    :params: plot_feature_importance - whether to plot feature importance of LGB\n",
    "    :params: model - sklearn model, works only for \"sklearn\" model type\n",
    "    \n",
    "    \"\"\"\n",
    "    columns = X.columns if columns is None else columns\n",
    "    n_splits = folds.n_splits if splits is None else n_folds\n",
    "    X_test = X_test[columns]\n",
    "    \n",
    "    # to set up scoring parameters\n",
    "    metrics_dict = {'auc': {'lgb_metric_name': eval_auc,\n",
    "                        'catboost_metric_name': 'AUC',\n",
    "                        'sklearn_scoring_function': metrics.roc_auc_score},\n",
    "                    }\n",
    "    \n",
    "    result_dict = {}\n",
    "    if averaging == 'usual':\n",
    "        # out-of-fold predictions on train data\n",
    "        oof = np.zeros((len(X), 1))\n",
    "\n",
    "        # averaged predictions on train data\n",
    "        prediction = np.zeros((len(X_test), 1))\n",
    "        \n",
    "    elif averaging == 'rank':\n",
    "        # out-of-fold predictions on train data\n",
    "        oof = np.zeros((len(X), 1))\n",
    "\n",
    "        # averaged predictions on train data\n",
    "        prediction = np.zeros((len(X_test), 1))\n",
    "\n",
    "    \n",
    "    # list of scores on folds\n",
    "    scores = []\n",
    "    feature_importance = pd.DataFrame()\n",
    "    \n",
    "    # split and train on folds\n",
    "    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):\n",
    "        print(f'Fold {fold_n + 1} started at {time.ctime()}')\n",
    "        if type(X) == np.ndarray:\n",
    "            X_train, X_valid = X[columns][train_index], X[columns][valid_index]\n",
    "            y_train, y_valid = y[train_index], y[valid_index]\n",
    "        else:\n",
    "            X_train, X_valid = X[columns].iloc[train_index], X[columns].iloc[valid_index]\n",
    "            y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
    "            \n",
    "        if model_type == 'lgb':\n",
    "            model = lgb.LGBMClassifier(**params, n_estimators=n_estimators, n_jobs = n_jobs)\n",
    "            model.fit(X_train, y_train, \n",
    "                    eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric=metrics_dict[eval_metric]['lgb_metric_name'],\n",
    "                    verbose=verbose, early_stopping_rounds=early_stopping_rounds)\n",
    "            \n",
    "            y_pred_valid = model.predict_proba(X_valid)[:, 1]\n",
    "            y_pred = model.predict_proba(X_test, num_iteration=model.best_iteration_)[:, 1]\n",
    "            \n",
    "        if model_type == 'xgb':\n",
    "            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X.columns)\n",
    "            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X.columns)\n",
    "\n",
    "            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n",
    "            model = xgb.train(dtrain=train_data, num_boost_round=n_estimators, evals=watchlist, early_stopping_rounds=early_stopping_rounds, verbose_eval=verbose, params=params)\n",
    "            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n",
    "            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n",
    "        \n",
    "        if model_type == 'sklearn':\n",
    "            model = model\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            y_pred_valid = model.predict(X_valid).reshape(-1,)\n",
    "            score = metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid)\n",
    "            print(f'Fold {fold_n}. {eval_metric}: {score:.4f}.')\n",
    "            print('')\n",
    "            \n",
    "            y_pred = model.predict_proba(X_test)\n",
    "        \n",
    "        if model_type == 'cat':\n",
    "            model = CatBoostClassifier(iterations=n_estimators, eval_metric=metrics_dict[eval_metric]['catboost_metric_name'], **params,\n",
    "                                      loss_function=Logloss)\n",
    "            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)\n",
    "\n",
    "            y_pred_valid = model.predict(X_valid)\n",
    "            y_pred = model.predict(X_test)\n",
    "        \n",
    "        if averaging == 'usual':\n",
    "            \n",
    "            oof[valid_index] = y_pred_valid.reshape(-1, 1)\n",
    "            scores.append(metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid))\n",
    "            \n",
    "            prediction += y_pred.reshape(-1, 1)\n",
    "\n",
    "        elif averaging == 'rank':\n",
    "                                  \n",
    "            oof[valid_index] = y_pred_valid.reshape(-1, 1)\n",
    "            scores.append(metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid))\n",
    "                                  \n",
    "            prediction += pd.Series(y_pred).rank().values.reshape(-1, 1)        \n",
    "        \n",
    "        if model_type == 'lgb' and plot_feature_importance:\n",
    "            # feature importance\n",
    "            fold_importance = pd.DataFrame()\n",
    "            fold_importance[\"feature\"] = columns\n",
    "            fold_importance[\"importance\"] = model.feature_importances_\n",
    "            fold_importance[\"fold\"] = fold_n + 1\n",
    "            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n",
    "\n",
    "    prediction /= n_splits\n",
    "    \n",
    "    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n",
    "    \n",
    "    result_dict['oof'] = oof\n",
    "    result_dict['prediction'] = prediction\n",
    "    result_dict['scores'] = scores\n",
    "    \n",
    "    if model_type == 'lgb':\n",
    "        if plot_feature_importance:\n",
    "            feature_importance[\"importance\"] /= n_splits\n",
    "            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n",
    "                by=\"importance\", ascending=False)[:50].index\n",
    "\n",
    "            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n",
    "\n",
    "            plt.figure(figsize=(16, 12));\n",
    "            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n",
    "            plt.title('LGB Features (avg over folds)');\n",
    "            \n",
    "            result_dict['feature_importance'] = feature_importance\n",
    "            result_dict['top_columns'] = cols\n",
    "        \n",
    "    return result_dict\n",
    "\n",
    "# setting up altair\n",
    "# workaround = prepare_altair()\n",
    "# HTML(\"\".join((\n",
    "#     \"<script>\",\n",
    "#     workaround,\n",
    "#     \"</script>\",\n",
    "# )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0ee48c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir=f\"{exps_dir}/exp3\"\n",
    "if os.path.exists(save_dir) == False: \n",
    "  os.makedirs(save_dir, exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfad7f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data download location c:\\Users\\PC\\Data\\ads_fraud_detection\\src\\prj3\\1.EDA\\tmp\n",
      "ieeecis\n",
      "Train set: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EVENT_LABEL</th>\n",
       "      <th>transactionamt</th>\n",
       "      <th>productcd</th>\n",
       "      <th>card1</th>\n",
       "      <th>card2</th>\n",
       "      <th>card3</th>\n",
       "      <th>card5</th>\n",
       "      <th>card6</th>\n",
       "      <th>addr1</th>\n",
       "      <th>dist1</th>\n",
       "      <th>...</th>\n",
       "      <th>id_17</th>\n",
       "      <th>id_19</th>\n",
       "      <th>id_20</th>\n",
       "      <th>devicetype</th>\n",
       "      <th>deviceinfo</th>\n",
       "      <th>EVENT_ID</th>\n",
       "      <th>ENTITY_ID</th>\n",
       "      <th>EVENT_TIMESTAMP</th>\n",
       "      <th>LABEL_TIMESTAMP</th>\n",
       "      <th>ENTITY_TYPE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TransactionID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2987000.0</th>\n",
       "      <td>0</td>\n",
       "      <td>68.5</td>\n",
       "      <td>W</td>\n",
       "      <td>13926.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>150.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>credit</td>\n",
       "      <td>315.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f61550a7-6e0d-442f-8ff4-e1f3b403cc5d</td>\n",
       "      <td>13926.0_315.0_-13.0</td>\n",
       "      <td>2021-01-02T00:00:00Z</td>\n",
       "      <td>2025-05-05T02:19:57Z</td>\n",
       "      <td>user</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2987001.0</th>\n",
       "      <td>0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>W</td>\n",
       "      <td>2755.0</td>\n",
       "      <td>404.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>credit</td>\n",
       "      <td>325.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>e18bcdbf-2755-4802-a0a8-41910874184d</td>\n",
       "      <td>2755.0_325.0_1.0</td>\n",
       "      <td>2021-01-02T00:00:01Z</td>\n",
       "      <td>2025-05-05T02:19:57Z</td>\n",
       "      <td>user</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2987002.0</th>\n",
       "      <td>0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>W</td>\n",
       "      <td>4663.0</td>\n",
       "      <td>490.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>debit</td>\n",
       "      <td>330.0</td>\n",
       "      <td>287.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>290681f0-d7dd-42be-bd0d-79295d9047d0</td>\n",
       "      <td>4663.0_330.0_1.0</td>\n",
       "      <td>2021-01-02T00:01:09Z</td>\n",
       "      <td>2025-05-05T02:19:57Z</td>\n",
       "      <td>user</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2987003.0</th>\n",
       "      <td>0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>W</td>\n",
       "      <td>18132.0</td>\n",
       "      <td>567.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>debit</td>\n",
       "      <td>476.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>83cd79fe-5cfa-4b1a-9579-f39f8376e2d3</td>\n",
       "      <td>18132.0_476.0_-111.0</td>\n",
       "      <td>2021-01-02T00:01:39Z</td>\n",
       "      <td>2025-05-05T02:19:57Z</td>\n",
       "      <td>user</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2987004.0</th>\n",
       "      <td>0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>H</td>\n",
       "      <td>4497.0</td>\n",
       "      <td>514.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>credit</td>\n",
       "      <td>420.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>166.0</td>\n",
       "      <td>542.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>mobile</td>\n",
       "      <td>SAMSUNG SM-G892A Build/NRD90M</td>\n",
       "      <td>1fb3253b-530a-4d2d-b2e4-c0843f4d648a</td>\n",
       "      <td>4497.0_420.0_1.0</td>\n",
       "      <td>2021-01-02T00:01:46Z</td>\n",
       "      <td>2025-05-05T02:19:57Z</td>\n",
       "      <td>user</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 73 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               EVENT_LABEL  transactionamt productcd    card1  card2  card3  \\\n",
       "TransactionID                                                                 \n",
       "2987000.0                0            68.5         W  13926.0    NaN  150.0   \n",
       "2987001.0                0            29.0         W   2755.0  404.0  150.0   \n",
       "2987002.0                0            59.0         W   4663.0  490.0  150.0   \n",
       "2987003.0                0            50.0         W  18132.0  567.0  150.0   \n",
       "2987004.0                0            50.0         H   4497.0  514.0  150.0   \n",
       "\n",
       "               card5   card6  addr1  dist1  ...  id_17  id_19  id_20  \\\n",
       "TransactionID                               ...                        \n",
       "2987000.0      142.0  credit  315.0   19.0  ...    NaN    NaN    NaN   \n",
       "2987001.0      102.0  credit  325.0    NaN  ...    NaN    NaN    NaN   \n",
       "2987002.0      166.0   debit  330.0  287.0  ...    NaN    NaN    NaN   \n",
       "2987003.0      117.0   debit  476.0    NaN  ...    NaN    NaN    NaN   \n",
       "2987004.0      102.0  credit  420.0    NaN  ...  166.0  542.0  144.0   \n",
       "\n",
       "               devicetype                     deviceinfo  \\\n",
       "TransactionID                                              \n",
       "2987000.0             NaN                            NaN   \n",
       "2987001.0             NaN                            NaN   \n",
       "2987002.0             NaN                            NaN   \n",
       "2987003.0             NaN                            NaN   \n",
       "2987004.0          mobile  SAMSUNG SM-G892A Build/NRD90M   \n",
       "\n",
       "                                           EVENT_ID             ENTITY_ID  \\\n",
       "TransactionID                                                               \n",
       "2987000.0      f61550a7-6e0d-442f-8ff4-e1f3b403cc5d   13926.0_315.0_-13.0   \n",
       "2987001.0      e18bcdbf-2755-4802-a0a8-41910874184d      2755.0_325.0_1.0   \n",
       "2987002.0      290681f0-d7dd-42be-bd0d-79295d9047d0      4663.0_330.0_1.0   \n",
       "2987003.0      83cd79fe-5cfa-4b1a-9579-f39f8376e2d3  18132.0_476.0_-111.0   \n",
       "2987004.0      1fb3253b-530a-4d2d-b2e4-c0843f4d648a      4497.0_420.0_1.0   \n",
       "\n",
       "                    EVENT_TIMESTAMP       LABEL_TIMESTAMP  ENTITY_TYPE  \n",
       "TransactionID                                                           \n",
       "2987000.0      2021-01-02T00:00:00Z  2025-05-05T02:19:57Z         user  \n",
       "2987001.0      2021-01-02T00:00:01Z  2025-05-05T02:19:57Z         user  \n",
       "2987002.0      2021-01-02T00:01:09Z  2025-05-05T02:19:57Z         user  \n",
       "2987003.0      2021-01-02T00:01:39Z  2025-05-05T02:19:57Z         user  \n",
       "2987004.0      2021-01-02T00:01:46Z  2025-05-05T02:19:57Z         user  \n",
       "\n",
       "[5 rows x 73 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73\n",
      "(561013, 73)\n",
      "Test set: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transactionamt</th>\n",
       "      <th>productcd</th>\n",
       "      <th>card1</th>\n",
       "      <th>card2</th>\n",
       "      <th>card3</th>\n",
       "      <th>card5</th>\n",
       "      <th>card6</th>\n",
       "      <th>addr1</th>\n",
       "      <th>dist1</th>\n",
       "      <th>p_emaildomain</th>\n",
       "      <th>...</th>\n",
       "      <th>id_13</th>\n",
       "      <th>id_17</th>\n",
       "      <th>id_19</th>\n",
       "      <th>id_20</th>\n",
       "      <th>devicetype</th>\n",
       "      <th>deviceinfo</th>\n",
       "      <th>EVENT_ID</th>\n",
       "      <th>ENTITY_ID</th>\n",
       "      <th>EVENT_TIMESTAMP</th>\n",
       "      <th>ENTITY_TYPE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TransactionID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3548013.0</th>\n",
       "      <td>125.000000000000000</td>\n",
       "      <td>S</td>\n",
       "      <td>15775.0</td>\n",
       "      <td>481.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>credit</td>\n",
       "      <td>330.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>52.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>633.0</td>\n",
       "      <td>533.0</td>\n",
       "      <td>desktop</td>\n",
       "      <td>Windows</td>\n",
       "      <td>63b5e3ef-c3ef-4158-bc28-2656b15a7d69</td>\n",
       "      <td>15775.0_330.0_129.0</td>\n",
       "      <td>2021-06-21T23:11:15Z</td>\n",
       "      <td>user</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3548014.0</th>\n",
       "      <td>125.000000000000000</td>\n",
       "      <td>S</td>\n",
       "      <td>15775.0</td>\n",
       "      <td>481.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>credit</td>\n",
       "      <td>330.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>52.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>633.0</td>\n",
       "      <td>533.0</td>\n",
       "      <td>desktop</td>\n",
       "      <td>Windows</td>\n",
       "      <td>c91059d8-1875-4f86-b463-8a90de3fcd95</td>\n",
       "      <td>15775.0_330.0_129.0</td>\n",
       "      <td>2021-06-21T23:11:29Z</td>\n",
       "      <td>user</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3548015.0</th>\n",
       "      <td>125.000000000000000</td>\n",
       "      <td>S</td>\n",
       "      <td>15775.0</td>\n",
       "      <td>481.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>credit</td>\n",
       "      <td>330.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>52.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>633.0</td>\n",
       "      <td>533.0</td>\n",
       "      <td>desktop</td>\n",
       "      <td>Windows</td>\n",
       "      <td>fc21d560-e91c-4dd5-9b49-c6dc9ee0e19f</td>\n",
       "      <td>15775.0_330.0_129.0</td>\n",
       "      <td>2021-06-21T23:11:45Z</td>\n",
       "      <td>user</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3548016.0</th>\n",
       "      <td>125.000000000000000</td>\n",
       "      <td>S</td>\n",
       "      <td>15775.0</td>\n",
       "      <td>481.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>credit</td>\n",
       "      <td>330.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>52.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>633.0</td>\n",
       "      <td>533.0</td>\n",
       "      <td>desktop</td>\n",
       "      <td>Windows</td>\n",
       "      <td>176be8a2-c615-488f-97b8-27f33436cd5d</td>\n",
       "      <td>15775.0_330.0_129.0</td>\n",
       "      <td>2021-06-21T23:12:00Z</td>\n",
       "      <td>user</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3548017.0</th>\n",
       "      <td>31.950000762939453</td>\n",
       "      <td>W</td>\n",
       "      <td>9500.0</td>\n",
       "      <td>321.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>debit</td>\n",
       "      <td>204.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>a597cc34-284b-470c-b182-7900a15675f8</td>\n",
       "      <td>9500.0_204.0_150.0</td>\n",
       "      <td>2021-06-21T23:12:11Z</td>\n",
       "      <td>user</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 71 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    transactionamt productcd    card1  card2  card3  card5  \\\n",
       "TransactionID                                                                \n",
       "3548013.0      125.000000000000000         S  15775.0  481.0  150.0  102.0   \n",
       "3548014.0      125.000000000000000         S  15775.0  481.0  150.0  102.0   \n",
       "3548015.0      125.000000000000000         S  15775.0  481.0  150.0  102.0   \n",
       "3548016.0      125.000000000000000         S  15775.0  481.0  150.0  102.0   \n",
       "3548017.0       31.950000762939453         W   9500.0  321.0  150.0  226.0   \n",
       "\n",
       "                card6  addr1  dist1 p_emaildomain  ... id_13  id_17  id_19  \\\n",
       "TransactionID                                      ...                       \n",
       "3548013.0      credit  330.0    NaN           NaN  ...  52.0  166.0  633.0   \n",
       "3548014.0      credit  330.0    NaN           NaN  ...  52.0  166.0  633.0   \n",
       "3548015.0      credit  330.0    NaN           NaN  ...  52.0  166.0  633.0   \n",
       "3548016.0      credit  330.0    NaN           NaN  ...  52.0  166.0  633.0   \n",
       "3548017.0       debit  204.0   74.0           NaN  ...   NaN    NaN    NaN   \n",
       "\n",
       "               id_20  devicetype  deviceinfo  \\\n",
       "TransactionID                                  \n",
       "3548013.0      533.0     desktop     Windows   \n",
       "3548014.0      533.0     desktop     Windows   \n",
       "3548015.0      533.0     desktop     Windows   \n",
       "3548016.0      533.0     desktop     Windows   \n",
       "3548017.0        NaN         NaN         NaN   \n",
       "\n",
       "                                           EVENT_ID            ENTITY_ID  \\\n",
       "TransactionID                                                              \n",
       "3548013.0      63b5e3ef-c3ef-4158-bc28-2656b15a7d69  15775.0_330.0_129.0   \n",
       "3548014.0      c91059d8-1875-4f86-b463-8a90de3fcd95  15775.0_330.0_129.0   \n",
       "3548015.0      fc21d560-e91c-4dd5-9b49-c6dc9ee0e19f  15775.0_330.0_129.0   \n",
       "3548016.0      176be8a2-c615-488f-97b8-27f33436cd5d  15775.0_330.0_129.0   \n",
       "3548017.0      a597cc34-284b-470c-b182-7900a15675f8   9500.0_204.0_150.0   \n",
       "\n",
       "                    EVENT_TIMESTAMP  ENTITY_TYPE  \n",
       "TransactionID                                     \n",
       "3548013.0      2021-06-21T23:11:15Z         user  \n",
       "3548014.0      2021-06-21T23:11:29Z         user  \n",
       "3548015.0      2021-06-21T23:11:45Z         user  \n",
       "3548016.0      2021-06-21T23:12:00Z         user  \n",
       "3548017.0      2021-06-21T23:12:11Z         user  \n",
       "\n",
       "[5 rows x 71 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29527, 71)\n",
      "Test scores\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EVENT_LABEL</th>\n",
       "      <th>EVENT_ID</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TransactionID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3548013.0</th>\n",
       "      <td>0</td>\n",
       "      <td>63b5e3ef-c3ef-4158-bc28-2656b15a7d69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3548014.0</th>\n",
       "      <td>0</td>\n",
       "      <td>c91059d8-1875-4f86-b463-8a90de3fcd95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3548015.0</th>\n",
       "      <td>0</td>\n",
       "      <td>fc21d560-e91c-4dd5-9b49-c6dc9ee0e19f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3548016.0</th>\n",
       "      <td>0</td>\n",
       "      <td>176be8a2-c615-488f-97b8-27f33436cd5d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3548017.0</th>\n",
       "      <td>0</td>\n",
       "      <td>a597cc34-284b-470c-b182-7900a15675f8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               EVENT_LABEL                              EVENT_ID\n",
       "TransactionID                                                   \n",
       "3548013.0                0  63b5e3ef-c3ef-4158-bc28-2656b15a7d69\n",
       "3548014.0                0  c91059d8-1875-4f86-b463-8a90de3fcd95\n",
       "3548015.0                0  fc21d560-e91c-4dd5-9b49-c6dc9ee0e19f\n",
       "3548016.0                0  176be8a2-c615-488f-97b8-27f33436cd5d\n",
       "3548017.0                0  a597cc34-284b-470c-b182-7900a15675f8"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVENT_LABEL\n",
      "0    28358\n",
      "1     1169\n",
      "Name: count, dtype: int64\n",
      "EVENT_LABEL\n",
      "0    0.965252142107224\n",
      "1    0.034747857892776\n",
      "Name: proportion, dtype: float64\n",
      "=========\n"
     ]
    }
   ],
   "source": [
    "from fdb.datasets import FraudDatasetBenchmark\n",
    "\n",
    "# all_keys = ['fakejob', 'vehicleloan', 'malurl', 'ieeecis', 'ccfraud', 'fraudecom', 'twitterbot', 'ipblock'] \n",
    "key = 'ieeecis'\n",
    "\n",
    "obj = FraudDatasetBenchmark(\n",
    "    key=key,\n",
    "    load_pre_downloaded=False,  # default\n",
    "    delete_downloaded=True,  # default\n",
    "    add_random_values_if_real_na = { \n",
    "        \"EVENT_TIMESTAMP\": True, \n",
    "        \"LABEL_TIMESTAMP\": True,\n",
    "        \"ENTITY_ID\": True,\n",
    "        \"ENTITY_TYPE\": True,\n",
    "        \"ENTITY_ID\": True,\n",
    "        \"EVENT_ID\": True\n",
    "        } # default\n",
    "    )\n",
    "print(obj.key)\n",
    "\n",
    "print('Train set: ')\n",
    "display(obj.train.head())\n",
    "print(len(obj.train.columns))\n",
    "print(obj.train.shape)\n",
    "\n",
    "print('Test set: ')\n",
    "display(obj.test.head())\n",
    "print(obj.test.shape)\n",
    "\n",
    "print('Test scores')\n",
    "display(obj.test_labels.head())\n",
    "print(obj.test_labels['EVENT_LABEL'].value_counts())\n",
    "print(obj.train['EVENT_LABEL'].value_counts(normalize=True))\n",
    "print('=========')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc9a9dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_identity = pd.read_csv(f'{data_dir}/ieee-fraud-detection/train_identity.csv')\n",
    "# train_transaction = pd.read_csv(f'{data_dir}/ieee-fraud-detection/train_transaction.csv')\n",
    "# test_identity = pd.read_csv(f'{data_dir}/ieee-fraud-detection/test_identity.csv')\n",
    "# test_transaction = pd.read_csv(f'{data_dir}/ieee-fraud-detection/test_transaction.csv')\n",
    "# sub = pd.read_csv(f'{data_dir}/ieee-fraud-detection/sample_submission.csv')\n",
    "# # let's combine the data and work with the whole dataset\n",
    "# train = pd.merge(train_transaction, train_identity, on='TransactionID', how='left')\n",
    "# test = pd.merge(test_transaction, test_identity, on='TransactionID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fda989da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(561013, 72) (29527, 73)\n"
     ]
    }
   ],
   "source": [
    "test = obj.test\n",
    "test = pd.concat([test, obj.test_labels], axis=1)\n",
    "train = obj.train\n",
    "train.drop(columns=['LABEL_TIMESTAMP'], inplace=True) \n",
    "print(train.shape, test.shape)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58dd09be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Không có cột nào bị thiếu trong test.\n",
      "Không có cột nào bị thiếu trong train.\n"
     ]
    }
   ],
   "source": [
    "train_cols = set(train.columns)\n",
    "test_cols = set(test.columns)\n",
    "\n",
    "# Cột có trong train nhưng không có trong test\n",
    "missing_in_test = train_cols - test_cols\n",
    "\n",
    "# Cột có trong test nhưng không có trong train\n",
    "missing_in_train = test_cols - train_cols\n",
    "\n",
    "# In kết quả\n",
    "if missing_in_test:\n",
    "    print(\"Cột có trong train nhưng thiếu trong test:\", missing_in_test)\n",
    "else:\n",
    "    print(\"Không có cột nào bị thiếu trong test.\")\n",
    "\n",
    "if missing_in_train:\n",
    "    print(\"Cột có trong test nhưng thiếu trong train:\", missing_in_train)\n",
    "else:\n",
    "    print(\"Không có cột nào bị thiếu trong train.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "021997f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset has 561013 rows and 72 columns.\n",
      "Test dataset has 29527 rows and 73 columns.\n"
     ]
    }
   ],
   "source": [
    "print(f'Train dataset has {train.shape[0]} rows and {train.shape[1]} columns.')\n",
    "print(f'Test dataset has {test.shape[0]} rows and {test.shape[1]} columns.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a522dfa-8c71-41bd-bf65-bf5eaab00fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 51 columns in train dataset with missing values.\n"
     ]
    }
   ],
   "source": [
    "print(f'There are {train.isnull().any().sum()} columns in train dataset with missing values.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a396fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.columns = [col.upper() for col in train.columns]\n",
    "# test.columns = [col.upper() for col in test.columns]\n",
    "\n",
    "train.rename(columns={\"EVENT_LABEL\": \"isFraud\"}, inplace=True)\n",
    "test.rename(columns={\"EVENT_LABEL\": \"isFraud\"}, inplace=True)\n",
    "# # train.rename(columns={\"ENTITY_ID\": \"UNIQUEID\"}, inplace=True)\n",
    "# # test.rename(columns={\"ENTITY_ID\": \"UNIQUEID\"}, inplace=True)\n",
    "\n",
    "# print(train.columns)\n",
    "# print(test.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46f32f41-5c71-48bf-b005-4c8c7a7f1504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Thay NaN bằng chuỗi trống để tránh lỗi khi ép kiểu string\n",
    "# one_value_cols_train = [col for col in train.columns if train[col].fillna(\"\").astype(str).nunique() <= 1]\n",
    "# one_value_cols_test = [col for col in test.columns if test[col].fillna(\"\").astype(str).nunique() <= 1]\n",
    "\n",
    "# # So sánh hai danh sách\n",
    "# print(set(one_value_cols_train) == set(one_value_cols_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4d1f800-41d4-492c-9d3f-ff0ae5f5fa38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'There are {len(one_value_cols)} columns in train dataset with one unique value.')\n",
    "# print(f'There are {len(one_value_cols_test)} columns in test dataset with one unique value.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72030d7b-b1c7-4612-b0e7-b07c3f63e21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(train['id_01'], bins=77);\n",
    "# plt.title('Distribution of id_01 variable');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45304438-e232-4ab7-9d72-94e1f24218bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train['id_03'].value_counts(dropna=False, normalize=True).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8192ea1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['isFraud', 'transactionamt', 'productcd', 'card1', 'card2', 'card3',\n",
       "       'card5', 'card6', 'addr1', 'dist1', 'p_emaildomain', 'r_emaildomain',\n",
       "       'c1', 'c2', 'c4', 'c5', 'c6', 'c7', 'c8', 'c9', 'c10', 'c11', 'c12',\n",
       "       'c13', 'c14', 'v62', 'v70', 'v76', 'v78', 'v82', 'v91', 'v127', 'v130',\n",
       "       'v139', 'v160', 'v165', 'v187', 'v203', 'v207', 'v209', 'v210', 'v221',\n",
       "       'v234', 'v257', 'v258', 'v261', 'v264', 'v266', 'v267', 'v271', 'v274',\n",
       "       'v277', 'v283', 'v285', 'v289', 'v291', 'v294', 'id_01', 'id_02',\n",
       "       'id_05', 'id_06', 'id_09', 'id_13', 'id_17', 'id_19', 'id_20',\n",
       "       'devicetype', 'deviceinfo', 'EVENT_ID', 'ENTITY_ID', 'EVENT_TIMESTAMP',\n",
       "       'ENTITY_TYPE'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5477b250-e2f3-4ec0-a403-56ab007bb873",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['transactionamt_to_mean_card1'] = train['transactionamt'] / train.groupby(['card1'])['transactionamt'].transform('mean')\n",
    "train['transactionamt_to_mean_card2'] = train['transactionamt'] / train.groupby(['card2'])['transactionamt'].transform('mean')\n",
    "train['transactionamt_to_std_card1'] = train['transactionamt'] / train.groupby(['card1'])['transactionamt'].transform('std')\n",
    "train['transactionamt_to_mean_card3'] = train['transactionamt'] / train.groupby(['card3'])['transactionamt'].transform('mean')\n",
    "train['transactionamt_to_std_card3'] = train['transactionamt'] / train.groupby(['card3'])['transactionamt'].transform('std')\n",
    "train['transactionamt_to_mean_card5'] = train['transactionamt'] / train.groupby(['card5'])['transactionamt'].transform('mean')\n",
    "train['transactionamt_to_std_card5'] = train['transactionamt'] / train.groupby(['card5'])['transactionamt'].transform('std')\n",
    "train['transactionamt_to_mean_card6'] = train['transactionamt'] / train.groupby(['card6'])['transactionamt'].transform('mean')\n",
    "train['transactionamt_to_std_card6'] = train['transactionamt'] / train.groupby(['card6'])['transactionamt'].transform('std')\n",
    "train['transactionamt_to_std_card2'] = train['transactionamt'] / train.groupby(['card2'])['transactionamt'].transform('std')\n",
    "\n",
    "test['transactionamt_to_mean_card1'] = test['transactionamt'] / test.groupby(['card1'])['transactionamt'].transform('mean')\n",
    "test['transactionamt_to_mean_card2'] = test['transactionamt'] / test.groupby(['card2'])['transactionamt'].transform('mean')\n",
    "test['transactionamt_to_std_card1'] = test['transactionamt'] / test.groupby(['card1'])['transactionamt'].transform('std')\n",
    "test['transactionamt_to_mean_card3'] = test['transactionamt'] / test.groupby(['card3'])['transactionamt'].transform('mean')\n",
    "test['transactionamt_to_std_card3'] = test['transactionamt'] / test.groupby(['card3'])['transactionamt'].transform('std')\n",
    "test['transactionamt_to_mean_card5'] = test['transactionamt'] / test.groupby(['card5'])['transactionamt'].transform('mean')\n",
    "test['transactionamt_to_std_card5'] = test['transactionamt'] / test.groupby(['card5'])['transactionamt'].transform('std')\n",
    "test['transactionamt_to_mean_card6'] = test['transactionamt'] / test.groupby(['card6'])['transactionamt'].transform('mean')\n",
    "test['transactionamt_to_std_card6'] = test['transactionamt'] / test.groupby(['card6'])['transactionamt'].transform('std')\n",
    "test['transactionamt_to_std_card2'] = test['transactionamt'] / test.groupby(['card2'])['transactionamt'].transform('std')\n",
    "\n",
    "\n",
    "train['id_02_to_mean_card1'] = train['id_02'] / train.groupby(['card1'])['id_02'].transform('mean')\n",
    "train['id_02_to_std_card1'] = train['id_02'] / train.groupby(['card1'])['id_02'].transform('std')\n",
    "train['id_02_to_mean_card2'] = train['id_02'] / train.groupby(['card2'])['id_02'].transform('mean')\n",
    "train['id_02_to_std_card2'] = train['id_02'] / train.groupby(['card2'])['id_02'].transform('std')\n",
    "train['id_02_to_mean_card3'] = train['id_02'] / train.groupby(['card3'])['id_02'].transform('mean')\n",
    "train['id_02_to_std_card3'] = train['id_02'] / train.groupby(['card3'])['id_02'].transform('std')\n",
    "train['id_02_to_mean_card5'] = train['id_02'] / train.groupby(['card5'])['id_02'].transform('mean')\n",
    "train['id_02_to_std_card5'] = train['id_02'] / train.groupby(['card5'])['id_02'].transform('std')\n",
    "train['id_02_to_mean_card6'] = train['id_02'] / train.groupby(['card6'])['id_02'].transform('mean')\n",
    "train['id_02_to_std_card6'] = train['id_02'] / train.groupby(['card6'])['id_02'].transform('std')\n",
    "\n",
    "\n",
    "test['id_02_to_mean_card1'] = test['id_02'] / test.groupby(['card1'])['id_02'].transform('mean')\n",
    "test['id_02_to_std_card1'] = test['id_02'] / test.groupby(['card1'])['id_02'].transform('std')\n",
    "test['id_02_to_mean_card2'] = test['id_02'] / test.groupby(['card2'])['id_02'].transform('mean')\n",
    "test['id_02_to_std_card2'] = test['id_02'] / test.groupby(['card2'])['id_02'].transform('std')\n",
    "test['id_02_to_mean_card3'] = test['id_02'] / test.groupby(['card3'])['id_02'].transform('mean')\n",
    "test['id_02_to_std_card3'] = test['id_02'] / test.groupby(['card3'])['id_02'].transform('std')\n",
    "test['id_02_to_mean_card5'] = test['id_02'] / test.groupby(['card5'])['id_02'].transform('mean')\n",
    "test['id_02_to_std_card5'] = test['id_02'] / test.groupby(['card5'])['id_02'].transform('std')\n",
    "test['id_02_to_mean_card6'] = test['id_02'] / test.groupby(['card6'])['id_02'].transform('mean')\n",
    "test['id_02_to_std_card6'] = test['id_02'] / test.groupby(['card6'])['id_02'].transform('std')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "242032f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[['P_emaildomain_1', 'P_emaildomain_2', 'P_emaildomain_3']] = train['p_emaildomain'].str.split('.', expand=True)\n",
    "train[['R_emaildomain_1', 'R_emaildomain_2', 'R_emaildomain_3']] = train['r_emaildomain'].str.split('.', expand=True)\n",
    "test[['P_emaildomain_1', 'P_emaildomain_2', 'P_emaildomain_3']] = test['p_emaildomain'].str.split('.', expand=True)\n",
    "test[['R_emaildomain_1', 'R_emaildomain_2', 'R_emaildomain_3']] = test['r_emaildomain'].str.split('.', expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "361aced8-0116-465b-a018-ad9b78209c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(f'{save_dir}/train_EDA.csv', index=None)\n",
    "test.to_csv(f'{save_dir}/test_EDA.csv', index=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecb735e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
